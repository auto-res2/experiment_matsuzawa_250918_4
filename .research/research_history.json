{
  "research_topic": "Graph Attention Networkの学習の高速化",
  "queries": [
    "accelerated GAT training",
    "fast GAT convergence",
    "distributed GAT training",
    "approximate GAT",
    "quantized GAT"
  ],
  "research_study_list": [
    {
      "title": "Transformer Quality in Linear Time",
      "abstract": "We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.",
      "full_text": "Transformer Quality in Linear Time Weizhe Hua* 1 2 Zihang Dai * 2 Hanxiao Liu * 2 Quoc V . Le2 Abstract We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head atten- tion with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH3, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9×on Wiki-40B and 12.1× on PG-19 for auto-regressive language modeling, and 4.8×on C4 for masked language modeling. 1. Introduction Transformers (Vaswani et al., 2017) have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language (Devlin et al., 2018; Brown et al., 2020) and vision (Dosovitskiy et al., 2020). Although they have been growing in model size, most Trans- formers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications. Many techniques have been proposed to speedup Transform- ers over extended context via more efﬁcient attention mech- anisms (Child et al., 2019; Dai et al., 2019; Rae et al., 2019; Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Roy et al., 2021; Jaegle et al., 2021). Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in *Equal contribution 1Cornell University 2Google Re- search, Brain Team. Correspondence to: Weizhe Hua <wh399@cornell.edu>, Zihang Dai <zihangd@google.com>, Hanxiao Liu <hanxiaol@google.com>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 0 5 10 15 20 25 30 TPU-core-days -2.8 -3.0 -3.2 Neg. log pplx FLASH TFM+ + TFM 4.9x 25.6x Speedup Length over TFM over TFM++ 512 1.8 × 1.2× 1024 9.0 × 1.3× 2048 8.9 × 1.6× 4096 13.1 × 2.7× 8192 25.6 × 4.9× Figure 1: TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki- 40B — All models are comparable in size at around 110M and trained for 125K steps with 218 tokens per batch. state-of-the-art systems. Here we examine this issue from a practical perspective, and ﬁnd existing efﬁcient attention methods suffer from at least one of the following drawbacks: • Inferior Quality. Our studies reveal that vanilla Trans- formers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure 1). Existing efﬁcient attention methods often incur signiﬁcant quality drop compared to augmented Trans- formers, and this drop outweighs their efﬁciency beneﬁts. • Overhead in Practice . As efﬁcient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs. • Inefﬁcient Auto-regressive Training. Most attention lin- earization techniques enjoy fast decoding during infer- ence, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large number of steps, making it infeasible to fully leverage the strength of modern accelerators during training. 3FLASH = Fast Linear Attention with a Single Head arXiv:2202.10447v2  [cs.LG]  27 Jun 2022Input Concat V softmax(QK+B) Gated Linear Unit  + Multi-Head Self-Attention Gated Attention Unit (Ours) Input Dense Dense Dense Dense Q K V V relu2(QK+B) Input Dense Dense Dense V Q & K def scale_offset(x): gamma = var(x.shape[=1:]) beta = var(x.shape[=1:]) return x ∗ gamma + beta def attn(x, v, s=128): z = dense(x, s) q, k = scale_offset(z), scale_offset(z) qk = tf.einsum(/quotesingle.ts1bns,bms→bnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 return tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, a, v) def gated_attn_unit(x, d=768, e=1536): shortcut, x = x, norm(x) u, v = dense(x, e), dense(x, e) x = u ∗ attn(x, v) return dense(x, d) + shortcut Figure 2: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity. We address the above issues by developing a new model fam- ily that, for the ﬁrst time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys lin- ear scalability over the context size on modern accelerators. Unlike existing efﬁcient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which nat- urally enables higher-quality approximation. Speciﬁcally, our model, named FLASH, is developed in two steps: First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit(GAU) in Figure 2. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of atten- tion. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss. We then propose an efﬁcient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to ﬁrst group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in Figure 4. We further describe how an accelerator-efﬁcient implementation can be naturally de- rived from this formulation, achieving linear scalability in practice with only a few lines of code change. We conduct extensive experiments to demonstrate the efﬁ- cacy of FLASH over a variety of tasks (masked and auto- regressive language modeling), datasets (C4, Wiki-40B, PG- 19) and model scales (110M to 500M). Remarkably, FLASH is competitive with fully-augmented Transformers (Trans- former++) in quality across a wide range of context sizes of practical interest (512–8K), while achieving linear scala- bility on modern hardware accelerators. For example, with comparable quality, FLASH achieves a speedup of 1.2×– 4.9×for language modeling on Wiki-40B and a speedup of 1.0×–4.8×for masked language modeling on C4 over Transformer++. As we further scale up to PG-19 (Rae et al., 2019), FLASH reduces the training cost of Transformer++ by up to 12.1×and achieves signiﬁcant gain in quality. 2. Gated Attention Unit Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers: Vanilla MLP. Let X ∈RT×d be the representations over T tokens. The output for Transformer’s MLP can be formu- lated as O= φ(XWu)Wo where Wu ∈Rd×e, Wo ∈Re×d. Here ddenotes the model size, edenotes the expanded inter- mediate size, and φis an element-wise activation function. Gated Linear Unit (GLU). This is an improved MLP augmented with gating (Dauphin et al., 2017). GLU has been proven effective in many cases (Shazeer, 2020; Narang et al., 2021) and is used in state-of-the-art Transformer language models (Du et al., 2021; Thoppilan et al., 2022). U = φu(XWu), V = φv(XWv) ∈RT×e (1) O= (U ⊙V)Wo ∈RT×d (2) where ⊙stands for element-wise multiplication. In GLU, each representation ui is gated by another representation vi associated with the same token.0.0 0.2 0.4 Step/sec/TPU-core 3.1 3.0 2.9 2.8 2.7 Neg. log pplx 42M 110M 335M 41M 105M 316M LM (Wiki-40B) MHSA+MLP MHSA+GLU GAU 0.0 0.5 1.0 Steps/sec/TPU-core 1.7 1.6 1.5 1.4 1.3 1.2 Neg. log pplx 42M 110M 335M 41M 105M 316M Masked LM (C4) MHSA+MLP MHSA+GLU GAU Layer Type # of Layers d MHSA+MLP 8+8 512 12+12 768 24+24 1024 MHSA+GLU 8+8 512 12+12 768 24+24 1024 GAU 15 512 22 768 46 1024 Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512). Gated Attention Unit (GAU). The key idea is to formu- late attention and GLU as a uniﬁed layer and to share their computation as much as possible (Figure 2). This not only results in higher param/compute efﬁciency, but also natu- rally enables a powerful attentive gating mechanism. Specif- ically, GAU generalizes Eq. (2) in GLU as follows: O= (U ⊙ˆV)Wo where ˆV = AV (3) where A∈RT×T contains token-token attention weights. Unlike GLU which always uses vi to gate ui (both asso- ciated with the same token), our GAU replaces vi with a potentially more relevant representation ˆvi = ∑ j aijvj “re- trieved” from all available tokens using attention. The above will reduce to GLU when Ais an identity matrix. Consistent with the ﬁndings in Liu et al. (2021), the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss: Z = φz(XWz) ∈RT×s (4) A= relu2 ( Q(Z)K(Z)⊤+ b ) ∈RT×T (5) Modiﬁcations PPLX (LM/MLM) Params (M) original GAU 16.78 / 4.23 105 relu2 − →softmax 17.04 / 4.31 105 single-head − →multi-head 17.76 / 4.48 105 no gating 17.45 / 4.58 131 Table 1: Impact of various modiﬁcations on GAU. where Z is a shared representation ( s ≪d)4, Qand K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay- erNorms), and bis the relative position bias. We also ﬁnd the softmax in MHSA can be simpliﬁed as a regular activa- tion function in the case of GAU5. The GAU layer and its 4Unless otherwise speciﬁed, we set s=128 in this work. 5We use squared ReLU (So et al., 2021) throughout this paper, which empirically works well on language tasks. Modiﬁcations PPLX (LM/MLM) Params (M) original MHSA 16.87 / 4.35 110 softmax − →relu2 17.15 / 4.77 110 multi-head − →single-head 17.89 / 4.73 110 add gating 17.25 / 4.43 106 Table 2: Impact of various modiﬁcations on MHSA. pseudocode are illustrated in Figure 2. Unlike Transformer’s MHSA which comes with4d2 param- eters, GAU’s attention introduces only a single small dense matrix Wz with dsparameters on top of GLU (scalars and offsets in Qand Kare negligible). By setting e = 2dfor GAU, this compact design allows us to replace each Trans- former block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed. GAU vs. Transformers. Figure 3 shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention. Layer Ablations. In Table 1 & 2 we show that both GAUs and Transformers are locally optimal on their own. 3. Fast Linear Attention with GAU There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences: • First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention with- out quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.• In addition, the number of attention modules is naturally doubled with GAU — recall MLP+MHSA ≈2×GAU in terms of cost (Section 2). Since approximate attention usu- ally requires more layers to capture full dependency (Dai et al., 2019; Child et al., 2019), this property also makes GAU more appealing in handling long sequences. With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformer- level quality in linear time on long sequences. 3.1. Existing Linear-Complexity Variants Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/s- parse patterns, including local window (Dai et al., 2019; Rae et al., 2019), local+sparse (Child et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), axial (Ho et al., 2019; Huang et al., 2019), learnable patterns through hashing (Kitaev et al., 2020) or clustering (Roy et al., 2021). Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical beneﬁts (speed and RAM efﬁciency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model. Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decompos- ing the attention matrix and then re-arranging the order of matrix multiplications (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021). Schematically, the linear attention can be expressed as ˆVlin = Q ( K⊤V )    Rd×d approx −−−→ˆVquad = Softmax ( QK⊤)    RT×T V where Q,K,V ∈RT×d are the query, key and value rep- resentations, respectively. Re-arranging the computation reduces the complexity w.r.tT from quadratic to linear. Another desirable property of linear attention is itsconstant6 computation and memory for each auto-regressive decoding step at inference time. To see that, deﬁne Mt = K⊤ :t V:t and notice that the computation of Mt can be fully incremental: Mt = Mt−1 + KtV⊤ t (6) 6Constant is with respective to the sequence length T. cumsum cumsum Figure 4: (top) Quadratic attention, (mid) Linear attention, (bottom) Proposed mixed chunk attention with a chunk size (C) of 2 (Cis always greater than or equal to 128 in our ex- periments). Our method signiﬁcantly reduces the compute in quadratic attention (red links), while requiring substan- tially less RNN-style steps (green squares) in conventional linear attention. This means we only need to maintain a cache with constant O(d2) memory and whenever a new input arrives at time stamp t, only constant O(d2) computation is required to accumulate KtV⊤ t into Mt−1 and get Mt. On the contrary, full quadratic attention requires linear O(Td) computation and memory for each decoding step, as each new input has to attend to all the previous steps. However, on the other hand, re-arranging the computation in linear attention leads to a severe inefﬁciency during auto- regressive training. As shown in Fig. 4 (mid), due to the causal constraint for auto-regressive training, the query vec- tor at each time step Qt corresponds to a different cache value Mt = K⊤ :t V:t. This requires the model to compute and cache T different values {Mt}T t=1 instead of only one value K⊤V in the non-autoregressive case. In theory, the sequence {Mt}T t=1 can be obtained in O(Td2) by ﬁrst com- puting {KtV⊤ t }T t=1 and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependencyof T steps, where an O(d2) state needs to be processed each step. The sequential dependency not only limits the degree of paral- lelism, but more importantly requires T memory accessin the loop, which usually costs much more time than comput- ing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoreti- cal complexity and actual running time. In practice, we ﬁnd that directly computing the full quadratic attention matrix iseven faster than the re-arranged (linearized) version on both TPUs (Figure 6(a)) and GPUs (Appendix C.1). 3.2. Our Method: Mixed Chunk Attention Based on the strengths and weaknesses of existing linear- complexity attentions, we propose mixed chunk attention, which merges the beneﬁts from both partial attention and linear attention. The high-level idea is illustrated in Figure 4. Below we reformulate GAU to incorporate this idea. Preparation. The input sequence is ﬁrst chunked into G non-overlapping chunks of size C, i.e. [T] →[T/C × C]. Then, Ug ∈RC×e, Vg ∈RC×e and Zg ∈RC×s are produced for each chunk gfollowing the GAU formulation in Eq. (1) and Eq. (4). Next, four types of attention heads Qquad g , Kquad g , Qlin g , Klin g are produced from Zg by applying per-dim scaling and offset (this is very cheap). We will describe how GAU’s attention can be efﬁciently approximated using a local attention plus a global attention. Note all the major tensors Ug, Vg and Zg are shared be- tween the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Qlin g and Klin g (4×sparameters). Local Attention per Chunk. First, a local quadratic at- tention is independently applied to each chunk of length C to produce part of the pre-gating state: ˆVquad g = relu2 ( Qquad g Kquad g ⊤ + b ) Vg. The complexity of this part is O(G×C2 ×d) =O(TCd), which is linear in T given that Cremains constant. Global Attention across Chunks. In addition, a global linear attention mechanism is employed to capture long- range interaction across chunks Non-Causal: ˆVlin g = Qlin g ( G∑ h=1 Klin h ⊤ Vh ) , (7) Causal: ˆVlin g = Qlin g (g−1∑ h=1 Klin h ⊤ Vh ) . (8) Note the summations in Eq. (7) and Eq. (8) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in token- level linear attention by a factor of C(a typical Cis 256 in our experiments), leading to a signiﬁcant training speedup. Finally, ˆVquad g and ˆVlin g are added together, followed by gat- ing and a post-attention projection analogous to Eq. (3): Og = [ Ug ⊙ ( ˆVquad g + ˆVlin g )] Wo. def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bgse/quotesingle.ts1, k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum(/quotesingle.ts1bgcs,bgse→bgce/quotesingle.ts1, q, kv) else: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bse/quotesingle.ts1, k, v) return tf.einsum(/quotesingle.ts1bgcs,bse→bgce/quotesingle.ts1, q, kv) def _local_quadratic_attn(q, k, v, causal): qk = tf.einsum(/quotesingle.ts1bgns,bgms→bgnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 a = causal_mask(a) if causal else a return tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, a, v) def attn(x, v, causal, s=128): # x: [B x G x C x D]; v: [B x G x C x E] z = dense(x, s) v_quad = _local_quadratic_attn( scale_offset(z), scale_offset(z), v, causal) v_lin = _global_linear_attn( scale_offset(z), scale_offset(z), v, causal) return v_quad + v_lin Code 1: Pseudocode for mixed chunk attention. The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1. 3.2.1. D ISCUSSIONS Fast Auto-regressive Training. Importantly, as depicted in Fig. 4 (bottom), thanks to chunking, the sequential de- pendency in the auto-regressive case reduces from T steps in the standard linear attention to G = T/C steps in the chunked version in Eq. (8). Therefore, we observe the auto- regressive training becomes dramatically faster with the chunk size is in {128,256,512}. With the inefﬁciency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and compu- tation of O(Cd2), where the additional constant C comes from the local quadratic attention. On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, in- stead of using the non-overlapping local attention, any par- tial attention variant could be used as a substitute while keeping the chunked linear attention ﬁxed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer (Belt- agy et al., 2020) and BigBird (Zaheer et al., 2020). While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-beneﬁt trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal par- tial attention variant is task-speciﬁc, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention.0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 26 27 28 Latency per step (ms) 29 210 211 212 213 Context length (a) Per-step training latency 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (b) Context length = 512 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (c) Context length = 1024 0 5 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (d) Context length = 2048 0 5 10 15 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 -1.5 -2.5 -3.5 -5.5 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 5: Masked language modeling validation-set results on the C4 dataset — All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. Connections to Combiner. Similar to our method, Com- biner (Ren et al., 2021) also splits the sequence into non- overlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the long- range information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions. 4. Experiments We focus on two of our models that have different com- plexities with respect to the context length. The quadratic- complexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH con- sists of both GAUs and the proposed mixed chunk attention. To demonstrate their efﬁcacy and general applicability, we evaluate them on both bidirectional and auto-regressive se- quence modeling tasks over multiple large-scale datasets. Baselines. First of all, the vanilla Transformer (Vaswani et al., 2017) with GELU activation function (Hendrycks & Gimpel, 2016) is included as a standard baseline for calibra- tion. Despite of being a popular baseline in the literature, we ﬁnd that RoPE (Su et al., 2021) and GLU (Shazeer, 2020) can lead to signiﬁcant performance boosts. We there- fore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity. To demonstrate the advantages of our models on long se- quences, we further compare our models with two notable linear-complexity Transformer variants—Performer (Choro- manski et al., 2020) and Combiner (Ren et al., 2021), where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-beneﬁt trade-off over many other approaches (Ren et al., 2021). To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner- Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE. For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Proﬁler. See Appendix B for detailed settings and model speciﬁcations. 4.1. Bidirectional Language Modeling In BERT (Devlin et al., 2018), masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 29 210 211 212 213 26 28 210 Context length Latency per step (ms) (a) Per-step training latency 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (b) Context length = 512 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (c) Context length = 1024 0 4 8 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (d) Context length = 2048 0 10 20 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 30 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 6: Auto-regressive language modeling validation-set results on the Wiki-40B dataset — All models are sized around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. the C4 dataset (Raffel et al., 2020). We consistently train each model with 218 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. Figure 5(a) shows the latency of each training step for all models at different context lengths. Results for Trans- former+ are omitted for brevity as it lies in between Trans- former and Transformer++. Across all the six models, laten- cies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating lin- ear complexity with respect to context length. FLASH-Quad is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2 × as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in Figures 5(b)-5(f), for all sequence lengths ranging from 512 to 8192, our mod- els always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++’s ﬁnal perplexity at step 125K, FLASH-Quad and FLASH can reduce the train- ing cost by 1.1×–2.5×and 1.0×–4.8×, respectively. It is worth noting that, to the best of our knowledge, FLASH is the only linear-complexity model that achieves perplexity competitive with the fully-augmented Transformers and its quadratic-complexity counterpart. See Appendix C.2 for a detailed quality and speed comparison of all models. 4.2. Auto-regressive Language Modeling For auto-regressive language modeling, we focus on the Wiki-40B (Guo et al., 2020) and PG-19 (Rae et al., 2019) datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model perfor- mance over long context lengths. We train and evaluate all models with 218 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Figure 6(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasingTable 3: Auto-regressive language models on the PG-19 dataset — Latency (Lat.) is measured with 64 TPU-v4 cores. Model Context Length 1024 2048 4096 8192 PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* Transformer+ 44.45 282 1.00 × 43.14 433 1.00 × 42.80 698 1.00 × 43.27 1292 1.00 × Transformer++ 44.47 292 – 43.18 441 – 43.13 712 – 43.26 1272 1.21 × Combiner 46.04 386 – 44.68 376 – 43.99 374 – 44.12 407 – FLASH-Quad 43.40 231 2.18 × 42.01 273 3.29× 41.46 371 3.59× 41.68 560 5.23× FLASH 44.06 234 1.66× 42.17 237 3.85 × 40.72 234 6.75 × 41.07 250 12.12 × * Measured based on time taken to match Transformer+’s ﬁnal quality (at step 125K) on TPU. – Indicates that the speciﬁc model fails to achieve the same perplexity as Transformer+. context lengths in Figures 6(b)-6(f). Similar to the ﬁndings on MLM tasks, our models dominate all other models in terms of quality-training speed for all sequence lengths. Speciﬁcally, FLASH-Quad reduces the training time of Transformer++ by 1.2×to 2.5×and FLASH cuts the com- pute cost by 1.2×to 4.9×while reaching a similar perplex- ity as Transformer++. Between our own models, FLASH closely tracks the perplexity of FLASH-Quad and starts to achieve a better perplexity-cost trade-off when the con- text length goes beyond 2048. Detailed quality and speed comparisons for all models are included in Appendix C.2. For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see Table 10) is used for all mod- els in comparison. The results are summarized in Table 3. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and train- ing time over the augmented Transformers on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the ﬁnal perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23× and 12.12×of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the long- range nature of PG-19 (which consists of books). Similar to our previous experiments, FLASH achieves a lower perplex- ity than all of the full-attention Transformer variants while being signiﬁcantly faster, demonstrating the effectiveness of our efﬁcient attention design. 4.3. Fine-tuning To demonstrate the effectiveness of FLASH over down- stream tasks, we ﬁne-tune our pre-trained models on the TriviaQA dataset (Joshi et al., 2017). Passages in Trivi- aQA can span multiple documents, which challenges the capability of the models in handling long contexts. For a fair and meaningful comparison, we pretrain all models on English Wikipedia (same domain as TriviaQA) with a context length of 4096 and a batch size of 64 for 125k steps. For ﬁne-tuning, we sweep over three different learn- ing rates, including 1e−4, 7e−5, and 5e−5, and report the best validation-set F1 score across these runs. Table 4: Results on TrivialQA with context length 4096 — “PT“ stands for pre-training and “FT“ stands for ﬁne-tuning. All models are comparable in size at around 110M. sstands for the head size of the single-head attention. For FLASH, “ﬁrst-to-all” means that we also let the ﬁrst token in each chunk to attend to the entire sequence using a single-head softmax attention. Latency (Lat.) is measured with 32 TPU-v4 cores. Model PT FT PT / FT PPLX F1 Lat. reduction Transformer+ 3.48 74.2 1.00 ×/ 1.00× Combiner 3.51 67.2 2.78×/ 2.75× FLASH-Quads=128 3.24 72.7 1.89 ×/ 1.79× FLASH-Quads=512 3.12 74.8 1.76×/ 1.67× FLASHs=512 3.23 73.3 2.61 ×/ 2.60× FLASHs=512 + ﬁrst-to-all 3.24 73.9 2.78×/ 2.69× We observe that the ﬁne-tuning results of the FLASH fam- ily can beneﬁt from several minor changes in the model conﬁguration. As shown in Table 4, increasing the head size of FLASH-Quad from 128 to 512 leads to a signiﬁcant boost of 2.1 point in the F1 score with negligible impact on speed. We further identify several other tweaks that improve the linear FLASH variant speciﬁcally, including using a small chunk size (128), disabling gradient clipping during ﬁnetuning, using softmax instead of squared ReLU for the [CLS] token, and (optionally) allowing the ﬁrst to- ken in each chunk to attend to the entire sequence using softmax. With those changes, FLASHs=512 achieves compa- rable quality to Transformer+ (0.3 difference in F1 is within the range of variance) while being 2.8×and 2.7×as fast as Transformer+ in pretraining and ﬁne-tuning, respectively. 4.4. Ablation Studies Signiﬁcance of quadratic & linear components. To bet- ter understand the efﬁcacy of FLASH, we ﬁrst study how much the local quadratic attention and the global linear atten- tion contribute to the performance individually. To this end,-3.2 -3.0 -2.8 FLASH FLASH (LocalOnly) FLASH (GlobalOnly) MC-TFM++ 0 2 4 6 8 -4.6 -4.4 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 TPU-core-days Neg. log pplx (a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 7: Ablation study of the proposed FLASH architecture. we create FLASH (LocalOnly) and FLASH (GlobalOnly) by only keeping the local quadratic attention and the global linear attention in FLASH, respectively. In FLASH (Glob- alOnly), we reduce the chunk size from 256 to 64 to produce more local summaries for the global linear attention. In Fig- ure 7 we see a signiﬁcant gap between the full model and the two variants, suggesting that the linear and global attention are complementary to each other — both are critical to the quality of the proposed mixed chunk attention. Signiﬁcance of GAU. Here we study the importance of using GAU in FLASH. To achieve this,we apply the same idea of mixed chunk attention to Transformer++. We re- fer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC- TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU. Figure 7 shows that FLASH outperforms MC-TFM++ by a large margin (more than 2×speedup when the sequence length is greater than 2048), conﬁrming the importance of GAU in our design. We further look into the perplexity in- crease due to our approximation method in Table 5, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than go- ing from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneﬁcial to weaker/approximate attention mechanisms. Impact of Chunk Size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3. Table 5: Perplexity increases when mixed chunk attention is applied to GAU (→FLASH) or to TFM++ (→MC-TFM++) — Results are reported for MLM and LM with increasing context lengths from 512 to 8192. MLM on C4 512 1024 2048 4096 8192 FLASH-Quad→FLASH 0.0 0.05 0.06 0.07 0.07 TFM++→MC-TFM++ 0.36 0.37 0.49 0.48 0.43 LM on Wiki-40B 512 1024 2048 4096 8192 FLASH-Quad→FLASH -0.05 0.06 0.22 0.30 0.11 TFM++→MC-TFM++ 0.54 0.75 0.86 0.90 0.87 5. Conclusion We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efﬁcient Transformer variants. This is achieved by designing a per- formant layer (gated linear unit) and by combining it with an accelerator-efﬁcient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art. A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks. Acknowledgements The authors would like to thank Gabriel Bender, John Blitzer, Maarten Bosma, Andrew Brock, Ed Chi, Hanjun Dai, Yann N. Dauphin, Pieter-Jan Kindermans and David So for their useful feedback. Weizhe Hua was supported in part by the Facebook fellowship. References Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019. Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan- guage modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 933–941. JMLR.org, 2017. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efﬁcient scaling of language models with mixture- of-experts. arXiv preprint arXiv:2112.06905, 2021. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3–11, 2018. Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 1243–1252. JMLR.org, 2017. Guo, M., Dai, Z., Vrandecic, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In LREC 2020, 2020. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y ., and Liu, W. Ccnet: Criss-cross attention for semantic segmen- tation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603–612, 2019. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with it- erative attention. In International Conference on Machine Learning, pp. 4651–4664. PMLR, 2021. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi- aQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, Vancouver, Canada, July 2017. Association for Compu- tational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020. Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efﬁcient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .- X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecast- ing. Advances in Neural Information Processing Systems, 32:5243–5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V . Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y ., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modiﬁcations transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. CoRR, abs/1910.05895, 2019. URL http://arxiv.org/ abs/1910.05895. Peng, H. et al. Random feature attention. In ICLR, 2021. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur- mans, D., and Dai, B. Combiner: Full attention trans- former with sparse computation cost. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=MQQeeDiO5vv. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient content-based sparse attention with routing transform- ers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. Shazeer, N. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/ abs/2002.05202. So, D. R., Ma ´nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V . Primer: Searching for efﬁcient transformers for language modeling. NeurIPS, 2021. Su, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2021. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul- shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y ., et al. Lamda: Language models for dialog appli- cations. arXiv preprint arXiv:2201.08239, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al- berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer se- quences. In NeurIPS, 2020.A. Connections to Combiner To capture long-term information, Combiner (Ren et al., 2021) additionally summarizes each chunk into summary key and value vectors Ksum,V sum ∈RT/C ×d and concatenate them into the local quadratic attention, i.e. ˆVg = Softmax ( Q[Kg; Ksum] ) [Vg; Vsum]. Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix Klin h ⊤ Vh of size O(sd) which is stimes larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners. Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra[T/C ×T/C] attention matrix to combine the chunk summaries, e.g. Alin = relu2 ( QsumKsum⊤+ bsum ) , ˆVlin g = Qlin g [T/C∑ h=1 alin gh ( Klin h ⊤ Vh )] . We indeed brieﬂy experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C+ T/C)d2) which is length-dependent and no longer constant. Hence, we do not include this feature in our default conﬁguration. B. Experimental Setup B.1. Hyperparameters Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in Table 6. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. Table 6: Hyperparameters for MLM pretraining on C4. MLM Results (Figure 5) Data C4 Sequence length 512 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Chunk size 256 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 * Applied to all models except the vanilla Transformer. Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in Table 7. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.Table 7: Hyperparameters for LM pretraining on Wiki-40B and PG-19. LM Results (Figure 6) LM Results (Table 3) Data Wiki-40B PG-19 Sequence length 512 - 8192 1024 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 Chunk size 256 512 * Applied to all models except the vanilla Transformer. B.2. Model Speciﬁcations Detailed speciﬁcations of all models used in our experiments are summarized in Tables 8, 9, and 10. In the experiments, SiLU/Swish (Elfwing et al., 2018; Hendrycks & Gimpel, 2016; Ramachandran et al., 2017) is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU (Hendrycks & Gimpel, 2016) in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model. Table 8: Model conﬁgurations for MLM experiments on the C4 dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type3 ScaleNorm ScaleNorm LayerNorm ScaleNorm ScaleNorm ScaleNorm ScaleNorm Absolute position emb. ScaledSin4 ScaledSin4 Learnable5 ScaledSin4 ScaledSin4 ScaledSin4 ScaledSin4 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 6 12+126 12+126 12+126 12+126 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleNorm and LayerNorm are proposed by Nguyen & Salazar (2019) and Ba et al. (2016), respectively. 4 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 5 The learnable position embedding is proposed by Gehring et al. (2017). 6 The model is consist of 12 attention layers and 12 FFN layers. C. Additional Experimental Results Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in Table 11) and the ablation study of chunk size for FLASH (in Figure 8).Table 9: Model conﬁgurations for LM experiments on the Wiki-40B dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 Learnable4 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 5 12+125 12+125 12+125 12+125 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The learnable position embedding is proposed by Gehring et al. (2017). 5 The model is consist of 12 attention layers and 12 FFN layers. Table 10: Model conﬁgurations for LM experiments on the PG-19 dataset in Section 4. FLASH-Quad FLASH Transformer+ Transformer++ Combiner # of attention heads 1 1 16 16 16 Attention kernel relu2 relu2 softmax softmax softmax Attention type Quadratic Mixed Chunk Quadratic Quadratic Rowmajor-Axial FFN type GAU1 GAU1 MLP GLU MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE RoPE RoPE RoPE # of layers 72 72 36+36 4 36+364 36+364 Hidden size 1024 1024 1024 1024 1024 Expansion rate 2 2 4 4 4 Chunk size – 512 – – 512 Params (M) 496 496 486 486 562 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The model is consist of 36 attention layers and 36 FFN layers. Table 11: Comparison of latency for each training step of auto-regressive language modeling on Wiki-40B using a single Nvidia Tesla V100 GPU — Latency is reported in millisecond. OOM stands for the CUDA out of memory error. Performer-Matmul implements the cumulative sum (cumsum) using matrix multiplication. Context length ×Batch size Model 512 ×4 1024 ×2 2048 ×1 4096 ×1 Transformer++ 222.4 243.9 315.0 OOM Performer 823.0 827.4 799.8 OOM Performer-Matmul 697.4 701.7 688.9 OOM FLASH 254.4 235.0 242.8 452.9C.1. Auto-regressive Training on GPU We observe that the inefﬁciency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in Table 11, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism. C.2. Tabular MLM and LM Results We summarize the experimental results of MLM on C4 and LM on Wiki-40B in Tables 12 and 13. Table 12: Bidirectional/masked language models on the C4 dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 4.517 47.7 4.436 63.9 4.196 90.9 4.602 142.5 4.8766 252.7 Transformer+ 4.283 48.8 4.151 64.4 4.032 91.5 3.989 142.9 3.986 252.9 Transformer++ 4.205 47.6 4.058 64.6 3.920 91.6 3.876 143.4 3.933 252.1 Performer 5.897 37.2 6.324 37.6 8.032 39.1 12.622 36.9 102.980 40.9 Combiner 4.449 67.2 4.317 66.4 4.238 66.4 4.195 68.3 4.225 77.3 FLASH-Quad 4.176 43.7 3.964 50.1 3.864 61.7 3.828 84.9 3.830 132.1 FLASH 4.172 51.2 4.015 50.1 3.928 51.4 3.902 50.7 3.897 59.9 Table 13: Auto-regressive language models on the Wiki-40B dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 17.341 54.0 19.808 70.9 18.154 96.3 17.731 149.1 18.254 260.7 Transformer+ 16.907 55.6 15.999 70.3 15.653 96.1 15.515 149.3 15.478 261.9 Transformer++ 16.835 54.7 15.943 70.9 15.489 96.6 15.282 149.2 15.254 261.0 Performer 18.989 1439.7 18.520 1386.9 18.547 1518.9 18.987 1526.7 19.923 1526.8 Combiner 17.338 75.5 16.710 74.4 16.344 71.8 16.171 71.7 16.119 77.9 FLASH-Quad 16.633 54.1 15.879 59.5 15.305 71.3 14.955 96.1 14.998 141.3 FLASH 16.581 57.2 15.935 56.9 15.525 56.7 15.259 57.0 15.109 62.5 C.3. Ablation Study of Chunk Size The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefﬁcient auto-regressive training. Figure 8 shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K. D. Pseudocode For FLASH-Quad and FLASH We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8.(a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 8: Ablation study of the chunk size (C) of FLASH for context lengths from 1K to 8K. def _get_scaledsin(embeddings): \"\"\"Create sinusoidal position embedding with a scaling factor.\"\"\" hidden_size = int(embeddings.shape[=1]) pos = tf.range(tf.shape(embeddings)[1]) pos = tf.cast(pos, tf.float32) half_d = hidden_size // 2 freq_seq = tf.cast(tf.range(half_d), tf.float32) / float(half_d) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1s,d→sd/quotesingle.ts1, pos, inv_freq) scaledsin = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis==1) scalar = tf.get_variable( /quotesingle.ts1scaledsin_scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1 / hidden_size ∗∗ 0.5)) scaledsin ∗= scalar return scaledsin Code 2: Pseudocode for ScaledSin absolute position embedding. def rope(x, axis): \"\"\"RoPE position embedding.\"\"\" shape = x.shape.as_list() if isinstance(axis, int): axis = [axis] spatial_shape = [shape[i] for i in axis] total_len = 1 for i in spatial_shape: total_len ∗= i position = tf.reshape( tf.cast(tf.range(total_len, delta=1.0), tf.float32), spatial_shape) for i in range(axis[=1] + 1, len(shape) = 1, 1): position = tf.expand_dims(position, axis==1) half_size = shape[=1] // 2 freq_seq = tf.cast(tf.range(half_size), tf.float32)/float(half_size) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1...,d→...d/quotesingle.ts1, position, inv_freq) sin = tf.sin(sinusoid) cos = tf.cos(sinusoid) x1, x2 = tf.split(x, 2, axis==1) return tf.concat([x1 ∗ cos = x2 ∗ sin, x2 ∗ cos + x1 ∗ sin], axis==1) Code 3: Pseudocode for RoPE.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def rel_pos_bias(n): \"\"\"Relative position bias.\"\"\" if n < 512: # Construct Toeplitz matrix directly when the sequence length is less than 512. w = tf.get_variable( /quotesingle.ts1weight/quotesingle.ts1, shape=[2 ∗ n = 1], dtype=tf.float32, initializer=WEIGHT_INITIALIZER) t = tf.pad(w, [[0, n]]) t = tf.tile(t, [n]) t = t[..., :=n] t = tf.reshape(t, [n, 3 ∗ n = 2]) r = (2 ∗ n = 1) // 2 t = t[..., r:=r] else: # Construct Toeplitz matrix using RoPE when the sequence length is over 512. a = tf.get_variable( /quotesingle.ts1a/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) b = tf.get_variable( /quotesingle.ts1b/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) a = rope(tf.tile(a[None, :], [n, 1]), axis=0) b = rope(tf.tile(b[None, :], [n, 1]), axis=0) t = tf.einsum(/quotesingle.ts1mk,nk→mn/quotesingle.ts1, a, b) return t Code 4: Pseudocode for relative position bias. def norm(x, begin_axis==1, eps=1e=5, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1): \"\"\"Normalization layer.\"\"\" shape = x.shape.as_list() axes = list(range(len(shape)))[begin_axis:] if norm_type == /quotesingle.ts1layer_norm/quotesingle.ts1: mean, var = tf.nn.moments(x, axes, keepdims=True) x = (x = mean) ∗ tf.rsqrt(var + eps) gamma = tf.get_variable( /quotesingle.ts1gamma/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.ones()) beta = tf.get_variable( /quotesingle.ts1beta/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.zeros()) return gamma ∗ x + beta elif norm_type == /quotesingle.ts1scale_norm/quotesingle.ts1: mean_square =tf.reduce_mean(tf.math.square(x), axes, keepdims=True) x = x ∗ tf.rsqrt(mean_square + eps) scalar = tf.get_variable(/quotesingle.ts1scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1.0)) return scale ∗ x Code 5: Pseudocode for LayerNorm and ScaleNorm.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def GAU(x, causal, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"GAU block. Input shape: batch size x sequence length x model size \"\"\" seq_len = tf.shape(x)[1] d = int(x.shape[=1]) e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query (q) and Key (k) from base. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[2, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[2, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=1) q, k = tf.unstack(base, axis==2) # Calculate the quadratic attention. qk = tf.einsum(/quotesingle.ts1bnd,bmd→bnm/quotesingle.ts1, q, k) bias = rel_pos_bias(seq_len) kernel = tf.math.square(tf.nn.relu(qk / seq_len + bias)) # Apply the causal mask for auto=regressive tasks. if causal: causal_mask = tf.linalg.band_part( tf.ones([seq_len, seq_len], dtype=x.dtype), num_lower==1, num_upper=0) kernel ∗= causal_mask x = u ∗ tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, kernel, v) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 6: Pseudocode for GAU (FLASH-Quad). def segment_ids_to_mask(segment_ids, causal=False): \"\"\"Generate the segment mask from the segment ids. The segment mask is used to remove the attention between tokens in different documents. \"\"\" min_ids, max_ids = tf.reduce_min(segment_ids, axis==1), tf.reduce_max(segment_ids, axis==1) # 1.0 indicates in the same group and 0.0 otherwise mask = tf.logical_and( tf.less_equal(min_ids[:, :, None], max_ids[:, None, :]), tf.greater_equal(max_ids[:, :, None], min_ids[:, None, :])) mask = tf.cast(mask, tf.float32) if causal: g = tf.shape(min_ids)[1] causal_mask = 1.0 = tf.linalg.band_part( tf.ones([g, g], dtype=tf.float32), num_lower=0, num_upper==1) mask ∗= causal_mask mask = tf.math.divide_no_nan(mask, tf.reduce_sum(mask, axis==1, keepdims=True)) return mask Code 7: Pseudocode for generating segment mask.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def FLASH(x, causal, segment_ids, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"FLASH block. Input shape: batch size x num chunks x chunk length x model size \"\"\" _, g, n, d = x.shape.as_list() e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query and Key for both quadratic and linear attentions. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[4, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[4, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=[1, 2]) quad_q, quad_k, lin_q, lin_k = tf.unstack(base, axis==2) if causal: # Linear attention part. lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids, causal=True) cum_lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, cum_lin_kv) # Quadratic attention part. quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu(quad_qk / n + bias)) # Apply the causal mask for auto=regressive tasks. causal_mask = tf.linalg.band_part(tf.ones([n, n], dtype=x.dtype), num_lower==1, num_upper=0) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel ∗ causal_mask, v) else: # Linear attention part lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids) lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, lin_kv) # Quadratic attention part quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu((quad_qk / n + bias)) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel, v) x = u ∗ (quadratic + linear) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 8: Pseudocode for FLASH.",
      "meta_data": {
        "arxiv_id": "2202.10447v2",
        "authors": [
          "Weizhe Hua",
          "Zihang Dai",
          "Hanxiao Liu",
          "Quoc V. Le"
        ],
        "published_date": "2022-02-21T18:59:38Z",
        "pdf_url": "https://arxiv.org/pdf/2202.10447v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the quadratic complexity and practical inefficiencies of Transformers for long sequences, proposing FLASH (Fast Linear Attention with a Single Head). FLASH introduces a Gated Attention Unit (GAU) that allows for simpler, weaker attention mechanisms with minimal quality loss, and a novel mixed chunk attention strategy that combines local quadratic attention within chunks and global linear attention across chunks. This approach achieves linear scalability over context size on modern accelerators while matching or exceeding the perplexity of fully-augmented Transformers, demonstrating significant training speedups (up to 12.1x on PG-19) across various language modeling tasks and context lengths (512-8K).",
        "methodology": "The methodology is developed in two steps: 1) **Gated Attention Unit (GAU)**: A new layer design incorporating a gating mechanism that unifies attention and GLU, sharing computation and making the model less reliant on high-precision attention. It allows a single-head, softmax-free attention without quality degradation. 2) **Mixed Chunk Attention**: An efficient approximation method for the quadratic attention in GAU. It partitions input sequences into non-overlapping chunks, applying precise quadratic attention within each chunk and fast linear attention across chunks. For auto-regressive tasks, this reduces sequential dependency from T steps to T/C (where C is chunk size), dramatically speeding up training and maintaining constant O(Cd^2) decoding memory and computation per step. The core idea is that GAU naturally enables higher-quality approximation.",
        "experimental_setup": "Experiments were conducted on bidirectional (Masked Language Modeling - MLM) and auto-regressive (Language Modeling - LM) tasks. Datasets included C4 for MLM and Wiki-40B and PG-19 for LM. Models scaled from 110M to 500M parameters. Context lengths varied from 512 to 8192 tokens. Training involved 125K steps with 2^18 tokens per batch. Hardware used included 64 TPU-v4 cores for most experiments and a single Nvidia Tesla V100 GPU for additional auto-regressive training benchmarks. Baselines included vanilla Transformer, Transformer+ (with RoPE), Transformer++ (with RoPE and GLU), Performer (ReLU-kernel variant with RoPE), and Combiner (rowmajor-axial variant with RoPE). Evaluation metrics were negative log perplexity for language modeling and F1 score for fine-tuning on TriviaQA. Ablation studies investigated the contributions of local vs. global attention and GAU, as well as the impact of chunk size.",
        "limitations": "The paper notes that while overlapping local attention (e.g., as in Longformer or BigBird) consistently improves quality, it introduces memory re-formatting operations that can harm actual running speed on TPUs, making non-overlapping local attention with chunked linear attention a preferred cost-benefit trade-off in some preliminary experiments. It also states that the optimal partial attention variant is task-specific. The choice of chunk size can affect both quality and training cost, requiring hyperparameter search for optimal performance. Additionally, the paper mentions that the original linear attention (without chunking) suffered from severe inefficiency during auto-regressive training due to RNN-style sequential state updates and T memory accesses, making it slower than full quadratic attention in practice.",
        "future_research_directions": "Future work will involve investigating the scaling laws of the new FLASH model family and evaluating its performance on various downstream tasks beyond the scope of this paper."
      }
    },
    {
      "title": "Transformer Quality in Linear Time",
      "abstract": "We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.",
      "full_text": "Transformer Quality in Linear Time Weizhe Hua* 1 2 Zihang Dai * 2 Hanxiao Liu * 2 Quoc V . Le2 Abstract We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head atten- tion with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH3, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9×on Wiki-40B and 12.1× on PG-19 for auto-regressive language modeling, and 4.8×on C4 for masked language modeling. 1. Introduction Transformers (Vaswani et al., 2017) have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language (Devlin et al., 2018; Brown et al., 2020) and vision (Dosovitskiy et al., 2020). Although they have been growing in model size, most Trans- formers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications. Many techniques have been proposed to speedup Transform- ers over extended context via more efﬁcient attention mech- anisms (Child et al., 2019; Dai et al., 2019; Rae et al., 2019; Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Roy et al., 2021; Jaegle et al., 2021). Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in *Equal contribution 1Cornell University 2Google Re- search, Brain Team. Correspondence to: Weizhe Hua <wh399@cornell.edu>, Zihang Dai <zihangd@google.com>, Hanxiao Liu <hanxiaol@google.com>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 0 5 10 15 20 25 30 TPU-core-days -2.8 -3.0 -3.2 Neg. log pplx FLASH TFM+ + TFM 4.9x 25.6x Speedup Length over TFM over TFM++ 512 1.8 × 1.2× 1024 9.0 × 1.3× 2048 8.9 × 1.6× 4096 13.1 × 2.7× 8192 25.6 × 4.9× Figure 1: TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki- 40B — All models are comparable in size at around 110M and trained for 125K steps with 218 tokens per batch. state-of-the-art systems. Here we examine this issue from a practical perspective, and ﬁnd existing efﬁcient attention methods suffer from at least one of the following drawbacks: • Inferior Quality. Our studies reveal that vanilla Trans- formers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure 1). Existing efﬁcient attention methods often incur signiﬁcant quality drop compared to augmented Trans- formers, and this drop outweighs their efﬁciency beneﬁts. • Overhead in Practice . As efﬁcient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs. • Inefﬁcient Auto-regressive Training. Most attention lin- earization techniques enjoy fast decoding during infer- ence, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large number of steps, making it infeasible to fully leverage the strength of modern accelerators during training. 3FLASH = Fast Linear Attention with a Single Head arXiv:2202.10447v2  [cs.LG]  27 Jun 2022Input Concat V softmax(QK+B) Gated Linear Unit  + Multi-Head Self-Attention Gated Attention Unit (Ours) Input Dense Dense Dense Dense Q K V V relu2(QK+B) Input Dense Dense Dense V Q & K def scale_offset(x): gamma = var(x.shape[=1:]) beta = var(x.shape[=1:]) return x ∗ gamma + beta def attn(x, v, s=128): z = dense(x, s) q, k = scale_offset(z), scale_offset(z) qk = tf.einsum(/quotesingle.ts1bns,bms→bnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 return tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, a, v) def gated_attn_unit(x, d=768, e=1536): shortcut, x = x, norm(x) u, v = dense(x, e), dense(x, e) x = u ∗ attn(x, v) return dense(x, d) + shortcut Figure 2: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity. We address the above issues by developing a new model fam- ily that, for the ﬁrst time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys lin- ear scalability over the context size on modern accelerators. Unlike existing efﬁcient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which nat- urally enables higher-quality approximation. Speciﬁcally, our model, named FLASH, is developed in two steps: First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit(GAU) in Figure 2. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of atten- tion. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss. We then propose an efﬁcient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to ﬁrst group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in Figure 4. We further describe how an accelerator-efﬁcient implementation can be naturally de- rived from this formulation, achieving linear scalability in practice with only a few lines of code change. We conduct extensive experiments to demonstrate the efﬁ- cacy of FLASH over a variety of tasks (masked and auto- regressive language modeling), datasets (C4, Wiki-40B, PG- 19) and model scales (110M to 500M). Remarkably, FLASH is competitive with fully-augmented Transformers (Trans- former++) in quality across a wide range of context sizes of practical interest (512–8K), while achieving linear scala- bility on modern hardware accelerators. For example, with comparable quality, FLASH achieves a speedup of 1.2×– 4.9×for language modeling on Wiki-40B and a speedup of 1.0×–4.8×for masked language modeling on C4 over Transformer++. As we further scale up to PG-19 (Rae et al., 2019), FLASH reduces the training cost of Transformer++ by up to 12.1×and achieves signiﬁcant gain in quality. 2. Gated Attention Unit Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers: Vanilla MLP. Let X ∈RT×d be the representations over T tokens. The output for Transformer’s MLP can be formu- lated as O= φ(XWu)Wo where Wu ∈Rd×e, Wo ∈Re×d. Here ddenotes the model size, edenotes the expanded inter- mediate size, and φis an element-wise activation function. Gated Linear Unit (GLU). This is an improved MLP augmented with gating (Dauphin et al., 2017). GLU has been proven effective in many cases (Shazeer, 2020; Narang et al., 2021) and is used in state-of-the-art Transformer language models (Du et al., 2021; Thoppilan et al., 2022). U = φu(XWu), V = φv(XWv) ∈RT×e (1) O= (U ⊙V)Wo ∈RT×d (2) where ⊙stands for element-wise multiplication. In GLU, each representation ui is gated by another representation vi associated with the same token.0.0 0.2 0.4 Step/sec/TPU-core 3.1 3.0 2.9 2.8 2.7 Neg. log pplx 42M 110M 335M 41M 105M 316M LM (Wiki-40B) MHSA+MLP MHSA+GLU GAU 0.0 0.5 1.0 Steps/sec/TPU-core 1.7 1.6 1.5 1.4 1.3 1.2 Neg. log pplx 42M 110M 335M 41M 105M 316M Masked LM (C4) MHSA+MLP MHSA+GLU GAU Layer Type # of Layers d MHSA+MLP 8+8 512 12+12 768 24+24 1024 MHSA+GLU 8+8 512 12+12 768 24+24 1024 GAU 15 512 22 768 46 1024 Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512). Gated Attention Unit (GAU). The key idea is to formu- late attention and GLU as a uniﬁed layer and to share their computation as much as possible (Figure 2). This not only results in higher param/compute efﬁciency, but also natu- rally enables a powerful attentive gating mechanism. Specif- ically, GAU generalizes Eq. (2) in GLU as follows: O= (U ⊙ˆV)Wo where ˆV = AV (3) where A∈RT×T contains token-token attention weights. Unlike GLU which always uses vi to gate ui (both asso- ciated with the same token), our GAU replaces vi with a potentially more relevant representation ˆvi = ∑ j aijvj “re- trieved” from all available tokens using attention. The above will reduce to GLU when Ais an identity matrix. Consistent with the ﬁndings in Liu et al. (2021), the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss: Z = φz(XWz) ∈RT×s (4) A= relu2 ( Q(Z)K(Z)⊤+ b ) ∈RT×T (5) Modiﬁcations PPLX (LM/MLM) Params (M) original GAU 16.78 / 4.23 105 relu2 − →softmax 17.04 / 4.31 105 single-head − →multi-head 17.76 / 4.48 105 no gating 17.45 / 4.58 131 Table 1: Impact of various modiﬁcations on GAU. where Z is a shared representation ( s ≪d)4, Qand K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay- erNorms), and bis the relative position bias. We also ﬁnd the softmax in MHSA can be simpliﬁed as a regular activa- tion function in the case of GAU5. The GAU layer and its 4Unless otherwise speciﬁed, we set s=128 in this work. 5We use squared ReLU (So et al., 2021) throughout this paper, which empirically works well on language tasks. Modiﬁcations PPLX (LM/MLM) Params (M) original MHSA 16.87 / 4.35 110 softmax − →relu2 17.15 / 4.77 110 multi-head − →single-head 17.89 / 4.73 110 add gating 17.25 / 4.43 106 Table 2: Impact of various modiﬁcations on MHSA. pseudocode are illustrated in Figure 2. Unlike Transformer’s MHSA which comes with4d2 param- eters, GAU’s attention introduces only a single small dense matrix Wz with dsparameters on top of GLU (scalars and offsets in Qand Kare negligible). By setting e = 2dfor GAU, this compact design allows us to replace each Trans- former block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed. GAU vs. Transformers. Figure 3 shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention. Layer Ablations. In Table 1 & 2 we show that both GAUs and Transformers are locally optimal on their own. 3. Fast Linear Attention with GAU There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences: • First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention with- out quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.• In addition, the number of attention modules is naturally doubled with GAU — recall MLP+MHSA ≈2×GAU in terms of cost (Section 2). Since approximate attention usu- ally requires more layers to capture full dependency (Dai et al., 2019; Child et al., 2019), this property also makes GAU more appealing in handling long sequences. With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformer- level quality in linear time on long sequences. 3.1. Existing Linear-Complexity Variants Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/s- parse patterns, including local window (Dai et al., 2019; Rae et al., 2019), local+sparse (Child et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), axial (Ho et al., 2019; Huang et al., 2019), learnable patterns through hashing (Kitaev et al., 2020) or clustering (Roy et al., 2021). Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical beneﬁts (speed and RAM efﬁciency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model. Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decompos- ing the attention matrix and then re-arranging the order of matrix multiplications (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021). Schematically, the linear attention can be expressed as ˆVlin = Q ( K⊤V )    Rd×d approx −−−→ˆVquad = Softmax ( QK⊤)    RT×T V where Q,K,V ∈RT×d are the query, key and value rep- resentations, respectively. Re-arranging the computation reduces the complexity w.r.tT from quadratic to linear. Another desirable property of linear attention is itsconstant6 computation and memory for each auto-regressive decoding step at inference time. To see that, deﬁne Mt = K⊤ :t V:t and notice that the computation of Mt can be fully incremental: Mt = Mt−1 + KtV⊤ t (6) 6Constant is with respective to the sequence length T. cumsum cumsum Figure 4: (top) Quadratic attention, (mid) Linear attention, (bottom) Proposed mixed chunk attention with a chunk size (C) of 2 (Cis always greater than or equal to 128 in our ex- periments). Our method signiﬁcantly reduces the compute in quadratic attention (red links), while requiring substan- tially less RNN-style steps (green squares) in conventional linear attention. This means we only need to maintain a cache with constant O(d2) memory and whenever a new input arrives at time stamp t, only constant O(d2) computation is required to accumulate KtV⊤ t into Mt−1 and get Mt. On the contrary, full quadratic attention requires linear O(Td) computation and memory for each decoding step, as each new input has to attend to all the previous steps. However, on the other hand, re-arranging the computation in linear attention leads to a severe inefﬁciency during auto- regressive training. As shown in Fig. 4 (mid), due to the causal constraint for auto-regressive training, the query vec- tor at each time step Qt corresponds to a different cache value Mt = K⊤ :t V:t. This requires the model to compute and cache T different values {Mt}T t=1 instead of only one value K⊤V in the non-autoregressive case. In theory, the sequence {Mt}T t=1 can be obtained in O(Td2) by ﬁrst com- puting {KtV⊤ t }T t=1 and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependencyof T steps, where an O(d2) state needs to be processed each step. The sequential dependency not only limits the degree of paral- lelism, but more importantly requires T memory accessin the loop, which usually costs much more time than comput- ing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoreti- cal complexity and actual running time. In practice, we ﬁnd that directly computing the full quadratic attention matrix iseven faster than the re-arranged (linearized) version on both TPUs (Figure 6(a)) and GPUs (Appendix C.1). 3.2. Our Method: Mixed Chunk Attention Based on the strengths and weaknesses of existing linear- complexity attentions, we propose mixed chunk attention, which merges the beneﬁts from both partial attention and linear attention. The high-level idea is illustrated in Figure 4. Below we reformulate GAU to incorporate this idea. Preparation. The input sequence is ﬁrst chunked into G non-overlapping chunks of size C, i.e. [T] →[T/C × C]. Then, Ug ∈RC×e, Vg ∈RC×e and Zg ∈RC×s are produced for each chunk gfollowing the GAU formulation in Eq. (1) and Eq. (4). Next, four types of attention heads Qquad g , Kquad g , Qlin g , Klin g are produced from Zg by applying per-dim scaling and offset (this is very cheap). We will describe how GAU’s attention can be efﬁciently approximated using a local attention plus a global attention. Note all the major tensors Ug, Vg and Zg are shared be- tween the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Qlin g and Klin g (4×sparameters). Local Attention per Chunk. First, a local quadratic at- tention is independently applied to each chunk of length C to produce part of the pre-gating state: ˆVquad g = relu2 ( Qquad g Kquad g ⊤ + b ) Vg. The complexity of this part is O(G×C2 ×d) =O(TCd), which is linear in T given that Cremains constant. Global Attention across Chunks. In addition, a global linear attention mechanism is employed to capture long- range interaction across chunks Non-Causal: ˆVlin g = Qlin g ( G∑ h=1 Klin h ⊤ Vh ) , (7) Causal: ˆVlin g = Qlin g (g−1∑ h=1 Klin h ⊤ Vh ) . (8) Note the summations in Eq. (7) and Eq. (8) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in token- level linear attention by a factor of C(a typical Cis 256 in our experiments), leading to a signiﬁcant training speedup. Finally, ˆVquad g and ˆVlin g are added together, followed by gat- ing and a post-attention projection analogous to Eq. (3): Og = [ Ug ⊙ ( ˆVquad g + ˆVlin g )] Wo. def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bgse/quotesingle.ts1, k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum(/quotesingle.ts1bgcs,bgse→bgce/quotesingle.ts1, q, kv) else: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bse/quotesingle.ts1, k, v) return tf.einsum(/quotesingle.ts1bgcs,bse→bgce/quotesingle.ts1, q, kv) def _local_quadratic_attn(q, k, v, causal): qk = tf.einsum(/quotesingle.ts1bgns,bgms→bgnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 a = causal_mask(a) if causal else a return tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, a, v) def attn(x, v, causal, s=128): # x: [B x G x C x D]; v: [B x G x C x E] z = dense(x, s) v_quad = _local_quadratic_attn( scale_offset(z), scale_offset(z), v, causal) v_lin = _global_linear_attn( scale_offset(z), scale_offset(z), v, causal) return v_quad + v_lin Code 1: Pseudocode for mixed chunk attention. The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1. 3.2.1. D ISCUSSIONS Fast Auto-regressive Training. Importantly, as depicted in Fig. 4 (bottom), thanks to chunking, the sequential de- pendency in the auto-regressive case reduces from T steps in the standard linear attention to G = T/C steps in the chunked version in Eq. (8). Therefore, we observe the auto- regressive training becomes dramatically faster with the chunk size is in {128,256,512}. With the inefﬁciency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and compu- tation of O(Cd2), where the additional constant C comes from the local quadratic attention. On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, in- stead of using the non-overlapping local attention, any par- tial attention variant could be used as a substitute while keeping the chunked linear attention ﬁxed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer (Belt- agy et al., 2020) and BigBird (Zaheer et al., 2020). While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-beneﬁt trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal par- tial attention variant is task-speciﬁc, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention.0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 26 27 28 Latency per step (ms) 29 210 211 212 213 Context length (a) Per-step training latency 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (b) Context length = 512 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (c) Context length = 1024 0 5 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (d) Context length = 2048 0 5 10 15 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 -1.5 -2.5 -3.5 -5.5 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 5: Masked language modeling validation-set results on the C4 dataset — All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. Connections to Combiner. Similar to our method, Com- biner (Ren et al., 2021) also splits the sequence into non- overlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the long- range information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions. 4. Experiments We focus on two of our models that have different com- plexities with respect to the context length. The quadratic- complexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH con- sists of both GAUs and the proposed mixed chunk attention. To demonstrate their efﬁcacy and general applicability, we evaluate them on both bidirectional and auto-regressive se- quence modeling tasks over multiple large-scale datasets. Baselines. First of all, the vanilla Transformer (Vaswani et al., 2017) with GELU activation function (Hendrycks & Gimpel, 2016) is included as a standard baseline for calibra- tion. Despite of being a popular baseline in the literature, we ﬁnd that RoPE (Su et al., 2021) and GLU (Shazeer, 2020) can lead to signiﬁcant performance boosts. We there- fore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity. To demonstrate the advantages of our models on long se- quences, we further compare our models with two notable linear-complexity Transformer variants—Performer (Choro- manski et al., 2020) and Combiner (Ren et al., 2021), where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-beneﬁt trade-off over many other approaches (Ren et al., 2021). To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner- Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE. For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Proﬁler. See Appendix B for detailed settings and model speciﬁcations. 4.1. Bidirectional Language Modeling In BERT (Devlin et al., 2018), masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 29 210 211 212 213 26 28 210 Context length Latency per step (ms) (a) Per-step training latency 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (b) Context length = 512 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (c) Context length = 1024 0 4 8 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (d) Context length = 2048 0 10 20 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 30 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 6: Auto-regressive language modeling validation-set results on the Wiki-40B dataset — All models are sized around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. the C4 dataset (Raffel et al., 2020). We consistently train each model with 218 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. Figure 5(a) shows the latency of each training step for all models at different context lengths. Results for Trans- former+ are omitted for brevity as it lies in between Trans- former and Transformer++. Across all the six models, laten- cies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating lin- ear complexity with respect to context length. FLASH-Quad is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2 × as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in Figures 5(b)-5(f), for all sequence lengths ranging from 512 to 8192, our mod- els always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++’s ﬁnal perplexity at step 125K, FLASH-Quad and FLASH can reduce the train- ing cost by 1.1×–2.5×and 1.0×–4.8×, respectively. It is worth noting that, to the best of our knowledge, FLASH is the only linear-complexity model that achieves perplexity competitive with the fully-augmented Transformers and its quadratic-complexity counterpart. See Appendix C.2 for a detailed quality and speed comparison of all models. 4.2. Auto-regressive Language Modeling For auto-regressive language modeling, we focus on the Wiki-40B (Guo et al., 2020) and PG-19 (Rae et al., 2019) datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model perfor- mance over long context lengths. We train and evaluate all models with 218 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Figure 6(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasingTable 3: Auto-regressive language models on the PG-19 dataset — Latency (Lat.) is measured with 64 TPU-v4 cores. Model Context Length 1024 2048 4096 8192 PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* Transformer+ 44.45 282 1.00 × 43.14 433 1.00 × 42.80 698 1.00 × 43.27 1292 1.00 × Transformer++ 44.47 292 – 43.18 441 – 43.13 712 – 43.26 1272 1.21 × Combiner 46.04 386 – 44.68 376 – 43.99 374 – 44.12 407 – FLASH-Quad 43.40 231 2.18 × 42.01 273 3.29× 41.46 371 3.59× 41.68 560 5.23× FLASH 44.06 234 1.66× 42.17 237 3.85 × 40.72 234 6.75 × 41.07 250 12.12 × * Measured based on time taken to match Transformer+’s ﬁnal quality (at step 125K) on TPU. – Indicates that the speciﬁc model fails to achieve the same perplexity as Transformer+. context lengths in Figures 6(b)-6(f). Similar to the ﬁndings on MLM tasks, our models dominate all other models in terms of quality-training speed for all sequence lengths. Speciﬁcally, FLASH-Quad reduces the training time of Transformer++ by 1.2×to 2.5×and FLASH cuts the com- pute cost by 1.2×to 4.9×while reaching a similar perplex- ity as Transformer++. Between our own models, FLASH closely tracks the perplexity of FLASH-Quad and starts to achieve a better perplexity-cost trade-off when the con- text length goes beyond 2048. Detailed quality and speed comparisons for all models are included in Appendix C.2. For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see Table 10) is used for all mod- els in comparison. The results are summarized in Table 3. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and train- ing time over the augmented Transformers on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the ﬁnal perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23× and 12.12×of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the long- range nature of PG-19 (which consists of books). Similar to our previous experiments, FLASH achieves a lower perplex- ity than all of the full-attention Transformer variants while being signiﬁcantly faster, demonstrating the effectiveness of our efﬁcient attention design. 4.3. Fine-tuning To demonstrate the effectiveness of FLASH over down- stream tasks, we ﬁne-tune our pre-trained models on the TriviaQA dataset (Joshi et al., 2017). Passages in Trivi- aQA can span multiple documents, which challenges the capability of the models in handling long contexts. For a fair and meaningful comparison, we pretrain all models on English Wikipedia (same domain as TriviaQA) with a context length of 4096 and a batch size of 64 for 125k steps. For ﬁne-tuning, we sweep over three different learn- ing rates, including 1e−4, 7e−5, and 5e−5, and report the best validation-set F1 score across these runs. Table 4: Results on TrivialQA with context length 4096 — “PT“ stands for pre-training and “FT“ stands for ﬁne-tuning. All models are comparable in size at around 110M. sstands for the head size of the single-head attention. For FLASH, “ﬁrst-to-all” means that we also let the ﬁrst token in each chunk to attend to the entire sequence using a single-head softmax attention. Latency (Lat.) is measured with 32 TPU-v4 cores. Model PT FT PT / FT PPLX F1 Lat. reduction Transformer+ 3.48 74.2 1.00 ×/ 1.00× Combiner 3.51 67.2 2.78×/ 2.75× FLASH-Quads=128 3.24 72.7 1.89 ×/ 1.79× FLASH-Quads=512 3.12 74.8 1.76×/ 1.67× FLASHs=512 3.23 73.3 2.61 ×/ 2.60× FLASHs=512 + ﬁrst-to-all 3.24 73.9 2.78×/ 2.69× We observe that the ﬁne-tuning results of the FLASH fam- ily can beneﬁt from several minor changes in the model conﬁguration. As shown in Table 4, increasing the head size of FLASH-Quad from 128 to 512 leads to a signiﬁcant boost of 2.1 point in the F1 score with negligible impact on speed. We further identify several other tweaks that improve the linear FLASH variant speciﬁcally, including using a small chunk size (128), disabling gradient clipping during ﬁnetuning, using softmax instead of squared ReLU for the [CLS] token, and (optionally) allowing the ﬁrst to- ken in each chunk to attend to the entire sequence using softmax. With those changes, FLASHs=512 achieves compa- rable quality to Transformer+ (0.3 difference in F1 is within the range of variance) while being 2.8×and 2.7×as fast as Transformer+ in pretraining and ﬁne-tuning, respectively. 4.4. Ablation Studies Signiﬁcance of quadratic & linear components. To bet- ter understand the efﬁcacy of FLASH, we ﬁrst study how much the local quadratic attention and the global linear atten- tion contribute to the performance individually. To this end,-3.2 -3.0 -2.8 FLASH FLASH (LocalOnly) FLASH (GlobalOnly) MC-TFM++ 0 2 4 6 8 -4.6 -4.4 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 TPU-core-days Neg. log pplx (a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 7: Ablation study of the proposed FLASH architecture. we create FLASH (LocalOnly) and FLASH (GlobalOnly) by only keeping the local quadratic attention and the global linear attention in FLASH, respectively. In FLASH (Glob- alOnly), we reduce the chunk size from 256 to 64 to produce more local summaries for the global linear attention. In Fig- ure 7 we see a signiﬁcant gap between the full model and the two variants, suggesting that the linear and global attention are complementary to each other — both are critical to the quality of the proposed mixed chunk attention. Signiﬁcance of GAU. Here we study the importance of using GAU in FLASH. To achieve this,we apply the same idea of mixed chunk attention to Transformer++. We re- fer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC- TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU. Figure 7 shows that FLASH outperforms MC-TFM++ by a large margin (more than 2×speedup when the sequence length is greater than 2048), conﬁrming the importance of GAU in our design. We further look into the perplexity in- crease due to our approximation method in Table 5, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than go- ing from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneﬁcial to weaker/approximate attention mechanisms. Impact of Chunk Size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3. Table 5: Perplexity increases when mixed chunk attention is applied to GAU (→FLASH) or to TFM++ (→MC-TFM++) — Results are reported for MLM and LM with increasing context lengths from 512 to 8192. MLM on C4 512 1024 2048 4096 8192 FLASH-Quad→FLASH 0.0 0.05 0.06 0.07 0.07 TFM++→MC-TFM++ 0.36 0.37 0.49 0.48 0.43 LM on Wiki-40B 512 1024 2048 4096 8192 FLASH-Quad→FLASH -0.05 0.06 0.22 0.30 0.11 TFM++→MC-TFM++ 0.54 0.75 0.86 0.90 0.87 5. Conclusion We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efﬁcient Transformer variants. This is achieved by designing a per- formant layer (gated linear unit) and by combining it with an accelerator-efﬁcient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art. A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks. Acknowledgements The authors would like to thank Gabriel Bender, John Blitzer, Maarten Bosma, Andrew Brock, Ed Chi, Hanjun Dai, Yann N. Dauphin, Pieter-Jan Kindermans and David So for their useful feedback. Weizhe Hua was supported in part by the Facebook fellowship. References Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019. Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan- guage modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 933–941. JMLR.org, 2017. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efﬁcient scaling of language models with mixture- of-experts. arXiv preprint arXiv:2112.06905, 2021. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3–11, 2018. Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 1243–1252. JMLR.org, 2017. Guo, M., Dai, Z., Vrandecic, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In LREC 2020, 2020. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y ., and Liu, W. Ccnet: Criss-cross attention for semantic segmen- tation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603–612, 2019. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with it- erative attention. In International Conference on Machine Learning, pp. 4651–4664. PMLR, 2021. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi- aQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, Vancouver, Canada, July 2017. Association for Compu- tational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020. Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efﬁcient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .- X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecast- ing. Advances in Neural Information Processing Systems, 32:5243–5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V . Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y ., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modiﬁcations transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. CoRR, abs/1910.05895, 2019. URL http://arxiv.org/ abs/1910.05895. Peng, H. et al. Random feature attention. In ICLR, 2021. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur- mans, D., and Dai, B. Combiner: Full attention trans- former with sparse computation cost. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=MQQeeDiO5vv. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient content-based sparse attention with routing transform- ers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. Shazeer, N. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/ abs/2002.05202. So, D. R., Ma ´nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V . Primer: Searching for efﬁcient transformers for language modeling. NeurIPS, 2021. Su, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2021. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul- shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y ., et al. Lamda: Language models for dialog appli- cations. arXiv preprint arXiv:2201.08239, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al- berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer se- quences. In NeurIPS, 2020.A. Connections to Combiner To capture long-term information, Combiner (Ren et al., 2021) additionally summarizes each chunk into summary key and value vectors Ksum,V sum ∈RT/C ×d and concatenate them into the local quadratic attention, i.e. ˆVg = Softmax ( Q[Kg; Ksum] ) [Vg; Vsum]. Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix Klin h ⊤ Vh of size O(sd) which is stimes larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners. Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra[T/C ×T/C] attention matrix to combine the chunk summaries, e.g. Alin = relu2 ( QsumKsum⊤+ bsum ) , ˆVlin g = Qlin g [T/C∑ h=1 alin gh ( Klin h ⊤ Vh )] . We indeed brieﬂy experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C+ T/C)d2) which is length-dependent and no longer constant. Hence, we do not include this feature in our default conﬁguration. B. Experimental Setup B.1. Hyperparameters Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in Table 6. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. Table 6: Hyperparameters for MLM pretraining on C4. MLM Results (Figure 5) Data C4 Sequence length 512 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Chunk size 256 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 * Applied to all models except the vanilla Transformer. Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in Table 7. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.Table 7: Hyperparameters for LM pretraining on Wiki-40B and PG-19. LM Results (Figure 6) LM Results (Table 3) Data Wiki-40B PG-19 Sequence length 512 - 8192 1024 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 Chunk size 256 512 * Applied to all models except the vanilla Transformer. B.2. Model Speciﬁcations Detailed speciﬁcations of all models used in our experiments are summarized in Tables 8, 9, and 10. In the experiments, SiLU/Swish (Elfwing et al., 2018; Hendrycks & Gimpel, 2016; Ramachandran et al., 2017) is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU (Hendrycks & Gimpel, 2016) in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model. Table 8: Model conﬁgurations for MLM experiments on the C4 dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type3 ScaleNorm ScaleNorm LayerNorm ScaleNorm ScaleNorm ScaleNorm ScaleNorm Absolute position emb. ScaledSin4 ScaledSin4 Learnable5 ScaledSin4 ScaledSin4 ScaledSin4 ScaledSin4 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 6 12+126 12+126 12+126 12+126 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleNorm and LayerNorm are proposed by Nguyen & Salazar (2019) and Ba et al. (2016), respectively. 4 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 5 The learnable position embedding is proposed by Gehring et al. (2017). 6 The model is consist of 12 attention layers and 12 FFN layers. C. Additional Experimental Results Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in Table 11) and the ablation study of chunk size for FLASH (in Figure 8).Table 9: Model conﬁgurations for LM experiments on the Wiki-40B dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 Learnable4 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 5 12+125 12+125 12+125 12+125 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The learnable position embedding is proposed by Gehring et al. (2017). 5 The model is consist of 12 attention layers and 12 FFN layers. Table 10: Model conﬁgurations for LM experiments on the PG-19 dataset in Section 4. FLASH-Quad FLASH Transformer+ Transformer++ Combiner # of attention heads 1 1 16 16 16 Attention kernel relu2 relu2 softmax softmax softmax Attention type Quadratic Mixed Chunk Quadratic Quadratic Rowmajor-Axial FFN type GAU1 GAU1 MLP GLU MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE RoPE RoPE RoPE # of layers 72 72 36+36 4 36+364 36+364 Hidden size 1024 1024 1024 1024 1024 Expansion rate 2 2 4 4 4 Chunk size – 512 – – 512 Params (M) 496 496 486 486 562 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The model is consist of 36 attention layers and 36 FFN layers. Table 11: Comparison of latency for each training step of auto-regressive language modeling on Wiki-40B using a single Nvidia Tesla V100 GPU — Latency is reported in millisecond. OOM stands for the CUDA out of memory error. Performer-Matmul implements the cumulative sum (cumsum) using matrix multiplication. Context length ×Batch size Model 512 ×4 1024 ×2 2048 ×1 4096 ×1 Transformer++ 222.4 243.9 315.0 OOM Performer 823.0 827.4 799.8 OOM Performer-Matmul 697.4 701.7 688.9 OOM FLASH 254.4 235.0 242.8 452.9C.1. Auto-regressive Training on GPU We observe that the inefﬁciency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in Table 11, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism. C.2. Tabular MLM and LM Results We summarize the experimental results of MLM on C4 and LM on Wiki-40B in Tables 12 and 13. Table 12: Bidirectional/masked language models on the C4 dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 4.517 47.7 4.436 63.9 4.196 90.9 4.602 142.5 4.8766 252.7 Transformer+ 4.283 48.8 4.151 64.4 4.032 91.5 3.989 142.9 3.986 252.9 Transformer++ 4.205 47.6 4.058 64.6 3.920 91.6 3.876 143.4 3.933 252.1 Performer 5.897 37.2 6.324 37.6 8.032 39.1 12.622 36.9 102.980 40.9 Combiner 4.449 67.2 4.317 66.4 4.238 66.4 4.195 68.3 4.225 77.3 FLASH-Quad 4.176 43.7 3.964 50.1 3.864 61.7 3.828 84.9 3.830 132.1 FLASH 4.172 51.2 4.015 50.1 3.928 51.4 3.902 50.7 3.897 59.9 Table 13: Auto-regressive language models on the Wiki-40B dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 17.341 54.0 19.808 70.9 18.154 96.3 17.731 149.1 18.254 260.7 Transformer+ 16.907 55.6 15.999 70.3 15.653 96.1 15.515 149.3 15.478 261.9 Transformer++ 16.835 54.7 15.943 70.9 15.489 96.6 15.282 149.2 15.254 261.0 Performer 18.989 1439.7 18.520 1386.9 18.547 1518.9 18.987 1526.7 19.923 1526.8 Combiner 17.338 75.5 16.710 74.4 16.344 71.8 16.171 71.7 16.119 77.9 FLASH-Quad 16.633 54.1 15.879 59.5 15.305 71.3 14.955 96.1 14.998 141.3 FLASH 16.581 57.2 15.935 56.9 15.525 56.7 15.259 57.0 15.109 62.5 C.3. Ablation Study of Chunk Size The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefﬁcient auto-regressive training. Figure 8 shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K. D. Pseudocode For FLASH-Quad and FLASH We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8.(a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 8: Ablation study of the chunk size (C) of FLASH for context lengths from 1K to 8K. def _get_scaledsin(embeddings): \"\"\"Create sinusoidal position embedding with a scaling factor.\"\"\" hidden_size = int(embeddings.shape[=1]) pos = tf.range(tf.shape(embeddings)[1]) pos = tf.cast(pos, tf.float32) half_d = hidden_size // 2 freq_seq = tf.cast(tf.range(half_d), tf.float32) / float(half_d) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1s,d→sd/quotesingle.ts1, pos, inv_freq) scaledsin = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis==1) scalar = tf.get_variable( /quotesingle.ts1scaledsin_scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1 / hidden_size ∗∗ 0.5)) scaledsin ∗= scalar return scaledsin Code 2: Pseudocode for ScaledSin absolute position embedding. def rope(x, axis): \"\"\"RoPE position embedding.\"\"\" shape = x.shape.as_list() if isinstance(axis, int): axis = [axis] spatial_shape = [shape[i] for i in axis] total_len = 1 for i in spatial_shape: total_len ∗= i position = tf.reshape( tf.cast(tf.range(total_len, delta=1.0), tf.float32), spatial_shape) for i in range(axis[=1] + 1, len(shape) = 1, 1): position = tf.expand_dims(position, axis==1) half_size = shape[=1] // 2 freq_seq = tf.cast(tf.range(half_size), tf.float32)/float(half_size) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1...,d→...d/quotesingle.ts1, position, inv_freq) sin = tf.sin(sinusoid) cos = tf.cos(sinusoid) x1, x2 = tf.split(x, 2, axis==1) return tf.concat([x1 ∗ cos = x2 ∗ sin, x2 ∗ cos + x1 ∗ sin], axis==1) Code 3: Pseudocode for RoPE.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def rel_pos_bias(n): \"\"\"Relative position bias.\"\"\" if n < 512: # Construct Toeplitz matrix directly when the sequence length is less than 512. w = tf.get_variable( /quotesingle.ts1weight/quotesingle.ts1, shape=[2 ∗ n = 1], dtype=tf.float32, initializer=WEIGHT_INITIALIZER) t = tf.pad(w, [[0, n]]) t = tf.tile(t, [n]) t = t[..., :=n] t = tf.reshape(t, [n, 3 ∗ n = 2]) r = (2 ∗ n = 1) // 2 t = t[..., r:=r] else: # Construct Toeplitz matrix using RoPE when the sequence length is over 512. a = tf.get_variable( /quotesingle.ts1a/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) b = tf.get_variable( /quotesingle.ts1b/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) a = rope(tf.tile(a[None, :], [n, 1]), axis=0) b = rope(tf.tile(b[None, :], [n, 1]), axis=0) t = tf.einsum(/quotesingle.ts1mk,nk→mn/quotesingle.ts1, a, b) return t Code 4: Pseudocode for relative position bias. def norm(x, begin_axis==1, eps=1e=5, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1): \"\"\"Normalization layer.\"\"\" shape = x.shape.as_list() axes = list(range(len(shape)))[begin_axis:] if norm_type == /quotesingle.ts1layer_norm/quotesingle.ts1: mean, var = tf.nn.moments(x, axes, keepdims=True) x = (x = mean) ∗ tf.rsqrt(var + eps) gamma = tf.get_variable( /quotesingle.ts1gamma/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.ones()) beta = tf.get_variable( /quotesingle.ts1beta/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.zeros()) return gamma ∗ x + beta elif norm_type == /quotesingle.ts1scale_norm/quotesingle.ts1: mean_square =tf.reduce_mean(tf.math.square(x), axes, keepdims=True) x = x ∗ tf.rsqrt(mean_square + eps) scalar = tf.get_variable(/quotesingle.ts1scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1.0)) return scale ∗ x Code 5: Pseudocode for LayerNorm and ScaleNorm.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def GAU(x, causal, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"GAU block. Input shape: batch size x sequence length x model size \"\"\" seq_len = tf.shape(x)[1] d = int(x.shape[=1]) e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query (q) and Key (k) from base. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[2, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[2, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=1) q, k = tf.unstack(base, axis==2) # Calculate the quadratic attention. qk = tf.einsum(/quotesingle.ts1bnd,bmd→bnm/quotesingle.ts1, q, k) bias = rel_pos_bias(seq_len) kernel = tf.math.square(tf.nn.relu(qk / seq_len + bias)) # Apply the causal mask for auto=regressive tasks. if causal: causal_mask = tf.linalg.band_part( tf.ones([seq_len, seq_len], dtype=x.dtype), num_lower==1, num_upper=0) kernel ∗= causal_mask x = u ∗ tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, kernel, v) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 6: Pseudocode for GAU (FLASH-Quad). def segment_ids_to_mask(segment_ids, causal=False): \"\"\"Generate the segment mask from the segment ids. The segment mask is used to remove the attention between tokens in different documents. \"\"\" min_ids, max_ids = tf.reduce_min(segment_ids, axis==1), tf.reduce_max(segment_ids, axis==1) # 1.0 indicates in the same group and 0.0 otherwise mask = tf.logical_and( tf.less_equal(min_ids[:, :, None], max_ids[:, None, :]), tf.greater_equal(max_ids[:, :, None], min_ids[:, None, :])) mask = tf.cast(mask, tf.float32) if causal: g = tf.shape(min_ids)[1] causal_mask = 1.0 = tf.linalg.band_part( tf.ones([g, g], dtype=tf.float32), num_lower=0, num_upper==1) mask ∗= causal_mask mask = tf.math.divide_no_nan(mask, tf.reduce_sum(mask, axis==1, keepdims=True)) return mask Code 7: Pseudocode for generating segment mask.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def FLASH(x, causal, segment_ids, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"FLASH block. Input shape: batch size x num chunks x chunk length x model size \"\"\" _, g, n, d = x.shape.as_list() e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query and Key for both quadratic and linear attentions. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[4, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[4, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=[1, 2]) quad_q, quad_k, lin_q, lin_k = tf.unstack(base, axis==2) if causal: # Linear attention part. lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids, causal=True) cum_lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, cum_lin_kv) # Quadratic attention part. quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu(quad_qk / n + bias)) # Apply the causal mask for auto=regressive tasks. causal_mask = tf.linalg.band_part(tf.ones([n, n], dtype=x.dtype), num_lower==1, num_upper=0) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel ∗ causal_mask, v) else: # Linear attention part lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids) lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, lin_kv) # Quadratic attention part quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu((quad_qk / n + bias)) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel, v) x = u ∗ (quadratic + linear) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 8: Pseudocode for FLASH.",
      "meta_data": {
        "arxiv_id": "2202.10447v2",
        "authors": [
          "Weizhe Hua",
          "Zihang Dai",
          "Hanxiao Liu",
          "Quoc V. Le"
        ],
        "published_date": "2022-02-21T18:59:38Z",
        "pdf_url": "https://arxiv.org/pdf/2202.10447v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the quadratic complexity of Transformers in handling long sequences, a limitation that hinders their application in many domains. It proposes FLASH, a novel model family that for the first time matches the quality (perplexity) of fully augmented Transformers while achieving true linear scalability over context size on modern accelerators. Key contributions include the Gated Attention Unit (GAU), a new layer design that enables the use of weaker single-head attention with minimal quality loss, and an accelerator-friendly mixed chunk attention method that combines local quadratic attention within chunks and global linear attention across chunks to achieve linear complexity and significantly faster auto-regressive training.",
        "methodology": "The FLASH model is developed in two main steps. First, the Gated Attention Unit (GAU) is introduced as a replacement for the standard Transformer layer. GAU integrates a gating mechanism with attention, allowing for a simpler, single-head, softmax-free attention while maintaining performance, thus making the attention component less critical and more amenable to approximation. Second, to achieve linear complexity, GAU's quadratic attention is approximated using a mixed chunk attention strategy. This involves splitting the input sequence into non-overlapping chunks. Within each chunk, a local quadratic attention is applied, while a global linear attention mechanism captures long-range interactions across chunks. This chunked approach dramatically reduces the sequential dependencies in auto-regressive training from T steps to T/C steps (where C is the chunk size), optimizing for modern accelerators and ensuring constant O(Cd^2) decoding memory and computation per step.",
        "experimental_setup": "The efficacy of FLASH was evaluated through extensive experiments on bidirectional masked language modeling (MLM) using the C4 dataset and auto-regressive language modeling (LM) on Wiki-40B and PG-19 (the latter specifically for long contexts with average document length of 69K words). Model scales ranged from 110M to 500M parameters, covering context lengths from 512 to 8192 tokens. For downstream task evaluation, pre-trained models were fine-tuned on the TriviaQA dataset. Baselines included vanilla Transformer, Transformer+, Transformer++, Performer, and Combiner. Performance was measured by perplexity (negative log perplexity), F1 score for fine-tuning, training latency per step (in milliseconds), and total training cost (in TPU-v4-core-days). Experiments were primarily conducted on 64 TPU-v4 cores, with some auto-regressive training comparisons also performed on a single Nvidia Tesla V100 GPU.",
        "limitations": "The paper notes that the non-overlapping local attention used in FLASH, while chosen for practical speed benefits due to avoiding extensive memory re-formatting operations, might not always yield the best quality compared to overlapping local attention variants. Preliminary experiments suggested that the cost-benefit trade-off of overlapping local attention was not as good as simply adding more layers. Additionally, the choice of chunk size can significantly impact both quality and training cost, with larger chunk sizes generally performing better for longer context lengths, implying that optimal performance may require hyperparameter search over this setting.",
        "future_research_directions": "Future research directions explicitly mentioned include investigating the scaling laws of the new FLASH model family and further exploring its performance and applicability on a wider range of downstream tasks beyond those evaluated in this paper."
      }
    },
    {
      "title": "Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data",
      "abstract": "This paper investigates the intriguing question of whether we can create\nlearning algorithms that automatically generate training data, learning\nenvironments, and curricula in order to help AI agents rapidly learn. We show\nthat such algorithms are possible via Generative Teaching Networks (GTNs), a\ngeneral approach that is, in theory, applicable to supervised, unsupervised,\nand reinforcement learning, although our experiments only focus on the\nsupervised case. GTNs are deep neural networks that generate data and/or\ntraining environments that a learner (e.g. a freshly initialized neural\nnetwork) trains on for a few SGD steps before being tested on a target task. We\nthen differentiate through the entire learning process via meta-gradients to\nupdate the GTN parameters to improve performance on the target task. GTNs have\nthe beneficial property that they can theoretically generate any type of data\nor training environment, making their potential impact large. This paper\nintroduces GTNs, discusses their potential, and showcases that they can\nsubstantially accelerate learning. We also demonstrate a practical and exciting\napplication of GTNs: accelerating the evaluation of candidate architectures for\nneural architecture search (NAS), which is rate-limited by such evaluations,\nenabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art,\nfinding higher performing architectures when controlling for the search\nproposal mechanism. GTN-NAS also is competitive with the overall state of the\nart approaches, which achieve top performance while using orders of magnitude\nless computation than typical NAS methods. Speculating forward, GTNs may\nrepresent a first step toward the ambitious goal of algorithms that generate\ntheir own training data and, in doing so, open a variety of interesting new\nresearch questions and directions.",
      "full_text": "GENERATIVE TEACHING NETWORKS : ACCELERATING NEURAL ARCHITECTURE SEARCH BY LEARNING TO GENERATE SYNTHETIC TRAINING DATA Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth O. Stanley∗& Jeff Clune∗ Uber AI Labs ABSTRACT This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neu- ral networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneﬁcial property that they can theoretically gener- ate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and excit- ing application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, ﬁnding higher performing architectures when controlling for the search pro- posal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may repre- sent a ﬁrst step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions. 1 I NTRODUCTION AND RELATED WORK Access to vast training data is now common in machine learning. However, to effectively train neural networks (NNs) does not require using all available data. For example, recent work in cur- riculum learning (Graves et al., 2017), active learning (Konyushkova et al., 2017; Settles, 2010) and core-set selection (Sener & Savarese, 2018; Tsang et al., 2005) demonstrates that a surrogate dataset can be created by intelligently sampling a subset of training data, and that such surrogates enable competitive test performance with less training effort. Being able to more rapidly determine the per- formance of an architecture in this way could particularly beneﬁt architecture search, where training thousands or millions of candidate NN architectures on full datasets can become prohibitively ex- pensive. From this lens, related work in learning-to-teach has shown promise. For example, the learning to teach (L2T) (Fan et al., 2018) method accelerates learning for a NN learner (hereafter, just learner) through reinforcement learning, by learning how to subsample mini-batches of data. A key insight in this paper is that the surrogate data need not be drawn from the original data distribution (i.e. they may not need to resemble the original data). For example, humans can learn new skills from reading a book or can prepare for a team game like soccer by practicing skills, such as passing, dribbling, juggling, and shooting. This paper investigates the question of whether we can train a data-generating network that can produce synthetic data that effectively and efﬁciently ∗co-senior authors. Corresponding authors: {felipe.such,kstanley,jeffclune}@uber.com 1 arXiv:1912.07768v1  [cs.LG]  17 Dec 2019teaches a target task to a learner. Related to the idea of generating data, Generative Adversarial Networks (GANs) can produce impressive high-resolution images (Goodfellow et al., 2014; Brock et al., 2018), but they are incentivized to mimic real data (Goodfellow et al., 2014), instead of being optimized to teach learners more efﬁciently than real data. Another approach for creating surrogate training data is to treat the training data itself as a hyper- parameter of the training process and learn it directly. Such learning can be done through meta- gradients (also called hyper-gradients), i.e. differentiating through the training process to optimize a meta-objective. This approach was described in Maclaurin et al. (2015), where 10 synthetic training images were learned using meta-gradients such that when a network is trained on these images, the network’s performance on the MNIST validation dataset is maximized. In recent work concurrent with our own, Wang et al. (2019b) scaled this idea to learn 100 synthetic training examples. While the 100 synthetic examples were more effective for training than 100 original (real) MNIST training examples, we show that it is difﬁcult to scale this approach much further without the regularity across samples provided by a generative architecture (Figure 2b, green line). Being able to very quickly train learners is particularly valuable for neural architecture search (NAS), which is exciting for its potential to automatically discover high-performing architectures, which otherwise must be undertaken through time-consuming manual experimentation for new domains. Many advances in NAS involve accelerating the evaluation of candidate architectures by training a predictor of how well a trained learner would perform, by extrapolating from previously trained architectures (Luo et al., 2018; Liu et al., 2018a; Baker et al., 2017). This approach is still expensive because it requires many architectures to be trained and evaluated to train the predictor. Other approaches accelerate training by sharing training across architectures, either through shared weights (e.g. as in ENAS; Pham et al. (2018)), or Graph HyperNetworks (Zhang et al., 2018). We propose a scalable, novel, meta-learning approach for creating synthetic data called Generative Teaching Networks (GTNs). GTN training has two nested training loops: an inner loop to train a learner network, and an outer-loop to train a generator network that produces synthetic training data for the learner network. Experiments presented in Section 3 demonstrate that the GTN approach produces synthetic data that enables much faster learning, speeding up the training of a NN by a fac- tor of 9. Importantly, the synthetic data in GTNs is not only agnostic to the weight initialization of the learner network (as in Wang et al. (2019b)), but is also agnostic to the learner’sarchitecture. As a result, GTNs are a viable method for accelerating evaluation of candidate architectures in NAS. Indeed, controlling for the search algorithm (i.e. using GTN-produced synthetic data as a drop-in replacement for real data when evaluating a candidate architecture’s performance), GTN-NAS im- proves the NAS state of the art by ﬁnding higher-performing architectures than comparable methods like weight sharing (Pham et al., 2018) and Graph HyperNetworks (Zhang et al., 2018); it also is competitive with methods using more sophisticated search algorithms and orders of magnitude more computation. It could also be combined with those methods to provide further gains. One promising aspect of GTNs is that they make very few assumptions about the learner. In contrast, NAS techniques based on shared training are viable only if the parameterizations of the learners are similar. For example, it is unclear how weight-sharing or HyperNetworks could be applied to architectural search spaces wherein layers could be either convolutional or fully-connected, as there is no obvious way for weights learned for one layer type to inform those of the other. In contrast, GTNs are able to create training data that can generalize between such diverse types of architectures. GTNs also open up interesting new research questions and applications to be explored by future work. Because they can rapidly train new architectures, GTNs could be used to create NNs on- demand that meet speciﬁc design constraints (e.g. a given balance of performance, speed, and en- ergy usage) and/or have a speciﬁc subset of skills (e.g. perhaps one needs to rapidly create a compact network capable of three particular skills). Because GTNs can generate virtually any learning en- vironment, they also one day could be a key to creating AI-generating algorithms, which seek to bootstrap themselves from simple initial conditions to powerful forms of AI by creating an open- ended stream of challenges (learning opportunities) while learning to solve them (Clune, 2019). 2 M ETHODS The main idea in GTNs is to train a data-generating network such that a learner network trained on data it rapidly produces high accuracy in a target task. Unlike a GAN, here the two networks 2cooperate (rather than compete) because their interests are aligned towards having the learner per- form well on the target task when trained on data produced by the GTN. The generator and the learner networks are trained with meta-learning via nested optimization that consists of inner and outer training loops (Figure 1a). In the inner-loop, the generator G(z,y) takes Gaussian noise ( z) and a label ( y) as input and outputs synthetic data (x). Optionally, the generator could take only noise as input and produce both data and labels as output (Appendix F). The learner is then trained on this synthetic data for a ﬁxed number of inner-loop training steps with any optimizer, such as SGD or Adam (Kingma & Ba, 2014): we use SGD with momentum in this paper. SI Equation 1 deﬁnes the inner-loop SGD with momentum update for the learner parameters θt. We sample zt (noise vectors input to the generator) from a unit-variance Gaussian andyt labels for each generated sample) uniformly from all available class labels. Note that both zt and yt are batches of samples. We can also learn a curriculum directly by additionally optimizing zt directly (instead of sampling it randomly) and keeping yt ﬁxed throughout all of training. The inner-loop loss function ℓinner can be cross-entropy for classiﬁcation problems or mean squared error for regression problems. Note that the inner-loop objective does not depend on the outer- loop objective and could even be parameterized and learned through meta-gradients with the rest of the system (Houthooft et al., 2018). In the outer-loop, the learner θT (i.e. the learner parameters trained on synthetic data after the T inner-loop steps) is evaluated on the realtraining data, which is used to compute the outer-loop loss (aka meta-training loss). The gradient of the meta-training loss with respect to the generator is computed by backpropagating through the entire inner-loop learning process. While computing the gradients for the generator we also compute the gradients of hyper- parameters of the inner-loop SGD update rule (its learning rate and momentum), which are updated after each outer-loop at no additional cost. To reduce memory requirements, we leverage gradient- checkpointing (Griewank & Walther, 2000) when computing meta-gradients. The computation and memory complexity of our approach can be found in Appendix D. (1) Noise Inner-loop Generator Learner (4) Meta-loss Real  Data (2) Data (3) SGD Step (5) Gradient of Meta-loss w.r.t. Generator Outer-loop (a) Overview of Generative Teaching Networks Without WN With WN 0.0 0.2 0.4 0.6 0.8 1.0 1.2Validation Loss (b) GTN stability with WN 0 500 1000 1500 2000 Outer-loop Iterations 0.850 0.875 0.900 0.925 0.950 0.975 1.000Test Accuracy No Curriculum All Shuffled Shuffled Batch Full Curriculum (c) GTN curricula comparison Figure 1: (a) Generative Teaching Network (GTN) Method. The numbers in the ﬁgure reﬂect the order in which a GTN is executed. Noise is fed as an input to the Generator (1), which uses it to gen- erate new data (2). The learner is trained (e.g. using SGD or Adam) to perform well on the generated data (3). The trained learner is then evaluated on the real training data in the outer-loop to compute the outer-loop meta-loss (4). The gradients of the generator parameters are computed w.r.t. to the meta-loss to update the generator (5). Both a learned curriculum and weight normalization sub- stantially improve GTN performance. (b) Weight normalization improves meta-gradient training of GTNs, and makes the method much more robust to different hyperparameter settings. Each boxplot reports the ﬁnal loss of 20 runs obtained during hyperparameter optimization with Bayesian Opti- mization (lower is better). (c) shows a comparison between GTNs with different types of curricula. The GTN method with the most control over how samples are presented performs the best. A key motivation for this work is to generate synthetic data that is learner agnostic, i.e. that gener- alizes across different potential learner architectures and initializations. To achieve this objective, at the beginning of each new outer-loop training, we choose a new learner architecture according to a predeﬁned set and randomly initialize it (details in Appendix A). Meta-learning with Weight Normalization. Optimization through meta-gradients is often unsta- ble (Maclaurin et al., 2015). We observed that this instability greatly complicates training because of its hyperparameter sensitivity, and training quickly diverges if they are not well-set. Combining the gradients from Evolution Strategies (Salimans et al., 2017) and backpropagation using inverse variance weighting (Fleiss, 1993; Metz et al., 2019) improved stability in our experiments, but op- timization still consistently diverged whenever we increased the number of inner-loop optimization 3steps. To mitigate this issue, we introduce applying weight normalization (Salimans & Kingma, 2016) to stabilize meta-gradient training by normalizing the generator and learner weights. Instead of updating the weights (W) directly, we parameterize them as W = g·V/∥V∥and instead update the scalar gand vector V. Weight normalization eliminates the need for (and cost of) calculating ES gradients and combining them with backprop gradients, simplifying and speeding up the algorithm. We hypothesize that weight normalization will help stabilize meta-gradient training more broadly, although future work is required to test this hypothesis in meta-learning contexts besides GTNs. The idea is that applying weight normalization to meta-learning techniques is analogous to batch normalization for deep networks (Ioffe & Szegedy, 2015). Batch normalization normalizes the forward propagation of activations in a long sequence of parameterized operations (a deep NN). In meta-gradient training both the activations and weights result from a long sequence of parameterized operations and thus both should be normalized. Results in section 3.1 support this hypothesis. Learning a Curriculum with Generative Teaching Networks. Previous work has shown that a learned curriculum can be more effective than training from uniformly sampled data (Graves et al., 2017). A curriculum is usually encoded with indexes to samples from a given dataset, rendering it non-differentiable and thereby complicating the curriculum’s optimization. With GTNs however, a curriculum can be encoded as a series of input vectors to the generator (i.e. instead of sampling the zt inputs to the generator from a Gaussian distribution, a sequence of zt inputs can be learned). A curriculum can thus be learned by differentiating through the generator to optimize this sequence (in addition to the generator’s parameters). Experiments conﬁrm that GTNs more effectively teach learners when optimizing such a curriculum (Section 3.2). Accelerating NAS with Generative Teaching Networks.Since GTNs can accelerate learner train- ing, we propose harnessing GTNs to accelerate NAS. Rather than evaluating each architecture in a target task with a standard training procedure, we propose evaluating architectures with a meta- optimized training process (that generates synthetic data in addition to optimizing inner-loop hyper- parameters). We show that doing so signiﬁcantly reduces the cost of running NAS (Section 3.4). The goal of these experiments is to ﬁnd a high-performing CNN architecture for the CIFAR10 image-classiﬁcation task (Krizhevsky et al., 2009) with limited compute costs. We use the same architecture search-space, training procedure, hyperparameters, and code from Neural Architecture Optimization (Luo et al., 2018), a state-of-the-art NAS method. The search space consists of the topology of two cells: a reduction cell and a convolutional cell. Multiple copies of such cells are stacked according to a predeﬁned blueprint to form a full CNN architecture (see Luo et al. (2018) for more details). The blueprint has two hyperparameters N and F that control how many times the convolutional cell is repeated (depth) and the width of each layer, respectively. Each cell contains B = 5nodes. For each node within a cell, the search algorithm has to choose two inputs as well as two operations to apply to those inputs. The inputs to a node can be previous nodes or the outputs of the last two layers. There are 11 operations to choose from (Appendix C). Following Luo et al. (2018), we report the performance of our best cell instantiated with N = 6,F = 36after the resulting architecture is trained for a signiﬁcant amount of time (600 epochs). Since evaluating each architecture in those settings (named ﬁnal evaluation from now on) is time consuming, Luo et al. (2018) uses a surrogate evaluation (named search evaluation) to estimate the performance of a given cell wherein a smaller version of the architecture ( N = 3,F = 32) is trained for less epochs (100) on real data. We further reduce the evaluation time of each cell by replacing the training data in the search evaluation with GTN synthetic data, thus reducing the training time per evaluation by 300x (which we call GTN evaluation). While we were able to train GTNs directly on the complex architectures from the NAS search space, training was prohibitively slow. Instead, for these experiments, we optimize our GTN ahead of time using proxy learners described in Appendix A.2, which are smaller fully-convolutional networks (this meta-training took 8h on one p6000 GPU). Interestingly, although we never train our GTN on any NAS architectures, because of generalization, synthetic data from GTNs were still effective for training them. 3 R ESULTS We ﬁrst demonstrate that weight normalization signiﬁcantly improves the stability of meta-learning, an independent contribution of this paper (Section 3.1). We then show that training with synthetic data is more effective when learning such data jointly with a curriculum that orders its presentation 4to the learner (Section 3.2). We next show that GTNs can generate a synthetic training set that enables more rapid learning in a few SGD steps than real training data in two supervised learning domains (MNIST and CIFAR10) and in a reinforcement learning domain (cart-pole, Appendix H). We then apply GTN-synthetic training data for neural architecture search to ﬁnd high performing architectures for CIFAR10 with limited compute, outperforming comparable methods like weight sharing (Pham et al., 2018) and Graph HyperNetworks (Zhang et al., 2018) (Section 3.4). We uniformly split the usual MNIST training set into training (50k) and validation sets (10k). The training set was used for inner-loop training (for the baseline) and to compute meta-gradients for all the treatments. We used the validation set for hyperparameter tuning and report accuracy on the usual MNIST test set (10k images). We followed the same procedure for CIFAR10, resulting in training, validation, and test sets with 45k, 5k, and 10k examples, respectively. Unless otherwise speciﬁed, we ran each experiment 5 times and plot the mean and its 95% conﬁdence intervals from (n=1,000) bootstrapping. Appendix A describes additional experimental details. 3.1 I MPROVING STABILITY WITH WEIGHT NORMALIZATION To demonstrate the effectiveness of weight normalization for stabilizing and robustifying meta- optimization, we compare the results of running hyperparameter optimization for GTNs with and without weight normalization on MNIST. Figure 1b shows the distribution of the ﬁnal performance obtained for 20 runs during hyperparameter tuning, which reﬂects how sensitive the algorithms are to hyperparameter settings. Overall, weight normalization substantially improved robustness to hyperparameters and ﬁnal learner performance, supporting the initial hypothesis. 3.2 I MPROVING GTN S WITH A CURRICULUM We experimentally evaluate four different variants of GTNs, each with increasing control over the ordering of the zcodes input to the generator, and thus the order of the inputs provided to the learner. The ﬁrst variant (called GTN - No Curriculum), trains a generator to output synthetic training data by sampling the noise vector zfor each sample independently from a Gaussian distribution. In the next three GTN variants, the generator is provided with a ﬁxed set of input samples (instead of a noise vector). These input samples are learned along with the generator parameters during GTN training. The second GTN variant (called GTN - All Shufﬂed ) learns a ﬁxed set of 4,096 input samples that are presented in a random order without replacement (thus learning controls the data, but not the order in which they are presented). The third variant (calledGTN - Shufﬂed Batch) learns 32 batches of 128 samples each (so learning controls which samples coexist within a batch), but the order in which the batches are presented is randomized (without replacement). Finally, the fourth variant (calledGTN - Full Curriculum) learns a deterministic sequence of 32 batches of 128 samples, giving learning full control. Learning such a curriculum incurs no additional computational expense, as learning the zt tensor is computationally negligible and avoids the cost of repeatedly sampling new Gaussian z codes. We plot the test accuracy of a learner (with random initial weights and architecture) as a function of outer-loop iterations for all four variants in Figure 1c. Although GTNs - No curriculum can seemingly generate endless data (see Appendix G), it performs worse than the other three variants with a ﬁxed set of generator inputs. Overall, training the GTN with exact ordering of input samples (GTN - Full Curriculum) outperforms all other variants. While curriculum learning usually refers to training on easy tasks ﬁrst and increasing their difﬁculty over time, our curriculum goes beyond presenting tasks in a certain order. Speciﬁcally, GTN - Full Curriculum learns both the order in which to present samples and the speciﬁc group of samples to present at the same time. The ability to learn a full curriculum improves GTN performance. For that reason, we adopt that approach for all GTN experiments. 3.3 GTN S FOR SUPERVISED LEARNING To explore whether GTNs can generate training data that helps networks learn rapidly, we compare to 3 treatments for MNIST classiﬁcation. 1)Real Data - Training learners with random mini-batches of real data, as is ubiquitous in SGD. 2) Dataset Distillation - Training learners with synthetic data, where training examples are directly encoded as tensors optimized by the meta-objective, as in Wang et al. (2019b). 3) GTN - Our method where the training data presented to the learner is generated by a neural network. Note that all three methods meta-optimize the inner-loop hyperparameters (i.e. the learning rate and momentum of SGD) as part of the meta-optimization. 5We emphasize that producing state-of-the-art (SOTA) performance (e.g. on MNIST or CIFAR) when training with GTN-generated data is not important for GTNs. Because the ultimate aim for GTNs is to accelerate NAS (Section 3.4), what matters ishow well and inexpensively we can identify architectures that achieve high asymptotic accuracy when later trained on the full (real) training set. A means to that end is being able to train architectures rapidly, i.e. with very few SGD steps, because doing so allows NAS to rapidly identify promising architectures. We are thus interested in “few-step accuracy (i.e. accuracy after a few–e.g. 32 or 128–SGD steps). Besides, there are many reasons not to expect SOTA performance with GTNs (Appendix B). Figure 2a shows that the GTN treatment signiﬁcantly outperforms the other ones ( p <0.01) and trains a learner to be much more accurate whenin the few-step performance regime. Speciﬁcally, for each treatment the ﬁgure shows the test performance of a learner following 32 inner-loop training steps with a batch size of 128. We would not expect training on synthetic data to produce higher accuracy than unlimited SGD steps on real data, but here the performance gain comes because GTNs can compress the real training data by producing synthetic data that enables learners to learn more quickly than on real data. For example, the original dataset might contain many similar images, where only a few of them would be sufﬁcient for training (and GTN can produce just these few). GTN could also combine many different things that need to be learned about images into one image. Figure 2b shows the few-step performance of a learner from each treatment after 2000 total outer- loop iterations (∼1 hour on a p6000 GPU). For reference, Dataset Distillation (Wang et al., 2019b) reported 79.5% accuracy for a randomly initialized network (using 100 synthetic images vs. our 4,096) and L2T (Fan et al., 2018) reported needing 300x more training iterations to achieve >98% MNIST accuracy. Surprisingly, although recognizable as digits and effective for training, GTN- generated images (Figure 2c) were not visually realistic (see Discussion). 0 500 1000 1500 2000 Outer-loop Iterations 0.90 0.92 0.94 0.96 0.98 1.00Test Accuracy GTN Real Data Dataset Distillation (a) Meta-training curves 0 10 20 30 Inner-loop Iterations 0.90 0.92 0.94 0.96 0.98 1.00Test Accuracy GTN Real Data Dataset Distillation (b) Training curves  (c) GTN-generated samples Figure 2: Teaching MNIST with GTN-generated images. (a) MNIST test set few-step accuracy across outer-loop iterations for different sources of inner-loop training data. The inner-loop consists of 32 SGD steps and the outer-loop optimizes MNIST validation accuracy. Our method (GTN) outperforms the two controls (dataset distillation and samples from real data). (b) For the ﬁnal meta-training iteration, across inner-loop training, accuracy on the MNIST test set when inner-loop training on different data sources. (c) 100 random samples from the trained GTN. Samples are often recognizable as digits, but are not realistic (see Discussion). Each column contains samples from a different digit class, and each row is taken from different inner-loop iterations (evenly spaced from the 32 total iterations, with early iterations at the top). 3.4 A RCHITECTURE SEARCH WITH GTN S We next test the beneﬁts of GTN for NAS (GTN-NAS) in CIFAR10, a domain where NAS has previously shown signiﬁcant improvements over the best architectures produced by armies of hu- man scientists. Figure 3a shows the few-step training accuracy of a learner trained with either GTN-synthetic data or real (CIFAR10) data over meta-training iterations. After 8h of meta-training, training with GTN-generated data was signiﬁcantly faster than with real data, as in MNIST. To explore the potential for GTN-NAS to accelerate CIFAR10 architecture search, we investigated the Spearman rank correlation (across architectures sampled from the NAS search space) between accelerated GTN-trained network performance (GTN evaluation) and the usual more expensive per- formance metric used during NAS (search evaluation). A correlation plot is shown in Figure 3c; note that a strong correlation implies we can train architectures using GTN evaluation as an inexpensive surrogate. We ﬁnd that GTN evaluation enables predicting the performance of an architecture efﬁ- 6ciently. The rank-correlation between 128 steps of training with GTN-synthetic data vs. 100 epochs of real data is 0.3606. The correlation improves to 0.5582 when considering the top 50% of archi- tectures recommended by GTN evaluation scores, which is important because those are the ones that search would select. This improved correlation is slightly stronger than that from 3 epochs of training with real data (0.5235), a ∼9×cost-reduction per trained model. 20 40 60 80 100 120 Inner-loop Iterations 0.3 0.4 0.5 0.6 0.7Training Accuracy GTN Real Data (a) CIFAR10 inner-loop training  (b) CIFAR10 GTN samples 0.1 0.2 0.3 0.4 0.5 GTN Predicted Performance 0.90 0.91 0.92 0.93 0.94 0.95Real Data Predicted Perf.  (c) CIFAR10 correlation Figure 3: Teaching CIFAR10 with GTN-generated images. (a) CIFAR10 training set performance of the ﬁnal learner (after 1,700 meta-optimization steps) across inner-loop learning iterations. (b) Samples generated by GTN to teach CIFAR10 are unrecognizable, despite being effective for train- ing. Each column contains a different class, and each row is taken from the same inner-loop iteration (evenly spaced from all 128 iterations, early iterations at the top). (c) Correlation between perfor- mance prediction using GTN-data vs. Real Data. When considering the top half of architectures (as ranked by GTN evaluation), correlation between GTN evaluation and search evaluation is strong (0.5582 rank-correlation), suggesting that GTN-NAS has potential to uncover high performing ar- chitectures at a signiﬁcantly lower cost. Architectures shown are uniformly sampled from the NAS search space. The top 10% of architectures according to the GTN evaluation (blue squares)– those likely to be selected by GTN-NAS–have high true asymptotic accuracy. Architecture search methods are composed of several semi-independent components, such as the choice of search space, search algorithm, and proxy evaluation of candidate architectures. GTNs are proposed as an improvement to this last component, i.e. as a new way to quickly evaluate a new architecture. Thus we test our method under the standard search space for CIFAR10, using a simple form of search (random search) for which there are previous benchmark results. In particular, we ran an architecture search experiment where we evaluated 800 randomly generated architectures trained with GTN-synthetic data. We present the performance after ﬁnal evaluation of the best architecture found in Table 1. This experimental setting is similar to that of Zhang et al. (2018). Highlighting the potential of GTNs as an improved proxy evaluation for architectures, we achieve state-of-the-art results when controlling for search algorithm (the choice of which is orthogonal to our contribution). While it is an apples-to-oranges comparison, GTN-NAS is competitive even with methods that use more advanced search techniques than random search to propose architectures (Appendix E). GTN is compatible with such techniques, and would likely improve their performance, an interesting area of future work. Furthermore, because of the NAS search space, the modules GTN found can be used to create even larger networks. A further test of whether GTNs predictions generalize is if such larger networks would continue performing better than architectures generated by the real- data control, similarly scaled. We tried F=128 and show it indeed does perform better (Table 1), suggesting additional gains can be had by searching post-hoc for the correct F and N settings. 4 D ISCUSSION , FUTURE WORK , AND CONCLUSION The results presented here suggest potential future applications and extensions of GTNs. Given the ability of GTNs to rapidly train new models, they are particularly useful when training many independent models is required (as we showed for NAS). Another such application would be to teach networks on demand to realize particular trade-offs between e.g. accuracy, inference time, and memory requirements. While to address a range of such trade-offs would ordinarily require training many models ahead of time and selecting amongst them (Elsken et al., 2019), GTNs could instead rapidly train a new network only when a particular trade-off is needed. Similarly, agents with unique combinations of skills could be created on demand when needed. 7Table 1: Performance of different architecture search methods. Our results report mean ±SD of 5 evaluations of the same architecture with different initializations. It is common to report scores with and without Cutout (DeVries & Taylor, 2017), a data augmentation technique used during training. We found better architectures compared to other methods that reduce architecture evaluation speed and were tested with random search (Random Search+WS and Random Search+GHN). Increasing the width of the architecture found (F=128) further improves performance. Because each NAS method ﬁnds a different architecture, the number of parameters differs. Each method ran once. Model Error(%) #params GPU Days Random Search + GHN (Zhang et al., 2018) 4.3 ±0.1 5.1M 0.42 Random Search + Weight Sharing (Luo et al., 2018) 3.92 3.9M 0.25 Random Search + Real Data (baseline) 3.88 ±0.08 12.4M 10 Random Search + GTN (ours) 3.84 ±0.06 8.2M 0.67 Random Search + Real Data + Cutout (baseline) 3.02 ±0.03 12.4M 10 Random Search + GTN + Cutout (ours) 2.92 ±0.06 8.2M 0.67 Random Search + Real Data + Cutout (F=128) (baseline) 2.51 ±0.13 151.7M 10 Random Search + GTN + Cutout (F=128) (ours) 2.42 ±0.03 97.9M 0.67 Interesting questions are raised by the lack of similarity between the synthetic GTN data and real MNIST and CIFAR10 data. That unrealistic and/or unrecognizable images can meaningfully affect NNs is reminiscent of the ﬁnding that deep neural networks are easily fooled by unrecognizable images (Nguyen et al., 2015). It is possible that if neural network architectures were functionally more similar to human brains, GTNs’ synthetic data might more resemble real data. However, an alternate (speculative) hypothesis is that the human brain might also be able to rapidly learn an arbitrary skill by being shown unnatural, unrecognizable data (recalling the novel Snow Crash). The improved stability of training GTNs from weight normalization naturally suggests the hypoth- esis that weight normalization might similarly stabilize, and thus meaningfully improve, any tech- niques based on meta-gradients (e.g. MAML (Finn et al., 2017), learned optimizers (Metz et al., 2019), and learned update rules (Metz et al., 2018)). In future work, we will more deeply investigate how consistently, and to what degree, this hypothesis holds. Both weight sharing and GHNs can be combined with GTNs by using the shared weights or Hyper- Network for initialization of proposed learners and then ﬁne-tuning on GTN-produced data. GTNs could also be combined with more intelligent ways to propose which architecture to sample next such as NAO (Luo et al., 2018). Many other extensions would also be interesting to consider. GTNs could be trained for unsupervised learning, for example by training a useful embedding function. Additionally, they could be used to stabilize GAN training and prevent mode collapse (Appendix I shows encouraging initial results). One particularly promising extension is to introduce a closed- loop curriculum (i.e. one that responds dynamically to the performance of the learner throughout training), which we believe could signiﬁcantly improve performance. For example, a recurrent GTN that is conditioned on previous learner outputs could adapt its samples to be appropriately easier or more difﬁcult depending on an agent’s learning progress, similar in spirit to the approach of a human tutor. Such closed-loop teaching can improve learning (Fan et al., 2018). An additional interesting direction is having GTNs generate training environments for RL agents. Appendix H shows this works for the simple RL task of CartPole. That could be either for a pre- deﬁned target task, or could be combined with more open-ended algorithms that attempt to con- tinuously generate new, different, interesting tasks that foster learning (Clune, 2019; Wang et al., 2019a). Because GTNs can encode any possible environment, they (or something similar) may be necessary to have truly unconstrained, open-ended algorithms (Stanley et al., 2017). If techniques could be invented to coax GTNs to produce recognizable, human-meaningful training environments, the technique could also produce interesting virtual worlds for us to learn in, play in, or explore. This paper introduces a new method called Generative Teaching Networks, wherein data genera- tors are trained to produce effective training data through meta-learning. We have shown that such an approach can produce supervised datasets that yield better few-step accuracy than an equivalent amount of real training data, and generalize across architectures and random initializations. We leverage such efﬁcient training data to create a fast NAS method that generates state-of-the-art ar- chitectures (controlling for the search algorithm). While GTNs may be of particular interest to the 8ﬁeld of architecture search (where the computational cost to evaluate candidate architectures often limits the scope of its application), we believe that GTNs open up an intriguing and challenging line of research into a variety of algorithms that learn to generate their own training data. 5 A CKNOWLEDGEMENTS For insightful discussions and suggestions, we thank the members of Uber AI Labs, especially Theofanis Karaletsos, Martin Jankowiak, Thomas Miconi, Joost Huizinga, and Lawrence Murray. REFERENCES Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa- chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018. Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. arXiv preprint arXiv:1705.10823, 2017. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Jeff Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artiﬁcial intelligence. arXiv preprint arXiv:1905.10985, 2019. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Gamaleldin F. Elsayed, Ian J. Goodfellow, and Jascha Sohl-Dickstein. Adversarial reprogramming of neural networks. CoRR, abs/1806.11146, 2018. URL http://arxiv.org/abs/1806. 11146. Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efﬁcient multi-objective neural architecture search via lamarckian evolution. In International Conference on Learning Representations, 2019. Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. arXiv preprint arXiv:1805.03643, 2018. Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. arXiv preprint arXiv:1710.11622, 2017. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pp. 1126–1135. JMLR. org, 2017. JL Fleiss. Review papers: The statistical basis of meta-analysis. Statistical methods in medical research, 2(2):121–145, 1993. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pp. 2672–2680, 2014. Alex Graves, Marc G. Bellemare, Jacob Menick, R´emi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1311–1320, 2017. Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of check- pointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1):19–45, 2000. 9Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015. Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, and Pieter Abbeel. Evolved policy gradients. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 5405–5414. Curran Associates, Inc., 2018. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. In NIPS, 2017. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proceed- ings of the European Conference on Computer Vision (ECCV), pp. 19–34, 2018a. Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018b. Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. In Advances in neural information processing systems, pp. 7816–7827, 2018. Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural net- work acoustic models. In Proc. icml, volume 30, pp. 3, 2013. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti- mization through reversible learning. In Proceedings of the 32Nd International Conference on In- ternational Conference on Machine Learning - Volume 37, ICML’15, pp. 2113–2122. JMLR.org, 2015. Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning up- date rules for unsupervised representation learning. arXiv preprint arXiv:1804.00222, 2018. Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-dickstein. Learned optimizers that outperform on wall-clock and validation loss, 2019. V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928–1937, 2016. Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High con- ﬁdence predictions for unrecognizable images. In In Computer Vision and Pattern Recognition (CVPR ’15), 2015. Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efﬁcient neural architecture search via parameters sharing. In Jennifer Dy and Andreas Krause (eds.),Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4095–4104, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018. PMLR. Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classiﬁer architecture search. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 33, pp. 4780–4789, 2019. 10Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems , pp. 901–909, 2016. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, 2016. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations, 2018. Burr Settles. Active learning literature survey. Technical report, 2010. Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. Journal of computer and system sciences, 50(1):132–150, 1995. Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 3308–3318. Curran Associates, Inc., 2017. Kenneth O. Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge youve never heard of. O’Reilly Online, 2017. URL https://www.oreilly.com/ideas/ open-endedness-the-last-grand-challenge-youve-never-heard-of . Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do- main randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pp. 23–30. IEEE, 2017. Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. Ivor Tsang, James Kwok, and Pak-Ming Cheung. Core vector machines: Fast svm training on very large data sets. Journal of Machine Learning Research, 6:363–392, 04 2005. Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019a. Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. Dataset distillation, 2019b. Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for neural architecture search. arXiv preprint arXiv:1810.05749, 2018. Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. 2017. URL https://arxiv.org/abs/1611.01578. 11APPENDIX A A DDITIONAL EXPERIMENTAL DETAILS The outer loop loss function is domain speciﬁc. In the supervised experiments on MNIST and CIFAR, the outer loop loss was cross-entropy for logistic regression on real MNIST or CIFAR data. The inner-loop loss matches the outer-loop loss, but with synthetic data instead of real data. Appendix H describes the losses for the RL experiments. The following equation deﬁnes the inner-loop SGD with momentum update for the learner parame- ters θt. θt+1 = θt −α ∑ 0≤t′≤t βt−t′ ∇ℓinner(G(zt′ ,yt′ ),yt′ ,θt′ ), (1) where αand β are the learning rate and momentum hyperparameters, respectively. zt is a batch of noise vectors that are input to the generator and are sampled from a unit-variance Gaussian.yt are a batch of labels for each generated sample/input and are sampled uniformly from all available class labels. Instead of randomly samplingzt, we can also learn a curriculum by additionally optimizingzt directly and keeping yt ﬁxed throughout all of training. Results for both approaches (and additional curriculum ablations) are reported in Section 3.2. A.1 MNIST E XPERIMENTS : For the GTN training for MNIST we sampled architectures from a distribution that produces ar- chitectures with convolutional (conv) and fully-connectd (FC) layers. All architectures had 2 conv layers, but the number of ﬁlters for each layer was sampled uniformly from the ranges U([32,128]) and U([64,256]), respectively. After each conv layer there is a max pooling layer for dimensionality reduction. After the last conv layer, there is a fully-connected layer with number of ﬁlters sampled uniformly from the range U([64,256]). We used Kaiming Normal initialization (He et al., 2015) and LeakyReLUs (Maas et al., 2013) (with α= 0.1). We use BatchNorm (Ioffe & Szegedy, 2015) for both the generator and the learners. The BatchNorm momentum for the learner was set to 0 (meta-training consistently converged to small values and we saw no signiﬁcant gain from learning the value). The generator consisted of 2 FC layers (1024 and 128 ∗H/4 ∗H/4 ﬁlters, respectively, where H is the ﬁnal width of the synthetic image). After the last FC layer there are 2 conv layers. The ﬁrst conv has 64 ﬁlters. The second conv has 1 ﬁlter followed by a Tanh. We found it particularly important to normalize (mean of zero and variance of one) all datasets. Hyperparameters are shown in Table 2. Hyperparameter Value Learning Rate 0.01 Initial LR 0.02 Initial Momentum 0.5 Adam Beta 1 0.9 Adam Beta 2 0.999 Size of latent variable 128 Inner-Loop Batch Size 128 Outer-Loop Batch Size 128 Table 2: Hyperparameters for MNIST experiments A.2 CIFAR10 E XPERIMENTS : For GTN training for CIFAR-10, the template architecture is a small learner with 5 convolutional layers followed by a global average pooling and an FC layer. The second and fourth convolution had stride=2 for dimensionality reduction. The number of ﬁlters of the ﬁrst conv layer was sam- pled uniformly from the range U([32,128]) while all others were sampled uniformly from the range U([64,256]). Other details including the generator architecture were the same as the MNIST exper- iments, except the CIFAR generator’s second conv layer had 3 ﬁlters instead of 1. Hyperparameters 12used can be found in Table 3. For CIFAR10 we augmented the real training set when training GTNs with random crops and horizontal ﬂips. We do not add weight normalization to the ﬁnal architectures found during architecture search, but we do so when we train architectures with GTN-generated data during architecture search to provide an estimate of their asymptotic performance. Hyperparameter Value Learning Rate 0.002 Initial LR 0.02 Initial Momentum 0.5 Adam Beta 1 0.9 Adam Beta 2 0.9 Adam ϵ 1e-5 Size of latent variable 128 Inner-loop Batch Size 128 Outer-loop Batch Size 256 Table 3: Hyperparameters for CIFAR10 experiments APPENDIX B R EASONS GTN S ARE NOT EXPECTED TO PRODUCE SOTA ACCURACY VS . ASYMPTOTIC PERFORMANCE WHEN TRAINING ON REAL DATA There are three reasons not to expect SOTA accuracy levels for the learners trained on synthetic data: (1) we train for very few SGD steps (32 or 128 vs. tens of thousands), (2) SOTA performance results from architectures explicitly designed (with much human effort) to achieve record accuracy, whereas GTN produces compressed training data optimized to generalize across diverse architectures with the aim of quickly evaluating a new architecture’s potential, and (3) SOTA methods often use data outside of the benchmark dataset and complex data-augmentation schemes. APPENDIX C C ELL SEARCH SPACE When searching for the operations in a CNN cell, the 11 possible operations are listed below. • identity • 1 ×1 convolution • 3 ×3 convolution • 1 ×3 + 3×1 convolution • 1 ×7 + 7×1 convolution • 2 ×2 max pooling • 3 ×3 max pooling • 5 ×5 max pooling • 2 ×2 average pooling • 3 ×3 average pooling • 5 ×5 average pooling APPENDIX D C OMPUTATION AND MEMORY COMPLEXITY With the traditional training of DNNs with back-propagation, the memory requirements are pro- portional to the size of the network because activations during the forward propagation have to be stored for the backward propagation step. With meta-gradients, the memory requirement also grows with the number of inner-loop steps because all activations and weights have to be stored for the 132nd order gradient to be computed. This becomes impractical for large networks and/or many inner- loop steps. To reduce the memory requirements, we utilize gradient-checkpointing (Griewank & Walther, 2000) by only storing the computed weights of learner after each inner-loop step and re- computing the activations during the backward pass. This trick allows us to compute meta-gradients for networks with 10s of millions of parameters over hundreds of inner-loop steps in a single GPU. While in theory the computational cost of computing meta-gradients with gradient-checkpointing is 4x larger than computing gradients (and 12x larger than forward propagation), in our experiments it is about 2.5x slower than gradients through backpropagation due to parallelism. We could further reduce the memory requirements by utilizing reversable hypergradients (Maclaurin et al., 2015), but, in our case, we were not constrained by the number of inner-loop steps we can store in memory. APPENDIX E E XTENDED NAS RESULTS In the limited computation regime (less than 1 day of computation), the best methods were, in order, GHN, ENAS, GTN, and NAONet with a mean error of 2.84%, 2.89%, 2.92%, and 2.93%, respectively. A 0.08% difference on CIFAR10 represents 8 out of the 10k test samples. For that reason, we consider all of these methods as state of the art. Note that out of the four, GTN is the only one relying on Random Search for architecture proposal. Table 4: Performance of different architecture search methods. Search with our method required 16h total, of which 8h were spent training the GTN and 8h were spent evaluating 800 architectures with GTN-produced synthetic data. Our results report mean ±SD of 5 evaluations of the same architec- ture with different initializations. It is common to report scores with and without Cutout (DeVries & Taylor, 2017), a data augmentation technique used during training.We found better architectures compared to other methods using random search (Random-WS and GHN-Top) and are competitive with algorithms that beneﬁt from more advanced search methods (e.g. NAONet and ENAS employ non-random architecture proposals for performance gains; GTNs could be combined with such non- random proposals, which would likely further improve performance). Increasing the width of the architecture found (F=128) further improves performance. Model Error(%) #params Random GPU Days NASNet-A (Zoph & Le, 2017) 3.41 3.3M \u0017 2000 AmoebaNet-B + Cutout (Real et al., 2019) 2.13 34.9M \u0017 3150 DARTS + Cutout (Liu et al., 2018b) 2.83 4.6M \u0017 4 NAONet + Cutout (Luo et al., 2018) 2.48 10.6M \u0017 200 NAONet-WS (Luo et al., 2018) 3.53 2.5M \u0017 0.3 NAONet-WS + Cutout (Luo et al., 2018) 2.93 2.5M \u0017 0.3 ENAS (Pham et al., 2018) 3.54 4.6M \u0017 0.45 ENAS + Cutout (Pham et al., 2018) 2.89 4.6M \u0017 0.45 GHN Top-Best + Cutout (Zhang et al., 2018) 2.84 ±0.07 5.7M \u0017 0.84 GHN Top (Zhang et al., 2018) 4.3 ±0.1 5.1M ✓ 0.42 Random-WS (Luo et al., 2018) 3.92 3.9M ✓ 0.25 Random Search + Real Data (baseline) 3.88 ±0.08 12.4M ✓ 10 RS + Real Data + Cutout (baseline) 3.02 ±0.03 12.4M ✓ 10 RS + Real Data + Cutout (F=128) (baseline) 2.51 ±0.13 151.7M ✓ 10 Random Search + GTN (ours) 3.84 ±0.06 8.2M ✓ 0.67 Random Search + GTN + Cutout (ours) 2.92 ±0.06 8.2M ✓ 0.67 RS + GTN + Cutout (F=128) (ours) 2.42 ±0.03 97.9M ✓ 0.67 APPENDIX F C ONDITIONED GENERATOR VS . XY-G ENERATOR Our experiments in the main paper conditioned the generator to create data with given labels, by concatenating a one-hot encoded label to the input vector. We also explored an alternative approach where the generator itself produced a target probability distribution to label the data it generates. Because more information is encoded into a soft label than a one-hot encoded one, we expected an improved training set to be generated by this variant. Indeed, such a “dark knowledge” dis- tillation setup has been shown to perform better than learning from labels (Hinton et al., 2015). 14However, the results in Figure 4 indicate that jointly generating both images and their soft labels under-performs generating only images, although the result could change with different hyperpa- rameter values and/or innovations that improve the stability of training. 0 500 1000 1500 2000 2500 3000 3500 Outer-loop Steps 0.800 0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000Validation Accuracy Real Data GTN DK Figure 4: Comparison between a conditional generator and a generator that outputs an image/label pair. We expected the latter “dark knowledge” approach to outperform the conditional generator, but that does not seem to be the case. Because initialization and training of the dark knowledge variant were more sensitive, we believe a more rigorous tuning of the process could lead to a different result. APPENDIX G GTN GENERATES (SEEMINGLY ) ENDLESS DATA While optimizing images directly (i.e. optimizing a ﬁxed tensor of images) would result in a ﬁxed number of samples, optimizing a generator can potentially result in an unlimited amount of new samples. We tested this generative capability by generating more data during evaluation (i.e. with no change to the meta-optimization procedure) in two ways. In the ﬁrst experiment, we increase the amount of data in each inner-loop optimization step by increasing the batch size (which results in lower variance gradients). In the second experiment, we keep the number of samples per batch ﬁxed, but increase the number of inner-loop optimization steps for which a new network is trained. Both cases result in an increased amount of training data. If the GTN generator has overﬁt to the number of inner-loop optimization steps during meta-training and/or the batch size, then we would not expect performance to improve when we have the generator produce more data. However, an alternate hypothesis is that the GTN is producing a healthy distribution of training data, irrespective of exactly how it is being used. Such a hypothesis would be supported by performance increase in these experiments. Figure 5a shows performance as a function of increasing batch size (beyond the batch size used during meta-optimization, i.e. 128). The increase in performance of GTN means that we can sample larger training sets from our generator (with diminishing returns) and that we are not limited by the choice of batch size during training (which is constrained due to both memory and computation requirements). Figure 5b shows the results of generating more data by increasing the number of inner-loop opti- mization steps. Generalization to more inner-loop optimization steps is important when the number of inner-loop optimization steps used during meta-optimization is not enough to achieve maximum performance. This experiment also tests the generalization of the optimizer hyperparameters be- cause they were optimized to maximize learner performance after a ﬁxed number of steps. There is an increase in performance of the learner trained on GTN-generated data as the number of inner- loop optimization steps is increased, demonstrating that the GTN is producing generally useful data instead of overﬁtting to the number of inner-loop optimization steps during training (Figure 5b). Extending the conclusion from Figure 2b, in the very low data regime, GTN is signiﬁcantly better than training on real data (p< 0.05). However, as more inner-loop optimization steps are taken and thus more unique data is available to the learner, training on the real data becomes more effective than learning from synthetic data (p< 0.05) (see Figure 5b). 15150 200 250 300 350 400 450 500 Inner loop batch size 0.915 0.920 0.925 0.930 0.935 0.940 0.945 0.950Validation Accuracy Real Data GTN (a) Increasing inner-loop batch size 20 40 60 80 100 120 Inner-loop Steps 0.92 0.93 0.94 0.95 0.96 0.97Validation Accuracy Real Data GTN (b) Increasing inner-loop optimization steps Figure 5: (a) The left ﬁgure shows that even though GTN was meta-trained to generate synthetic data of batch size 128, sampling increasingly larger batches results in improved learner performance (the inner-loop optimization steps are ﬁxed to 16). (b) The right ﬁgure shows that increasing the number of inner-loop optimization steps (beyond the 16 steps used during meta-training) improves learner performance. The performance gain with real data is larger in this setting. This improvement shows that GTNs do not overﬁt to a speciﬁc number of inner-loop optimization steps. Figure 6: GTN samples w/o curriculum. Another interesting test for our generative model is to test the distribution of learners after they have trained on the synthetic data. We want to know, for instance, if training on synthetic samples from one GTN results in a functionally similar set of learner weights regardless of learner initialization (this phenomena can be called learner mode collapse). Learner mode collapse would prevent the performance gains that can be achieved through ensembling diverse learners. We tested for learner mode collapse by evaluating the performance (on held-out data and held-out architecture) of an en- semble of 32 randomly initialized learners that are trained on independent batches from the same GTN. To construct the ensemble, we average the predicted probability distributions across the learn- ers to compute a combined prediction and accuracy. The results of this experiment can be seen in Figure 7, which shows that the combined performance of an ensemble is better (on average) than an individual learner, providing additional evidence that the distribution of synthetic data is healthy and allows ensembles to be harnessed to improve performance, as is standard with networks trained on real data. APPENDIX H GTN FOR RL To demonstrate the potential of GTNs for RL, we tested our approach with a small experiment on the classic CartPole test problem (see Brockman et al. (2016) for details on the domain. We conducted this experiment before the discovery that weight normalization improves GTN training, so these experiments do not feature it; it might further improve performance. For this experiment, the meta-objective the GTN is trained with is the advantage actor-critic formulation: log π(a|θπ)(R− V(s; θv)) (Mnih et al., 2016). The state-value V is provided by a separate neural network trained to estimate the average state-value for the learners produced so far during meta-training. The learners train on synthetic data via a single-step of SGD with a batch size of 512 and a mean squared error 160 500 1000 1500 2000 2500 3000 3500 Outer-loop Steps 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00Validation Accuracy Real Data - single Real Data - ensemble GTN - single GTN - ensemble Figure 7: Performance of an ensemble of GTN learners vs. individual GTN learners. Ensembling a set of neural networks that each had different weight initializations, but were trained on data from the same GTN substantially improves performance. This result provides more evidence that GTNs generate a healthy distribution of training data and are not somehow forcing the learners to all learn a functionally equivalent solution. regression loss, meaning the inner loop is supervised learning. The outer-loop is reinforced because the simulator is non-differentiable. We could have also used an RL algorithm in the inner loop. In that scenario the GTN would have to learn to produce an entire synthetic world an RL agent would learn in. Thus, it would create the initial state and then iteratively receive actions and generate the next state and optionally a reward. For example, a GTN could learn to produce an entire MDP that an agent trains on, with the meta-objective being that the trained agent then performs well on a target task. We consider such synthetic (PO)MDPs an exciting direction for future research. The score on CartPole is the number of frames out of 200 for which the pole is elevated. Both GTN and an A2C (Mnih et al., 2016) control effectively solve the problem (Figure 8). Interestingly, training GTNs takes the same number of simulator steps as training a single learner with policy- gradients (Figure 8). Incredibly, however, once trained, the synthetic data from a GTN can be used to train a learner to maximum performance in a single SGD step! While that is unlikely to be true for harder target RL tasks, these results suggest that the speed-up for architecture search from using GTNs in the RL domain can be even greater than in supervised domain. The CartPole experiments feature a single-layer neural network with 64 hidden units and a tanh activation function for both the policy and the value network. The inner-loop batch size was 512 and the number of inner-loop training iterations was 1. The observation space of this environment consists of a real-valued vector of size 4 (Cart position, Cart velocity, Pole position, Pole velocity). The action space consists of 2 discrete actions (move left or move right). The outer loop loss is the reward function for the target domain (here, pole-balancing). The inner loop loss is mean squared error (i.e. the network is doing supervised learning on the state-action mapping pairs provided by the GTN). APPENDIX I S OLVING MODE COLLAPSE IN GAN S WITH GTN S We created an implementation of generative adversarial networks (GANs) (Goodfellow et al., 2014) and found they tend to generate the same class of images (e.g. only 1s, Figure 9), which is a common training pathology in GANs known as mode collapse (Srivastava et al., 2017). While there are tech- niques to prevent mode collapse (e.g. minibatch discrimination and historical averaging (Salimans et al., 2016)), we hypothesized that combining the ideas behind GTNs and GANs might provide a different, additional technique to help combat mode collapse. The idea is to add a discriminator to the GTN forcing the data it generates to both be realistic and help a learner perform well on the meta-objective of classifying MNIST. The reason this approach should help prevent mode collapse is that if the generator only produces one class of images, a learner trained on that data will not be able to classify all classes of images. This algorithm (GTN-GAN) was able to produce realistic images with no identiﬁable mode collapse (Figure 10). GTNs offer a different type of solution to the issue of mode collapse than the many that have been proposed, adding a new tool to our toolbox 170 20000 40000 60000 80000 100000 Environment Steps 25 50 75 100 125 150 175 200Reward A2C Agent A2C + GTN Agent Figure 8: An A2C Agent control trains a single policy throughout all of training, while the GTN method starts with a new, randomly initialized network at each iteration and produces the plotted performance after a single step of SGD . This plot is difﬁcult to parse because of that difference: it compares the accumulated performance of A2C across all environment steps up to that point vs. the performance achieved with GTN data in a single step of SGD from a single batch of synthetic data. Thus, at the 100,000 th step of training, GTNs enable training a newly initialized network to the given performance (of around 190) 100,000 times faster with GTN synthetic data than with A2C from scratch. With GTNs, we can therefore train many new, high-performing agents quickly. That would be useful in many ways, such as greatly accelerating architecture search algorithms for RL. Of course, these results are on a simple problem, and (unlike our supervised learning experiments) have not yet shown that the GTN data works with different architectures, but these results demonstrate the intriguing potential of GTNs for RL. One reason we might expect even larger speedups for RL vs. supervised learning is because a major reason RL is sample inefﬁcient is because it requires exploration to ﬁgure out how to solve the problem. However, once that exploration has been done, the GTN can produce data to efﬁciently teach that solution to a new architecture. RL thus represents an exciting area of future research for GTNs. Performing that research is beyond the scope of this paper, but we highlight the intriguing potential here to inspire such future work. for solving that problem. Note we do not claim this approach is better than other techniques to prevent mode collapse, only that it is an interesting new type of option, perhaps one that could be productively combined with other techniques. Figure 9: Images generated by a basic GAN on MNIST before and after mode collapse. The left image shows GAN-produced images early in GAN training and the right image shows GAN samples later in training after mode collapse has occurred due to training instabilities. APPENDIX J A DDITIONAL MOTIVATION There is an additional motivation for GTNs that involves long-term, ambitious research goals: GTN is a step towards algorithms that generate their own training environments, such that agents trained in them eventually solve tasks we otherwise do not know how to train agents to solve (Clune, 2019). It is important to pursue such algorithms because our capacity to conceive of effective training en- vironments on our own as humans is limited, yet for our learning algorithms to achieve their full potential they will ultimately need to consume vast and complex curricula of learning challenges 18Figure 10: Images generated by a GTN with an auxiliary GAN loss. Combining GTNs with GANs produces far more realistic images than GTNs alone (which produced alien, unrecognizable images, Figure 6). The combination also stabilizes GAN training, preventing mode collapse. and data. Algorithms for generating curricula, such as the the paired open-ended trailblazer (POET) algorithm (Wang et al., 2019a), have proven effective for achieving behaviors that would otherwise be out of reach, but no algorithm yet can generate completely unconstrained training conditions. For example, POET searches for training environments within a highly restricted preconceived space of problems. GTNs are exciting because they can encode a rich set of possible environments with min- imal assumptions, ranging from labeled data for supervised learning to (in theory) entire complex virtual RL domains (with their own learned internal physics). Because RNNs are Turing-complete (Siegelmann & Sontag, 1995), GTNs should be able to theoretically encode all possible learning environments. Of course, while what is theoretically possible is different from what is achievable in practice, GTNs give us an expressive environmental encoding to begin exploring what potential is unlocked when we can learn to generate sophisticated learning environments. The initial results presented here show that GTNs can be trained end-to-end with gradient descent through the entire learning process; such end-to-end learning has proven highly scalable before, and may similarly in the future enable learning expressive GTNs that encode complex learning environments. APPENDIX K O N THE REALISM OF IMAGES There are two phenomenon related to the recognizability of the GTN-generated images that are interesting. (1) Many of the images generated by GTNs are unrecognizable (e.g. as digits), yet a network trained on them still performs well on a real, target task (e.g. MNIST). (2) Some conditions increase the realism (recognizability) of the images. We will focus on the MNIST experiments because that is where we have conducted experiments to investigate this phenomenon. Figure 12 shows all of the images generated by a GTN with a curriculum. Most of the images do not resemble real MNIST digits, and many are alien and unrecognizable. Interestingly, there is a qualitative change in the recognizability of the images at the very end of the curriculum (the last 4-5 rows, which show the last two training batches). Both phenomena are interesting, and we do not have satisfactory explanations for either. Here we present many hypothesis we have generated that could explain these phenomenon. We also present a few experiments we have done to shed light on these issues. A more detailed investigation is an interesting area for future research. Importantly, the recognizable images at the end of the curriculum are not required to obtain high performance on MNIST. The evidence for that fact is in Figure 2, which shows that the performance of a learner trained on GTN-data is already high after around 23 inner-loop iterations, before the network has seen the recognizable images in the last 4-5 rows (which are shown in the last two training batches, i.e. training iterations 31 and 32). Thus, a network can learn to get over 98% accuracy on MNIST training only on unrecognizable images. At a high level, there are three possible camps of explanation for these phenomenon. Camp 1. Performance would be higher with higher realism, but optimization difﬁculties (e.g. vanishing/exploding gradients) prevent learning a generator that produces such higher- performing, more realistic images.Evidence in favor of this camp of hypotheses is that the realistic images come at the end of the curriculum, where the gradient ﬂow is easiest (as gradients do not have to ﬂow back through multiple inner-loop steps of learning). A prediction of this hypothesis is that as we improve our ability to train GTNs, the images will become more realistic. 19Camp 2. Performance is higher with lower realism (at least when not late in the curriculum), which is why unrealistic images are produced. There are at least two reasons why unrealistic images could generate higher performance. (A) Compression enables faster learning (i.e. learning with fewer samples). Being able to produce unrealistic images allows much more information to be packed into a single training example. For example, imagine a single image that could teach a network about many different styles of the digit 7 all at the same time (and/or different translations, rotations, and scales of a 7). It is well known that data augmentation improves performance because it teaches a network, for example, that the same image at different locations in the image is of the same class. It is conceivable that a single image could do something similar by showing multiple 7s at different locations. (B) Unrealistic images allow better generalization. When trying to produce high performance with very few samples, the risk of performance loss due to overﬁtting is high. A small set of realistic images may not have enough variation in non-essential aspects of the image (e.g. the background color) that allow a network to reliably learn the class of interest in a way that will generalize to instances of that class not in the training set (e.g. images of that class with a background color not in the training set). With the ability to produce unrealistic images (e.g. 7s against many different artiﬁcial backdrops, such as by adding seemingly random noise to the background color), GTNs could prevent the network from overﬁtting to spurious correlations in the training set (e.g. background color). In other words, GTNs couldlearn to produce something similar to domain randomization (Tobin et al., 2017; Andrychowicz et al., 2018) to improve generalization, an exciting prospect. Camp 3. It makes no difference on performance whether the images are realistic, but there are more unrealistic images that are effective than realistic ones, explaining why they tend to be produced. This hypothesis is in line with the fact that deep neural networks are easily fooled (Nguyen et al., 2015) and susceptible to adversarial examples (Szegedy et al., 2013). The idea is that images that are unrecognizeable to us are surprisingly meaningful to (i.e. impactful on) DNNs. This hypothesis is also in line with the fact that images can be generated that hack a trained DNN to cause it to perform other functions it was not trained to do (e.g. to perform a different function entirely, such as hacking an ImageNet classiﬁcation network to perfom a counting task like counting the number of occurences of Zebras in an image) (Elsayed et al., 2018). This hypothesis is also in line with recent research into meta-learning, showing that an initial weight vector can be carefully chosen such that it will produce a desired outcome (including implementing any learning algorithm) once subjected to data and SGD (Finn et al., 2017; Finn & Levine, 2017). One thing not explained by this hypothesis is why images at the end of the curriculum are more recognizable. Within this third camp of hypotheses is the possibility that the key features required to recognize a type of image (e.g. a 7) could be broken up across images. For example, one image could teach a network about the bottom half of a 7 and another about the top half. Recognizing either on its own is evidence for a seven, and if across a batch or training dataset the network learned to associate both features with the class 7, there is no reason that both the top half and bottom half ever have to co-occur. That could lead to unrealistic images with partial features. One prediction of this hypothesis (although one not exclusive to this hypothesis), is that averaging all of the images for each class across the entire GTN-produced training set should reveal recognizable digits. The idea is that no individual image contains a full seven, but on average the images combine to produce sevens (and the other digits). Figure 11 shows the results of this experiment. On average the digits are recognizable. This result is also consistent with Camp 1 of hypotheses: perhaps performance would increase further if the images were individually more recognizable. It is also consistent with Camp 2: perhaps the network is forced to combine many sevens into each image, making them individually unrecognizeable, but recognizable as 7s on average. Additionally, in line with Camp 2, if the network has learned to produce something like domain randomization, it could add variation across the dataset in the background (making each individual image less recognizable), but Hypothesis 2 would predict that, on average, the aspects of the image that do not matter (e.g. the background) average out to a neutral value or the true dataset mean (for MNIST, black), whereas the true class information (e.g. the digit itself) would be recognizable on average, exactly as we see in Figure 11. Thus, the average images shed light on the overall subject, but do not provide conclusive results regarding which camp of hypotheses is correct. An additional experiment we performed was to see if the alien images somehow represent the edges of the decision boundaries between images. The hypothesis is that images in the center of a cluster (e.g. a Platonic, archetypal 7) are not that helpful to establish neural network decision boundaries 20between classes, and thus GTN does not need to produce many of them. Instead, it might bene- ﬁt by generating mostly edge cases to establish the decision boundaries, which is why the digits are mostly difﬁcult to recognize. To rephrase this hypothesis in the language of support vector machines, the GTN could be mostly producing the support vectors of each class, instead of more recognizable images well inside of each class (i.e. instead of producing many Platonic images with a high margin from the decision boundary). A prediction of this hypothesis is that the unrecog- nizable GTN-generated images should be closer to the decision boundaries than the recognizable GTN-generated images. To test this hypothesis, we borrow an idea and technique from Toneva et al. (2018), which argues that one way to identify images near (or far) from a decision boundary is to count the number of times that, during the training of a neural network, images in the training set have their classiﬁcation labels change. The intuition is that Platonic images in the center of a class will not have their labels change often across training, whereas images near the boundaries between classes will change labels often as the decision boundaries are updated repeatedly during training. We trained a randomly initialized network on real images (the results are qualitatively the same if the network is trained on the GTN-produced images). After each training step we classify the images in Figure 12 with the network being trained. We then rank the synthetic images from Figure 12 on the frequency that their classiﬁcation changed between adjacent SGD steps. Figure 15 presents these images reordered (in row-major order) according to the number of times the output label for that image changed during training. The recognizable images are all tightly clustered in this analysis, showing that there is a strong relationship between how recognizable an image is and how often its label changes during training. Interestingly, the images are not all the way at one end of the spectrum. However, keep in mind that many images in this sorted list are tied with respect to the number of changes (with ties broken randomly), and the number of ﬂips does not go up linearly with each row of the image. Figure 14 shows the number of label ﬂips vs. the order in this ranked list. The recognizable images on average have 2.0 label ﬂips (Figure 14, orange horizontal line), meaning that they are towards the extreme of images whose labels do not change often. This result is in line with the hypothesis that these are Platonic images well inside the class boundary. However, there are also many unrecognizable images whose labels do not ﬂip often, which is not explained by this hypothesis. Overall, this analysis suggests the discovery of something interesting, although much future work needs to be done to probe this question further. Why are images only realistic at the end of the curriculum? Separate from, but related to, the question of why most images are unrecognizable, is why the recognizable images are only produced at the end of the curriculum. We have come up with a few different hypotheses, but we do not know which is correct. (1) The gradients ﬂow best to those samples, and thus they become the most realistic (in line with Camp 1 of hypotheses above). (2) It helps performance for some reason to have realistic images right at the end of training, but realism does not help (Camp 3) or even hurts (Camp 2) earlier in the curriculum. For example, perhaps the Platonic images are the least likely to change the decision boundaries, allowing them to be used for ﬁnal ﬁne-tuning of the decision boundaries (akin to an annealed learning rate). In line with this hypothesis is that, when optimization cannot create a deterministic curriculum, realism seems to be higher on average (Figure 13). (3) The effect is produced by the decision to take the batch normalization (Ioffe & Szegedy, 2015) statistics from the ﬁnal batch of training. Batch normalization is a common technique to improve training. Following normal batch norm procedures, during inner-loop training the batch norm statistics (mean and variance) are computed per batch. However, during inner-loop testing/inference, the statistics are instead computed from the training set. In our experiments, we calculate these statistics from the last batch in the curriculum. Thus, if it helps performance on the meta-training test set (the inner loop test set performance the GTN is being optimized for) to have the statistics of that batch match the statistics of the target data set (which contains real images), there could be a pressure for those images to be more realistic. Contrary to this hypothesis, however, is the fact that realism increases in the last two batches of the curriculum, not just the last batch (most visible in Figure 2, which shows sample from each batch in a separate row). Another hypothesis (consistent with Camp 1 and Camp 3), is that producing ﬁrst unrealistic then realistic images might reﬂect how neural networks learn (e.g. ﬁrst learning low-level ﬁlters before moving to more complex examples). However, that hypothesis would presumably predict a gradual increase in realism across the curriculum, instead of realism only sharply increasing in the last few batches. Finally, we did not observe this phenomenon in the CIFAR experiments with a full 21curriculum: the last few batches are not realistic in that experiment (Figure 3b). We do not know why the results on this front are different between MNIST and CIFAR experiments. In short, we do not have a good understanding for why realism increases towards the end of the curriculum. Shedding more light on this issue is an interesting area for future research. Figure 11: Pixel-wise mean per class of all GTN-generated images from the full curriculum treat- ment. 22Figure 12: All images generated by the full-curriculum GTN. The images are shown in the order they are presented to the network, with the ﬁrst batch of images in the curriculum in the top row and the last batch of data in the last row. The batch size does not correspond to the number of samples per row, so batches wrap from the right side of one row to the left side of the row below. 23Figure 13: A sample of images generated by the no-curriculum GTN. 240 1000 2000 3000 4000 Image Index 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0Label Switching Frequency Figure 14: The number of times a class label changes across training for each GTN-generated sample (Y-axis) vs. the rank of that sample when ordered by that same statistic (X-axis). A relatively small fraction of the samples ﬂip labels many times (in line with the idea that they are near the class decision boundaries), whereas most samples change labels only a few times (i.e. once the are learned, they stay learned, in line with them being more canonical examples). The orange line shows the average number of class changes for recognizable images (those in the red box in Figure 15). While not the images with the least number of ﬂips, these recognizable images are towards the end of the spectrum of images whose labels do not change often, in line with the hypothesis that they are more canonical class exemplars. 25Figure 15: All images generated by the full-curriculum GTN ordered by the frequency that their labels change during training. Highlighted is a dense region of realistic images that we manually identiﬁed. 26",
      "meta_data": {
        "arxiv_id": "1912.07768v1",
        "authors": [
          "Felipe Petroski Such",
          "Aditya Rawal",
          "Joel Lehman",
          "Kenneth O. Stanley",
          "Jeff Clune"
        ],
        "published_date": "2019-12-17T00:57:50Z",
        "pdf_url": "https://arxiv.org/pdf/1912.07768v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces Generative Teaching Networks (GTNs), a novel meta-learning approach that trains a data-generating neural network to produce synthetic training data and environments. The main contributions include demonstrating that GTNs can substantially accelerate neural network learning (e.g., by a factor of 9) by generating synthetic data optimized for rapid 'few-step' training, rather than realistic mimicry of real data. A practical application is shown in accelerating Neural Architecture Search (NAS) evaluations, where GTN-NAS finds higher-performing architectures with significantly less computation than comparable state-of-the-art methods. The paper also demonstrates that weight normalization improves the stability of meta-gradient training, and that learning a curriculum for synthetic data presentation further enhances GTN effectiveness.",
        "methodology": "GTNs employ a nested optimization (meta-learning) scheme with inner and outer training loops. In the inner loop, a generator network G(z,y) takes Gaussian noise (z) and a label (y) to produce synthetic data (x) on which a learner network (e.g., a freshly initialized neural network) is trained for a few SGD steps. The inner-loop loss (e.g., cross-entropy) measures the learner's performance on this synthetic data. In the outer loop, the trained learner's parameters are evaluated on real training data to compute a meta-training loss. Gradients of this meta-loss are backpropagated through the entire inner-loop learning process to update the GTN's parameters (and inner-loop hyperparameters like learning rate and momentum). To stabilize meta-gradient training, weight normalization is applied to both generator and learner weights. Furthermore, a curriculum can be learned by optimizing the sequence of input noise vectors (zt) to the generator, providing control over the order of synthetic data presentation. Gradient-checkpointing is utilized to reduce memory requirements for computing meta-gradients.",
        "experimental_setup": "Experiments were conducted in supervised learning domains using MNIST and CIFAR10 datasets. For MNIST, a 50k/10k/10k split for training/validation/test was used. For CIFAR10, a 45k/5k/10k split was used. Evaluation primarily focused on 'few-step accuracy' (accuracy after 32 or 128 SGD steps) to measure rapid learning. GTNs were compared against 'Real Data' training and 'Dataset Distillation' methods. For NAS, the CIFAR10 image classification task was used with a search space based on Neural Architecture Optimization (Luo et al., 2018), evaluating randomly generated architectures. The GTN was pre-optimized using smaller 'proxy learners' (fully-convolutional networks) for 8 hours on a p6000 GPU before being applied to NAS architectures. Reinforcement learning potential was explored with a small experiment on the CartPole problem (Appendix H). All experiments were run 5 times, reporting mean and 95% confidence intervals using bootstrapping.",
        "limitations": "The GTN-generated images for MNIST and CIFAR10 were often visually unrealistic and unrecognizable, despite being effective for training, raising questions about the interpretability of generated data. GTNs are not expected to produce state-of-the-art asymptotic accuracy compared to training on full real datasets with extensive training, as their primary goal is rapid architecture evaluation, not maximizing performance from synthetic data alone. Training GTNs directly on complex NAS architectures was prohibitively slow, requiring the use of proxy learners for pre-optimization. The CartPole RL experiment did not include weight normalization, was limited to a simple problem, and did not demonstrate generalization across different architectures. The paper does not claim GTN-GANs are superior to existing mode collapse solutions, only offering a new approach. There are also open questions regarding why synthetic images are unrealistic but effective, and why realism increases towards the end of the learned curriculum for MNIST but not CIFAR10.",
        "future_research_directions": "Future research directions include developing closed-loop curricula for GTNs that dynamically adapt to a learner's progress, similar to a human tutor. Exploring GTNs for generating entire training environments for reinforcement learning agents, including synthetic (PO)MDPs, to advance AI-generating algorithms and open-ended learning is another promising avenue. Investigating GTNs for unsupervised learning, such as training useful embedding functions, is also suggested. Combining GTNs with GANs to stabilize training and prevent mode collapse could be further explored. The stabilizing effect of weight normalization on meta-gradient training should be investigated more broadly across other meta-learning techniques like MAML. Integrating GTNs with existing NAS methods, such as shared weights or advanced architecture proposal algorithms, to achieve further performance gains is also a key direction. Finally, GTNs could be used to create neural networks on-demand to meet specific design constraints or skill sets, and further research is needed to understand the lack of realism in generated data and the dynamics of curriculum learning with GTNs."
      }
    },
    {
      "title": "GACT: Activation Compressed Training for Generic Network Architectures",
      "abstract": "Training large neural network (NN) models requires extensive memory\nresources, and Activation Compressed Training (ACT) is a promising approach to\nreduce training memory footprint. This paper presents GACT, an ACT framework to\nsupport a broad range of machine learning tasks for generic NN architectures\nwith limited domain knowledge. By analyzing a linearized version of ACT's\napproximate gradient, we prove the convergence of GACT without prior knowledge\non operator type or model architecture. To make training stable, we propose an\nalgorithm that decides the compression ratio for each tensor by estimating its\nimpact on the gradient at run time. We implement GACT as a PyTorch library that\nreadily applies to any NN architecture. GACT reduces the activation memory for\nconvolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training\nwith a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We\nimplement GACT as a PyTorch library at\nhttps://github.com/LiuXiaoxuanPKU/GACT-ICML.",
      "full_text": "GACT: Activation Compressed Training for Generic Network Architectures Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael Mahoney, Alvin Cheung University of California, Berkeley xiaoxuan liu@berkeley.edu, jianfeic@tsinghua.edu.cn Abstract Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT’s approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1×, enabling training with a 4.2× to 24.7× larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML. 1 Introduction In recent years, we have witnessed the trend of using larger and larger neural network (NN) models to deliver improved accuracy and generalization in various machine learning tasks [1, 2]. However, training these models requires a considerable amount of on-device GPU memory. Unfortunately, the increase of GPU memory capacity has been relatively slow, leading to a fundamental barrier to the development of large NN models. Activation Compressed Training (ACT) is a promising approach to reduce the memory footprint of models during training. As all layers’ activations need to be kept in the memory for computing the gradients during training, ACT reduces memory consumption by compressing these saved activations. Prior work [3, 4, 5, 6] has shown the eﬀectiveness of ACT by reducing activation footprint by up to 12 ×with 2-bit activations. Although ACT has already demonstrated impressive compression capabilities, previous work on ACT is restricted to speciﬁc NN architectures. For example, ActNN [ 5] is a quantization framework for convolutional NNs only; Mesa [ 7] proposes a per head/layer quantization method for vision transformers; and AC-GC [ 6] derives convergence error bound for diﬀerent types of operators separately. Developing a generic ACT framework is challenging. Theoretically, convergence guarantees must be made without assumptions on the network architecture. Algorithmically, the framework should ﬁnd eﬀective compression strategies for all kinds of networks automatically. From the system perspective, the framework should support arbitrary NN operations, including user-deﬁned ones. In this work, we propose GACT, a general framework for ACT that is agnostic to the NN architecture. Neither specialized mathematical derivations nor customized implementation is needed to support diﬀerent operators. To enable this, we develop a general convergence theory by analyzing the stochastic gradient (SG) introduced by ACT. We show that the SG can be well approximated by a linearized version, which is unbiased to stochastic compressors. The variance of the linearized gradient has a particularly simple structure that allows a numerical algorithm to predict the variance given a compression strategy. Then, we generate the strategy by approximately solving an integer program. 1 arXiv:2206.11357v4  [cs.LG]  3 Sep 2022We implement our method as a library based on PyTorch that can be quickly integrated into real-world machine learning systems. The library also provides several optimization levels to explore the trade-oﬀ between memory and speed. We demonstrate the ﬂexibility and eﬃciency of GACT on various tasks, including image classiﬁcation, object detection, text, and graph node classiﬁcation. Our evaluation shows that GACT can reduce activation memory by up to 8.1 ×, enabling training with a 24.7 ×larger batch size on the same GPU. In sum, our main contributions are as follows: • We propose a general convergence theory for ACT. • We develop an algorithm that automatically estimates the sensitivity of each compressed tensor and selects the optimal compression strategy. • We build eﬃcient implementation of GACT in PyTorch with an easy-to-use API that can also be combined with other memory-saving techniques seamlessly. 2 Related Work Activation Compressed Training.ACT has been applied to convolutional NNs using diﬀerent compressors, such as quantizers [3, 4, 5], JPEG [8], or scientiﬁc data compression algorithms [ 9, 6]. ACT is also applied to transformers [7] and graph NNs [10]. However, the existing theory for ACT [ 3, 4, 5, 6] relies on the case-by-case analysis of speciﬁc network operators, such as convolution, ReLU, and batch normalization. It also requires dedicated implementations for each operator. On the contrary, GACT focuses on the generality of activation compressed training, not a speciﬁc quantizer design, which is the main topic of previous work. Instead of assuming that the network is a stack of layers, GACT formulates the problem as a computational graph of operators. This is general enough to cover transformers [11], graph NNs [12], second-order derivatives, and unknown future architectures. Reduced Precision Training. Apart from ACT, reduced precision training [13, 14, 15, 16, 17, 18] performs calculations directly on low precision data, reducing the computation cost and memory footprint simultaneously. To achieve this, specialized kernels are used to calculate on low precision data. In contrast, ACT only considers storage, and it can thus use more ﬂexible compression strategies and achieve a much better compression ratio with the same accuracy loss. Memory-Eﬃcient Training. Gradient checkpointing [19, 20] trades computation for memory by dropping some of the activations in the forward pass from memory and recomputing them in the backward pass. Swapping [21, 22, 23, 24] oﬄoads activation or model parameters to an external memory (e.g., CPU memory). Recent work [25] explores the possibility of combining the gradient checkpointing and swapping. All these methods save memory by storing fewer tensors on the GPU. In contrast, GACT compresses the saved tensors and is complementary to these approaches. Moreover, the generality of GACT enables easy combination with these methods, which we explore in this paper. 3 Formulation We ﬁrst present the mathematical formulation of our activation compressed training (ACT) framework. As we would like to develop a general ACT algorithm, applicable to a wide range of NN architectures, we make minimal assumptions on our formulation. Throughout the paper, we deﬁne the variance of a vector x as Var [x] = E [ ∥x∥2 ] −∥E[x]∥2. 3.1 Activation Compressed Training In this work, we abstract the forward propagation as two functions ℓ(x; θ) and h(x; θ). Both take a datum x and the model parameter θ as the input. The loss function ℓ(x; θ) outputs the loss of the network θ on datum x. The context function h(x; θ) outputs tensors to be stored in the memory for computing the gradients, which are referred as the context. Assume that the context consists of L tensors, where each tensor h(l)(x; θ) is represented by a ﬂattened Dl-dimensional vector. Denote h(x; θ) = ( h(l)(x; θ))L l=1. Our notations are somewhat unconventional in the sense that we do not explicitly deﬁne each layer’s activation. We do not even assume that there is a NN. It could be any computational graph that saves context tensors. Given a dataset X = {xn}N n=1, deﬁne the batch loss L(θ) := 1 N ∑N n=1 ℓ(x; θ). The dataset can be equivalently represented as an empirical data distribution pX(x) := 1 N ∑N n=1 δ(x−xn), where δ is the Dirac 2Type\tequation\there. ConvBNConv……ReLU ReLU……BNConvConv Compressed context tensors (GPU) ℓ(𝑥;𝜃) Pack hook Unpackhook context ℎ(𝑥;𝜃) Swap outSwap in 𝑄(ℎ(𝑥;𝜃)) Forward Computation Graph Backward Computation Graph CPU Memory bits 𝑔(𝑄ℎ(𝑥;𝜃);𝜃) Adaptive AlgorithmCompressorGACT Decompressor 𝑥,\t gradient𝑔(𝑄ℎ(𝑥;𝜃);𝜃) (𝜃)model Figure 1: The architecture of GACT. delta function. The batch loss can be written as L(θ) = EX[ℓ(x; θ)], where EX denotes for taking expectation over pX. The network is trained with stochastic gradient descent (SGD) [ 26]. Starting from an initial model θ0, at the t-th iteration, SGD updates the model with: θt+1 ←θt −η∇θℓ(x; θt), (1) where η is a learning rate, and the SG ∇θℓ(x; θ) is computed on a random datum x ∼pX. Notice that EX[∇θℓ(x; θ)] = ∇θL(θ), i.e., the SG is an unbiased estimator of the batch gradient ∇θL(θ). Crucially, the SG can be written in the form ∇θℓ(x; θt) = g(h(x; θt); θt). In other words, the back propagation only depends on the forward propagation through the context h(x; θt). The entire context must be kept in memory for computing the gradients. The context dominates the memory consumption in many applications. ACT reduces the training memory footprint by compressing the context. Let Q(h) be a compressor, which converts h to compact formats while keeping Q(h) ≈h. Then, ACT computes the gradient with compressed context: θt+1 ←θt −ηg(Q(h(x; θt)); θt). (2) We refer to g(Q(h(x; θt); θt) as the activation compressed (AC) gradient. ACT is signiﬁcantly more memory eﬃcient then the plain SGD, Eq. (1), since it only needs to store a compressed version of the context. Suppose the original context h(x; θt) consists of 32-bit ﬂoating point tensors, and Q(·) is a compressor which quantizes tensors to 2-bit integers, ACT will reduce the context memory by 16 ×. Fig. 1 illustrates the computational graph of ACT with these notations. In the following presentation, we might denote h(x,θ) simply by h when there is no confusion. 3.2 Convergence of ACT ACT is a lossy approximation of SGD, as it uses an approximate gradient g(Q(h); θ). Therefore, some kind of theoretical guarantee is required for ACT to be useful. Fortunately, analyzing ACT is made signiﬁcantly simpler by introducing an unbiased stochastic compressor Q(·), such that EQ[Q(x)] = x for any x. EQ[·] means taking expectation over the compressor. In this way, g(Q(h); θ) can be viewed as a stochastic estimator of the batch gradient ∇L(θ), but the randomness comes not only from the datum x but also the compressor Q(·). Therefore, ACT is still an SGD algorithm. Standard analytical tools for SGD [ 27] are applicable for studying ACT. SGD algorithms have particular good properties when the SG is unbiased. In our case, this means EQ[g(Q(h); θ)] = g(h; θ). However, the SG is biased general, even when the stochastic compressor itself is unbiased.1 The key technique in this work is to construct an unbiased approximation of the AC gradient by linearizing 1Consider the exampleg(h) =I(h ≥0.5), whereh ∈[0, 1] and its AC gradientg(Q(h)) =I(Q(h) ≥0.5) with the compressor Q(h) ∼Bernoulli(h). Then, E [g(Q(h))] =P(Q(h) = 1) =h ̸= g(h). 3the gradient function g(·; θ). Consider the ﬁrst-order Taylor expansion of g(·; θ) at h: ˆg(Q(h); h,θ) := g(h; θ) + J(h,θ)∆h, (3) where J(h,θ) := ∂g(h;θ) ∂h is a Jacobian matrix, ∆ h := Q(h) −h is the compression error. We further denote ˆgxθ(Q(h); h) := ˆg(Q(h); h,θ)|h=h(x;θ) and Jxθ(h) := J(h,θ)|h=h(x;θ) for short. Since E[∆h(x; θ)] = 0, ˆgxθ(Q(h); h) is an unbiased SG, Furthermore, the approximation error is small: Proposition 1. Assuming that g(h; θ) is twice diﬀerentiable w.r.t. h, and the second order derivative is bounded, then E[∥g(Q(h); θ) −ˆgxθ(Q(h); h)∥2] = O(VarQ[∆h]). Since ∆h itself is unbiased, VarQ[∆h] = EQ [ ∥∆h∥2 ] is simply the expected compression error. Prop. 1 implies that the linearization error is bounded by the compression error. The linearized gradient ˆg is accurate if the compression is accurate. Using ˆg as a bridge, we arrive in the following convergence theorem: Theorem 1. Assume that: A1. L(θ) is a continuous diﬀerentiable, ∇L(θ) is β-Lipschitz continuous. A2. L(θ) is bounded below by L∗. A3. g(h; θ) is diﬀerentiable w.r.t. h and ∃b> 0, s.t. ∀θ,E∥g(Q(h(x; θ)); θ) −ˆgxθ(Q(h); h)∥≤ b. A4. ∃σ2 >0, s.t., ∀θ, Var [ˆgxθ(Q(h); h)] ≤σ2. Then, for all η <1 2β, if we run ACT deﬁned as Eq. (2) for T iterations, then we have min t=0,...,T−1 E [ ∥∇L(θt)∥2 ] ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2 Remark: The analytical technique used in Thm. 1 is rather standard, see Thm. 4.8 in [ 27]. However, we consider the variance term σ2 of the linearized gradient, rather than the SG itself. This formulation brings better analytical properties and an adaptive algorithm for determining the compression scheme, as we shall see soon in Sec. 4. The convergence of ACT is aﬀected by both the linearization error (A3) and the variance of the unbiased gradient ˆg(·; θ) (A4). The latter is characterized as: Proposition 2. Var [ˆgxθ(Q(h); h)] = VarX[g(h; θ)] + EX[VarQ[ˆgxθ(Q(h); h)]] , where the second term on the RHS equals to EX[VarQ[Jxθ(h)∆h]] = O(VarQ[∆h]) . Prop. 2 separates the variance from diﬀerent noise sources. VarX[g(h(x,θ); θ)] is the variance raised by random sampling of data (“sampling variance”). EX[VarQ[Jxθ(h)∆h(x,θ)]] is the variance raised by compression. Now, the convergence in Thm. 1 is depicted by 3 b2 + ηβσ2. By Prop. 1, b2 = O(VarQ[∆h]2). By Prop. 2, σ2 = O(1) +O(VarQ[∆h]), since the sampling variance is not aﬀected by compression. Therefore, when the compression is accurate (∆ h →0), the impact of the linearization error is negligible, and the variance of the unbiased gradient dominates. ACT behaves as if the AC gradient is unbiased. 4 Adapting the Compression Rate In a network, some context tensors (such as those stored for computing the cross entropy loss) are extremely sensitive, a small amount of compression would result in diverged training, while other tensors are quite robust to compression. Therefore, we must apply diﬀerent amounts of compression for each context tensor. As a general framework, we have no prior knowledge of the users’ model architecture, so we designed an algorithm to infer the sensitivity for each context tensor and determine their compression rate automatically. There is a tradeoﬀ between the compression error and the storage requirement. We represent the storage requirement of the compressed context in bits per dimension . We assume that bl bits/dim. are used for compression h(l), and Qbl(h(l)) be the compression result. Let b = (bl)L l=1 be a compression scheme, Qb(h) := {Qbl(h(l))}L l=1, and ∆bh= Qb(h) −h. 44.1 Structure of Variance As discussed in Sec. 3.2, when the compression is relatively accurate, the variance plays the main role in determining the convergence. Therefore, we would like to investigate how the compression scheme would impact the variance. Formally, we are interested in: V(b; h,θ) := VarQ[ˆg(Qb(h); h,θ)] . Once V(b,h; θ) is known, we can ﬁnd the minimum variance compression scheme under a given total bits budget B, by solving the integer programming problem: min b V(b; h(x; θ),θ), s.t. L∑ i=1 blDl ≤B, (4) where Dl is the dimensionality of h(l). To proceed, we need the following assumptions on the compressor Qb(·): Assumption B1: The compressed result is element-wise uncorrelated. That is, for anyi̸= j, Cov [Qb(h)i,Qb(h)j] = 0. Assumption B2: For compressing h(l)(x; θ) to bl bits/dim., the compression error can be written in the form Var [ ∆blh(l)(x; θ)j ] ≤Rlj(x; θ)S(bl), where S(bl) is a known function. This isolates the eﬀect of bl through the unary factor S(bl). Both assumptions can be achieved by a stochastic rounding quantizer [28], where Rlj(x; θ) = 1 4 ( maxkh(l) k −minkh(l) k )2 and S(b) = (2bl −1)−2. See Appendix A.4 for the derivations. The following theorem reveals the structure of the variance: Theorem 2. Under assumptions B1, B2, there exists a family of functions {cl(h,θ)}L l=1, such that the compression variance can be written in the form V(b; h,θ) ≤ L∑ l=1 cl(h,θ)S(bl). (5) 4.2 Computing Sensitivity Thm. 2 reveals two good properties of the variance: (1) the impact of compressing diﬀerent context tensors simply sums up, without aﬀecting each other; and (2) the compression scheme only impacts the variance through S(bl). Both properties are brought about by linearization. Since S(·) is a known function, we only need to know cl(h,θ) to solve problem Eq. (4). cl(h,θ) can be understood as the sensitivity of the AC gradient to the compression of the l-th tensor. We can compute cl(h,θ) numerically by leveraging the idempotence of compressing a tensor: Assumption B3: If h= Q(h′) for some h′with non-zero probability, then Q(h) = h and VarQ[Q(h)] = 0. Let Q¬(l) b (h) = {Qb1 (h(1)),...,h (l),...,Q bL(h(L))}be some tensors, where every tensor except h(l) is compressed. Plug h= Q¬(l) b (h) into Eq. (5), and use B3, we have V(b; Q¬(l) b (h),θ) ≤cl(Q¬(l) b (h),θ)S(bl). The left hand side can be approximated by taking ˆg(Qb(h); h,θ) ≈g(Qb(h); θ). Assume that cl(·,θ) is reasonably continuous, we have cl(h,θ) ≈VarQ[g(Qb(h); θ)] |h=Q¬(l) b (h)/S(bl). The variance can be replaced by empirical variance. Alg. 1 illustrates this idea. To compute VarQ[g(Qb(h); θ)] at h= Q¬(l) b (h), we keep the random seeds ﬁxed for all the compressors except the l-th one. We compute the empirical variance by two evaluations of g(Qb(h); θ), which are two NN iterations (forward + backward propagation). Finally, we assume thatc(h,θ) remains stable for diﬀerent mini-batches h, and along the training trajectory (θt). Therefore, we maintain a cl for each tensor l, which is updated by periodically running Alg. 1. Eq. (4) is approximately solved by the O(Llog2 L) greedy algorithm [5]. Another useful feature of this approach is predicting failure (in an a posteriori manner). If the compression variance V(b; h,θ) is dominating the overall gradient variance Var [g(Q(h); θt)], compression is adding too much noise to the gradient, and the convergence might be aﬀected. The overall gradient variance can be 5Algorithm 1 Numerical algorithm for computing cl(h,θ). Require: A gradient evaluation function g(·; θ) Require: A series of L+ 1 random seeds (rl)L+1 l=1 . Require: Any compression scheme b= (bl)L l=1 ∀l, seed Q(l) with rl g0 ←g(Qb(h); θ) {First iteration} ∀l, seed Q(l) with rl seed Q(l) with rL+1 g1 ←g(Qb(h); θ) {Second iteration, with another seed } Return 1 2 ∥g0 −g1∥2 /S(bl) 1importtorch2 importgact3 4model = ... # user defined model5 controller = gact.controller(model, opt_level='L2')6  controller.install_hook()78  # trainingloop9forepochin...10 foriterin...11......12 # instruct gact how to perform forward and backward13deffwdbwdprop():14output = model(data)15loss =loss_func(output,target)16optimizer.zero_grad()17loss.backward()1819controller.iterate(fwdbwdprop) Figure 2: Usage example of GACT computed by maintaining a running mean of the gradient. If V(b; θ)/Var [ˆg(Q(h); θt)] is too large, we can raise an alert to the user to increase the storage budget. 5 System Implementation We implemented GACT as a lightweight library in PyTorch. Users can use GACT for any NN architecture with several lines of code change. GACT uses low-level PyTorch hooks to capture context tensors, so it supports arbitrary operators, including custom operators deﬁned by users. We implemented eﬃcient CUDA kernels to infer tensor sensitivity and to perform compression during run time. GACT uses the same per-group quantizer in ActNN [5] as the compressor. However, GACT diﬀers from ActNN in several aspects. ActNN relies on manual analytical deduction to compute the sensitivity for diﬀerent operators, while GACT infers tensor sensitivity automatically, as described in Sec. 4.2. Moreover, ActNN performs layer-level quantization. It has to implement an activation compressed version for each operator and substitute operators during the training (e.g., replace torch.nn.Conv2d with actnn.Conv2d). In contrast, GACT runs at tensor level and uses a single hook interface to compress saved tensors for all operators. 5.1 General API As shown in Fig. 2, the interface of GACT is straightforward and intuitive, requiring the user to (i) initialize the GACT controller and specify an optimization level (Line 5); (ii) install hooks (Line 6); and (iii) instruct GACT how to perform forward and backward propagation (Lines 13-17) and pass it as a function (fwdbwdprop) to the controller (Line 19). We require users to specify (iii) because GACT needs to numerically run the forward and backward pass to infer tensor sensitivity. Although fwdbwdprop is passed to the controller every iteration, it is only called internally every adapt interval iterations when tensor sensitivity changes. As shown in Sec. 6.1, tensor sensitivity stabilizes quickly after the ﬁrst several epochs, adapt interval can thus be set to a large number, introducing negligible impact on training speed. 65.2 System Architecture Fig. 1 shows an overview of GACT. The GACT controller has three modules: Adaptivate Algorithm; Compressor; and Decompressor. In the forward pass, the controller uses PyTorch pack hook to capture all context tensors. Then Adaptive Algorithm infers tensor sensitivity based on gradients and assigns higher bits to more sensitive tensors, as described in Sec. 4.2. The bits information is used to instruct Compressor to perform quantization. In the backward pass, Decompressor dequantizes context tensors and uses unpack hook to send the dequantized results back to the PyTorch’s auto diﬀerentiation engine. The controller is also responsible for swapping quantized tensors to the CPU and prefetching them back during the backward propagation if swapping is enabled. 5.3 Identifying Tensors to Quantize The pack hook and unpack hook process all types of context tensors, including activation, parameters trained by the optimizer, and training states such as running mean/variance used by batch normalization. To guarantee that only the activations are quantized, we ﬁlter out saved parameters by recording the data pointers of all the model parameters before training, and we skip quantization if the input tensor pointer exists in the parameter pointer set. Similarly, GACT does not quantize training states by checking if the input tensor requires gradients. However, using hooks blindly disables some memory-saving optimization. For example, in a transformer’s self-attention layer, the keys, query, value tensors are all calculated from the same input tensor. The saved objects of the three operations thus all refer to the same tensor. In this case, PyTorch triggers the pack hook three times. If we perform quantization blindly, we waste computation resources and introduce extra memory consumption because the same underlying tensor is quantized and saved more than once. GACT avoids duplication by generating footprints for each input context tensor. We use the CUDA data pointer, sampled data points, and the tensor statistics (e.g., sum) as the footprint. GACT manages all quantized context tensors and uses the footprint to diﬀerentiate them. If a tensor is already quantized, GACT will skip quantization and return previous results directly. 5.4 Parallel Swap and Prefetch To further reduce activation memory, we combine GACT with swapping. All compressed tensors are oﬄoaded to the CPU during the forward pass and swapped back in the backward pass. Here, we replace the original tensor with quantized activation, as data movement is more expensive than computation. Swapping the original tensor saves the quantization overhead but adds more data movement cost between CPU and GPU. As shown in Sec. 6.4, quantization overhead is much smaller than copying full-precision data to CPU in modern GPU architecture. Furthermore, we create two new streams (swap in/out) to parallelize the computation and swapping operation to reduce the swap overhead. The forward computation and swap-out process happen in parallel during the forward pass. During the backward pass, in each layer the swap-in stream is responsible for prefetching the compressed activation of the previous layer to avoid synchronization overhead. We leverage the CUDA event to ensure tasks in diﬀerent streams are executed in the correct order. 5.5 Other Memory Optimizations Gradient checkpointing. Gradient checkpointing [ 19] works by dividing the NN into segments. The algorithm only stores the inputs of each segment and recomputes the dropped activations segment by segment during backpropagation. The memory consumption is thus the cost of storing the inputs of all segments plus the maximum memory cost to backpropagate each segment. When combined with gradient checkpointing, GACT can reduce the memory consumption of both parts. GACT reduces the memory consumption of the ﬁrst part by quantizing the segment inputs. Moreover, the activations saved during the recompute phase are also quantized, reducing the memory cost of the second part. Combining GACT with gradient checkpointing might introduce more training noise because the recompute starts from quantized segment inputs, making the forward pass of recompute phase not exact. However, in Sec. 6.4, we show the noise introduced by forwarding from the quantized tensors is negligible. Memory eﬃcient self-attention. When the batch size is very large, the single layer after dequantization occupies a large amount of memory and prevents the batch size from increasing further. We observe this 7Table 1: Optimization levels for GACT. Level Compression Strategy Bits L0 Do not compress 32 L1 per-group quantization with auto-precision 4 L2 L1 + swapping/prefetching 4 CB1 L1 + gradient checkpointing 4 CB2 CB1 + eﬃcient self-attention 4 relu conv *pool conv *conv *pool conv *conv *pool conv *conv *pool linear *drop linear *drop linear loss 1 2 4 8 32bits 10−1 100 101 cl (a) Inferred per-tensor cl (line) and bits/dim. (bar) for VGG-11. Layers with * have a preceding ReLU layer with shared context. drop=dropout, loss=cross entropy loss. 1 2 4 8 bits/dim. 10−2 10−1 100 101 grad. var. uniform adapt (b) Gradient variance. 0 50 100 epoch 10−2 100 102 104 cl (c) Evolution of the per- tensor sensitivity. Each line iscl for a tensor. Figure 3: Eﬀectiveness of the adaptive algorithm. problem in transformer-based models where self-attention has quadratic space complexity in terms of sequence length. To reduce the memory footprint of the self-attention layer, we implement the algorithm introduced in [29] that achieves linear space complexity, and combines it with GACT. 5.6 Optimization level To exploit the trade-oﬀ between memory saving and training speed, GACT provides several optimization levels. Higher levels can save more memory but with more overhead. Tab. 1 lists these optimization levels. L1 uses per-group quantization with the adaptive algorithm. L2 combines per-group quantization with swapping and prefetching. For transformer-based models, CB1 combines GACT with gradient checkpointing. CB2 further reduces the peak memory by adding eﬃcient self-attention to CB1. 6 Experiments We ﬁrst demonstrate the eﬀectiveness of the GACT adaptive algorithm. We further apply GACT to a wide range of machine learning tasks, including image classiﬁcation, object detection, text, and graph node classiﬁcation. We compare the training accuracy and activation compression rate for full precision, adaptive 4/3/2 (using GACT to adaptively decide quantization bits with an average of 4/3/2 bit) and ﬁx-4 bit (quantizating all tensors uniformly with 4 bits). Next, we study the trade-oﬀ between compression rate and training throughput and compare GACT with other state-of-the-art memory-saving methods. Lastly, we demonstrate the ﬂexibility of GACT by exploring the possibility of combining it with other memory optimization methods (CB1, CB2 as listed in Table 1). We use open-source model implementations for all tasks. 6.1 Compression Strategy We ﬁrst test the eﬀectiveness of our adaptive compression rate algorithm for training VGG-11 [ 30] on ImageNet. Fig. 3(a) plots the inferred per-tensor sensitivity cl and the corresponding optimal bits/dim. GACT assigns more bits to more sensitive layers. The context tensor saved by the cross-entropy loss operator is most sensitive. A small amount of compression leads to a huge gradient variance. This makes sense since the loss is the ﬁrst operator to back-propagate through, where the error accumulates. Therefore, GACT assigns 32 bits/dim. for the tensors in the classiﬁcation head. With the adaptive algorithm, GACT with an average of 4 bits/dim. achieves smaller gradient variance than uniformly assigning 8 bits/dim. for all the tensors, as shown in Fig. 3(b). Finally, Fig. 3(c) shows that the sensitivity cl(h; θt) remains stable during training.Therefore, periodically updating cl at a large interval is reasonable, and this introduces negligible impact on training speed. 8Table 2: For classiﬁcation, we train VGG11 [ 30], ResNet-50 [31], and Swin-Tiny [ 32] on ImageNet [ 33]. For object detection, we train RetinaNet [ 34], Faster R-CNN [35] on Coco [ 36]. We report accuracy on validation sets (Div. indicates diverge) and the compression rate of context tensors (numbers in brackets) for both tasks. Task Model FP32 GACT Adapt 4bit (L1) GACTAdapt 2bit Cls. VGG11 68.75 68.77 (2.84×) 68.49 (3.34×) ResNet-50 77.29 76.96 (6.69×) 76.13 (11.39×) Swin-tiny 81.18 80.92 (7.44×) 77.91 (13.73×) Det. Faster RCNN37.4 37.0 (4.86×) 36.1 (6.81×) RetinaNet 36.5 36.3 (3.11×) Div. 6.2 Optimization level We apply GACT on various computer vision tasks, including image classiﬁcation and object detection, as shown in Fig. 2. We also vary the average bits used by the adaptive algorithm to explore the memory accuracy trade-oﬀ. On both tasks, GACT L1 achieves comparable ( <0.5% accuracy drop) or even better results than the full precision training, while reducing activation memory by up to 7.44 ×. Here, we list the accuracy of FP32 as the strongest accuracy baseline. For other lossy methods we consider in Sec. 6.3, the accuracy is no better than FP32, and we list their training accuracy in Appendix C. Notice that here GACT Adapt 2bit diverges on the detection task. This is because, as shown in Sec.3.2, although ACT has unbiased gradients, the compression error and learning rate aﬀect the convergence. When using 2 bit, the compression error is large and the learning rate has to be reduced accordingly to guarantee convergence. However, we do not want to slow training by decreasing the learning rate. All experiments are run with the same learning rate as the full precision. Therefore when compression error is large, the training diverges. Furthermore, we observe that the memory reduction varies among networks because GACT does not quantize intermediate states, and the size of intermediate states diﬀers between networks. For example, in VGG11, when the batch size is 128, GACT reduces the saved tensor size from 5889MB to 2080MB, among which 78% (1494MB) is used to store the intermediate index for the max-pooling layer that is not quantized by GACT. Next, we demonstrate the ﬂexibility of GACT by applying it to a wider variety of natural language processing (NLP) and graph machine learning (Graph) tasks. We run multiple seeds for each task, and we report the mean ±std of accuracy/F1 across runs as shown in Tab. 3. We include the detailed experimental setup in Appendix B. For both NLP and Graph tasks, GACT L1 achieves comparable training results with FP32, introducing less than 0.3% accuracy/F1-score drop, while reducing activation memory by 4.18 ×to 7.93×. Moreover, the results are stable across runs, introducing similar accuracy variance as FP32. We also show the training results of ﬁx-4bit quantization, where all tensors are uniformly quantized with 4 bits. As shown in Tab. 3, ﬁx-4 bit quantization causes signiﬁcant accuracy/F1-score loss on various graph models. For Bert-large, ﬁxed-4 bit quantization works ﬁne because all the context tensors have similar sensitivity. On the other hand, GACT L1, using a similar amount of memory as always quantizing each layer to 4 bits, still performs on par with full precision training on all the models. This shows the necessity of using adaptive algorithms to assign bits based on tensor sensitivity for stabilized training. Moreover, for Bert-large and three graph models (GCN/GAT/GCNII), GACT converges and gives lossless results with 3 bits. Remarkably, across all the graph models, training with 2-bit GACT causes little accuracy loss ( <1%). This shows the robustness of our adaptive algorithm. 6.3 Memory Saving and Computational Overhead Settings and baselines. We implement the benchmark with PyTorch 1.10 and measure the memory saving and overhead of GACT on an AWS g4dn.4xlarge instance, which has a 16GB NVIDIA T4 GPU and 64GB CPU memory. On ResNet-50, we compare with ActNN [ 5], a dedicated quantization framework for convolutional NNs, and DTR [ 21], a state-of-the-art rematerialization method for dynamic graphs. “swap” is a simple swapping strategy that swaps all activations to the CPU. For Bert-large, we also show the results on Mesa [ 7], a memory-saving resource-eﬃcient training framework for transformers, and ZeRO-Oﬄoad [ 37], a highly optimized system for training large-scale language models. Gradient checkpointing uses the default checkpointing policy provided by the transformer library [ 38], where only the input to each transformer block is saved before the backward pass. On Swin-tiny, we only include Mesa and swap because other baselines 9Table 3: Accuracy and activation compression rate for NLP and Graph tasks. Accuracy that drops > 1% is in italic font. Model Dataset FP32 Fix 4bit GACT Adapt 4bit (L1)GACT Adapt 3bitGACT Adapt 2bit GCN Flickr 51.17±0.19 50.93±0.16 (7.56×) 51.08±0.18 (7.93×) 51.14±0.18 (11.34×) 51.20±0.18 (17.56×) Reddit 95.33±0.07 94.42±0.11 (7.55×) 95.32±0.07 (7.90×) 95.31±0.07 (9.70×) 95.34±0.06 (13.68×) Yelp 39.86±0.94 39.85±1.22 (5.94×) 40.06±0.74 (6.42×) 40.21±0.82 (7.46×) 39.89±1.45 (9.00×) ogbn-arxiv71.51±0.65 68.61±0.77 (7.54×) 71.35±0.36 (8.09×) 70.82±0.95 (10.45×) 70.87±0.66 (13.75×) GAT Flickr 52.40±0.28 35.24±11.90 (4.23×) 52.26±0.31 (4.34×) 51.68±1.13 (5.04×) 51.62±1.19 (5.46×) Reddit 95.95±0.06 59.37±11.48 (4.12×) 96.02±0.09 (4.29×) 95.96±0.06 (4.64×) 95.82±0.06 (5.24×) Yelp 52.41±0.69 36.09±13.70 (4.04×) 52.18±0.38 (4.18×) 51.63±0.83 (4.53×) 51.15±0.53 (5.24×) ogbn-arxiv71.68±0.54 54.64±5.62 (5.04×) 71.80±0.47 (5.09×) 71.47±0.50 (6.14×) 71.21±0.68 (6.98×) GCNII Flickr 52.37±0.16 52.28±0.16 (4.84×) 52.31±0.16 (4.91×) 52.36±0.16 (5.54×) 52.23±0.15 (6.44×) Reddit 96.32±0.24 86.50±1.08 (4.51×) 96.11±0.22 (4.52×) 96.01±0.33 (5.16×) 95.54±0.29 (5.92×) Yelp 62.33±0.20 62.21±0.22 (5.26×) 62.28±0.26 (5.34×) 62.53±0.36 (6.29×) 62.33±0.37 (7.28×) ogbn-arxiv72.52±0.12 44.57±5.01 (6.54×) 72.28±0.35 (6.74×) 72.22±0.28 (7.98×) 71.74±0.26 (10.24×) Bert- large MNLI 86.74±0.24 85.98±0.16 (7.55×) 86.61±0.11 (7.38×) 86.68±0.08 (9.13×) 84.24±0.74 (12.87×) SST-2 93.69±0.30 93.46±0.23 (7.55×) 93.54±0.52 (7.30×) 93.20±0.37 (9.05×) 91.90±1.04 (12.91×) MRPC 88.20±0.02 87.36±0.19 (7.55×) 87.90±0.10 (7.40×) 87.69±0.07 (9.19×) 82.54±0.38 (12.91×) QNLI 92.29±0.14 92.34±0.07 (7.55×) 92.44±0.07 (7.42×) 92.43±0.31 (9.19×) 90.74±0.13 (12.95×) Table 4: Largest models GACT can train with 16G GPU memory. In ResNet (batch size=64), D (depth): number of layers, W (width): base width of the bottleneck block, R (resolution): width and height of input images. In Bert-large (batch size=16) and GCN, D (depth): number of transformer/gcn blocks, W (width): hidden size. Dim Maximum Value Throughput (TFLOPS) FP L1 L2 FP L1 L2 ResNet- 152 D 160 460 1124 0.43 0.47 0.41 W 88 304 320 0.44 0.89 0.6 R 232 548 716 0.41 0.39 0.44 Bert- large D 32 56 64 0.67 0.56 0.53 W 1280 1488 2032 0.68 0.61 0.60 GCN D 24 152 240 0.20 0.14 0.15 W 2464 3948 4244 0.36 0.38 0.40 lack the support for this network. Results. We compare the training throughput of GACT against other memory saving systems in Fig. 4. On ResNet-50, GACT achieves similar throughput as ActNN (ActNN optimization L5 is not listed because it optimizes PyTorch memory allocation, which is unrelated to quantization and can also be applied to GACT), but ActNN enables training with a larger batch size. This is expected because ActNN implements eﬃcient, customized layers for diﬀerent operators in convolutional NNs. For Bert-large, Zero-oﬄoad fails quickly because it only oﬄoads optimizer states that occupy a small portion of total memory to CPU. GACT L1 outperforms Mesa because Mesa only compresses tensors to 8 bit. When the batch is bigger, the activation size of each segment becomes the memory bottleneck and prevents gradient checkpointing from increasing the batch size. Moreover, combining GACT with gradient checkpointing and eﬃcient self-attention further reduces the peak memory, increasing the batch size by up to 24.7 ×. Meanwhile, it introduces a small throughput overhead compared with the original gradient checkpointing. Across all the network architectures, GACT enables training with a 4.2 ×to 24.9×larger batch size under the same memory budget. Network scaling. With GACT, we can construct larger models or train with a higher image resolution. Tab. 4 compares the largest model we can train against full precision. With the same batch size and memory budget, GACT can scale a ResNet-152 to 7.0 ×deeper, 3.6 ×wider or 3.0 ×higher resolution. Similarly, Bert-large can be scaled to 2.0 ×deeper or 1.6×wider. In GCN, GACT enables training 10.0 ×deeper and 1.7×wider network. Overall, GACT maintains 75% - 136% original training throughput. 6.4 Other Optimizations We evaluate the idea of combining GACT with swapping on Bert-large-cased. As shown in Tab. 5, swapping compressed tensors is faster than swapping the original ones because communication between CPU and GPU is more time-consuming than computation. Combining GACT with swapping increases training speed by 10(a) 0 200 400 600 800 Batch Size 0 50 100Training Throughput 4.3× L0 L1 L2 ResNet-50 DTR Swap ActNN GACT (b) 0 100 200 300 400 500 600 Batch Size 0 10 20Training Throughput 24.7× L0 L1 L2 CB1 CB2 Bert-large ZeroOff Swap Mesa CKPT GACT (c) 0 100 200 300 400 500 600 Batch Size 0 25 50 75Training Throughput 5.6× L0 L1 L2 Swin-tiny Mesa Swap GACT Figure 4: Training throughput vs batch size. Red cross mark means out-of-memory. The shaded yellow region denotes the batch sizes with full precision training given the memory budget. CKPT: Gradient checkpointing, ZeroOﬀ: ZeRO-Oﬄoad. Table 5: Swap and prefetch speed/memory on Bert-large. Algorithm Speed (sequence/s) Peak Mem. (MB) Total Mem. (MB) FP32 16.41 9573 9527 FP32 + swap 6.02 5215 5093 GACT swap 12.95 5426 5325 GACT swap + prefetch14.02 5426 5324 up to 2.3×. Notice here that the peak memory use of “GACT swap” is slightly higher than “FP32 + swap” because GACT does not quantize and swap intermediate states such as running mean/var of BatchNorm layer. Moreover, prefetch increases the speed by about 7% with negligible memory overhead. We next demonstrate combining GACT with gradient checkpointing (CB1). Gradient checkpointing is performed at the beginning of each transformer block, thus avoiding saving tensors generated within the block. We then apply GACT with gradient checkpointing, where the saved tensors are quantized with 4 bits. As shown in Tab. 6, the accuracy is unaﬀected. We also compare the activation memory and peak memory of CB1 and CB2 in Tab. 7. AM2 denotes the peak activation memory, which is the size of saved tensors after reforwarding the ﬁrst transformer block. When batch size = 288, compared with gradient checkpointing on full precision (FP32), CB1 and CB2 reduce the peak activation size by 4.7 ×and 5.4×respectively. 7 Conclusion This paper presents GACT, an ACT framework for generic NN architectures. We prove the convergence of GACT without prior knowledge about operator type or network architecture by analyzing a linearized Table 6: Accuracy of Bert-large-cased on SST-2 and QNLI datasets Algorithm SST-2 QNLI Algorithm SST-2 QNLI FP32 93.58 92.42 CB1 93.81 92.26 11Table 7: Memory use of diﬀerent algorithms on Bert-large. AM1: Activation size before backward, AM2: Activation size after reforwading the ﬁrst transformer block. When batch size = 288, L0 runs out of memory, and therefore it is not listed below. Batch Size Algorithm AM1(MB) AM2(MB) Peak Mem.(MB) 16 L0 4434 - 9573 FP32 + CKPT210 394 5541 CB1 37 99 5286 CB2 31 79 5269 288 FP32 + CKPT3783 7092 12885 CB1 515 1497 8251 CB2 486 1307 8102 approximation of ATC’s gradients. With the adaptive algorithm, GACT achieves negligible accuracy loss on various tasks, reducing activation memory by up to 8.1 ×and enabling training with up to 24.7 ×batch size compared with full precision training. Acknowledgements This work was supported by the National Key Research and Development Project of China (No. 2021ZD0110502); NSF of China Project (No. 62106120), by the National Science Foundation through grants IIS-1955488, IIS-2027575, CCF-1723352, ARO W911NF2110339, ONR N00014-21-1-2724, and DOE award DE-SC0016260. We would also like to acknowledge partial support from DARPA, IARPA, the Sloan Foundation, NSF, and ONR. Our conclusions do not necessarily reﬂect the position or the policy of our sponsors, and no oﬃcial endorsement should be inferred. References [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. [2] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and eﬃcient sparsity. arXiv preprint arXiv:2101.03961 , 2021. [3] Ayan Chakrabarti and Benjamin Moseley. Backprop with approximate activations for memory-eﬃcient network training. arXiv preprint arXiv:1901.07988 , 2019. [4] Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and Bin Cui. Don’t waste your bits! squeeze activations and gradients for deep neural networks via tinyscript. In International Conference on Machine Learning, pages 3304–3314. PMLR, 2020. [5] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W Mahoney, and Joseph E Gonzalez. Actnn: Reducing training memory footprint via 2-bit activation compressed training. In International Conference on Machine Learning , 2021. [6] R David Evans and Tor Aamodt. AC-GC: Lossy activation compression with guaranteed convergence. Advances in Neural Information Processing Systems , 34, 2021. [7] Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai, and Bohan Zhuang. Mesa: A memory-saving training framework for transformers. arXiv preprint arXiv:2111.11124 , 2021. [8] R David Evans, Lufei Liu, and Tor M Aamodt. Jpeg-act: accelerating deep learning via transform-based lossy compression. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pages 860–873. IEEE, 2020. [9] Sian Jin, Guanpeng Li, Shuaiwen Leon Song, and Dingwen Tao. A novel memory-eﬃcient deep learning training framework via error-bounded lossy compression. In 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming , pages 485–487, 2021. 12[10] Anonymous. EXACT: Scalable graph neural networks training via extreme activation compression. In Submitted to The Tenth International Conference on Learning Representations , 2022. under review. [11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, / Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017. [12] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907 , 2016. [13] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. [14] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. In International Conference on Learning Representations, 2018. [15] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit ﬂoating point numbers. In Advances in Neural Information Processing Systems, pages 7675–7684, 2018. [16] Ron Banner, Itay Hubara, Elad Hoﬀer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems , pages 5145–5153, 2018. [17] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. In Advances in neural information processing systems , 2020. [18] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkatara- mani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. In Advances in Neural Information Processing Systems , volume 33, 2020. [19] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 , 2016. [20] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica, and Joseph E Gonzalez. Checkmate: Breaking the memory wall with optimal tensor rematerialization. arXiv preprint arXiv:1910.02653 , 2019. [21] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic tensor rematerialization. arXiv preprint arXiv:2006.09616 , 2020. [22] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 1341–1355, 2020. [23] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic GPU memory management for training deep neural networks. In 23rd ACM SIGPLAN symposium on principles and practice of parallel programming , pages 41–53, 2018. [24] Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-based gpu memory management for deep learning. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 891–905, 2020. 13[25] Olivier Beaumont, Lionel Eyraud-Dubois, and Alena Shilova. Eﬃcient combination of rematerialization and oﬄoading for training dnns. Advances in Neural Information Processing Systems , 34, 2021. [26] L´ eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP- STAT’2010, pages 177–186. Springer, 2010. [27] L´ eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223–311, 2018. [28] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pages 3123–3131, 2015. [29] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [30] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV) , 2021. [33] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. [34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ ar. Focal loss for dense object detection. In International Conference on Computer Vision (ICCV) , pages 2980–2988, 2017. [35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. Advances in neural information processing systems , 28:91–99, 2015. [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014. [37] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-oﬄoad: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840, 2021. [38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´ emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online, October 2020. Association for Computational Linguistics. [39] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931 , 2019. [40] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. 14[41] Petar Veliˇ ckovi´ c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017. [42] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International Conference on Machine Learning , pages 1725–1735. PMLR, 2020. [43] Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, and Jie Tang. Cogdl: Toolkit for deep learning on graphs. arXiv preprint arXiv:2103.00959 , 2021. [44] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353–355, 2018. [45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, 2019. 15A Proof of Theorems A.1 Theorem 1: Convergence of ACT Assume that: A1. L(θ) is a continuous diﬀerentiable, ∇L(θ) is β-Lipschitz continuous. . A2. L(θ) is bounded below by L∗. A3. g(h; θ) is diﬀerentiable w.r.t. h and ∃b> 0, s.t. ∀θ,E∥g(Q(h(x,θ)); θ) −ˆg(h(x,θ); θ)∥≤ b. A4. ∃σ2 >0, s.t., ∀θ, Var [ˆg(h(x,θ)] ≤σ2. Then, for all η <1 2β, if we run ACT deﬁned as Eq. (2) for T iterations, then we have min t=0,...,T−1 E [ ∥∇L(θt)∥2 ] ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2 Proof. Denote m:= ∇θL(θt), ϵ:= ˆg(h(x,θt); θt) −m, d:= g(Q(h(x; θt)); θt) −ˆg(h(x,θt); θt). Then, by A3 and A4, we have E[ϵ] = E[g(h(x,θt); θt) −∇θL(θt)] + E[⟨J(x,θt),∆Q(h(x,θt))⟩] = ⟨J(x,θt),E[∆Q(h(x,θt))]⟩= 0. (6) E [ ∥ϵ∥2 ] = ∥E[ϵ]∥2 + Var [ϵ] = Var [ˆg(h(x,θt); θt)] ≤σ2. (7) E[∥d∥] ≤b. (8) By the deﬁnitions, the ACT dynamics can be written as θt+1 ←θt −η(m+ d+ ϵ). By A1, we have L(θt+1) ≤L(θt) −η⟨m,m + d+ ϵ⟩+ βη2 2 ∥m+ d+ ϵ∥2 . (9) By Eq.s (6,8) E[⟨m,m + d+ ϵ⟩] ≥∥m∥2 −∥m∥∥d∥+ ⟨m,E[ϵ]⟩≥∥ m∥2 −∥m∥b. (10) By Eq.s (6,7,8), and ∥x+ y∥2 ≤2 ∥x∥2 + 2∥y∥2, E [ ∥m+ d+ ϵ∥2 ] = E [ ∥m+ d∥2 ] + Var [ϵ] ≤2E[∥m∥]2 + 2E[∥d∥]2 + Var [ϵ] = 2E[∥m∥]2 + 2b2 + σ2. (11) Taking expectation on both sides of Eq. (9), plug in Eq.s (10, 11), and use η <1 2β, we have E[L(θt+1)] ≤L(θt) −η(∥m∥2 −∥m∥b) + βη2 2 (2E[∥m∥]2 + 2b2 + σ2). =L(θt) −(η−βη2) ∥m∥2 + η∥m∥b+ βη2 2 (2b2 + σ2) =L(θt) −η 2 ∥m∥2 + η∥m∥b+ βη2 2 (2b2 + σ2). Completing the squares, E[L(θt+1)] ≤L(θt) −η 2(∥m∥−b)2 + βη2 2 (2b2 + σ2). Take expectation on both sides and sum up for t= 0,...,T −1, E[L(θT)] −L(θ0) ≤−η 2 T−1∑ t=0 E(∥∇L(θt)∥−b)2 + βη2T 2 (2b2 + σ2). Reorganize the terms, Et [ E(∥∇L(θt)∥−b)2 ] ≤2(L(θ0) −L(θT)) ηT + ηβ(2b2 + σ2). Let t∗= argmintE[∥∇L(θt)∥], and use A1, we have E(∥∇L(θt∗)∥−b)2 ≤2(L(θ0) −L∗) ηT + ηβ(2b2 + σ2). 16Use (a+ b)2 ≤2a2 + 2b2, we have E [ ∥∇L(θt∗)∥2 ] ≤4(L(θ0) −L∗) ηT + (2βη+ 2)b2 + ηβσ2 ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2. A.2 Proposition 1: The Linearization Error Proof. Consider the gradient function g(Q(h(x; θ); θ)), whose output is a P-dimensional vector. Since it is twice diﬀerentiable, we construct the Taylor’s expansion at h(x; θ) with Lagrange remainder: ∃H1,...,H P,s.t., ∀i, gi(Q(h(x; θ)); θ) = gi(h(x,θ); θ) + Ji(x,θ)∆h(x,θ) + ∆h(x,θ)⊤Hi∆h(x,θ), where Ji(h(x; θ),θ) := ∂gi(h(x;θ);θ) ∂h . By the assumption, there exists P >0, such that the linearization error is ∥g(Q(h(x; θ)); θ) −ˆg(h(x; θ); h(x; θ),θ)∥1 = P∑ i=1 ∆h(x,θ)⊤Hi∆h(x,θ) ≤γP ∥∆h(x,θ)∥2 . Taking expectation, E[∥g(Q(h(x; θ)); h(x; θ),θ) −ˆg(h(x; θ); θ)∥2] ≤E[∥g(Q(h(x; θ)); θ) −ˆg(h(x; θ); h(x; θ),θ)∥1] ≤γPVar [∆h(x,θ)] = O(Var [∆h(x,θ)]). A.3 Proposition 2: The Order of the Variance The following proposition is convenient for isolating the diﬀerent noise sources. Proposition A. (Law of Total Variance) Var [X] = E[Var [X |Y]] + Var [E[X |Y]] . Proof. By deﬁnition Var [ˆg(h(x; θt); h(x; θ),θt)] = Var [g(h(x,θ); θ)] + Var [J(h(x; θ),θ)∆h(x,θ)] , where Var [g(h(x,θ); θ)] is the noise introduced by subsampling the data x. By law of total variance, Var [J(h(x; θ),θ)∆h(x,θ)] = EX[VarQ[J(h(x; θ); θt)∆h(x,θ)]] + VarX[EQ[J(h(x; θ); θt)∆h(x,θ)]]   =0 , where VarQ[J(h(x; θ); θt)∆h(x,θ)] =EQ [ ∥J(h(x; θ); θt)∆h(x,θ)∥2 ] ≤EQ [ ∥J(h(x; θ); θt)∥2 ∥∆h(x,θ)∥2 ] = ∥J(h(x; θ); θt)∥2 EQ [ ∥∆h(x,θ)∥2 ] = O(Var [∆h(x,θ)]) . A.4 Proposition 3: The Structure of the Variance Before investigating the structure of VarQ[J(x; θt)∆h(x,θ)], let’s do some recap: the parameter θt is a P-dimensional vector; the context diﬀerence ∆ h(x,θ) is a D-dimensional vector, and J(x; θt) is a P ×D matrix. Recall that ∆ h(x,θ) is the concatenation of L-vectors, ∆h(l)(x,θ), and let J(l)(x,θ) := ∂g ∂h(l) g ( (h(l)(x; θ))L l=1,θ ) , which is a P ×Dl matrix. Furthermore, let h(l) j (x,θ) be the j-th dimension, and J(l) j (x,θ) be its j-th column. To proceed, we need to make the following assumptions to the compressor Q(·) : RD →RD: B1: The compressed result is element-wise uncorrelated. That is, for any i̸= j, Cov [Q(h)i,Q(h)j] = 0. B2: For compressing a vector h to b bits, the compression variance of each dimension can be written in the form Var [Q(h)j] ≤Rj(h)S(b), where S(·) is a known function. Both assumptions can be achieved by a stochastic rounding [28] quantizer, where Q(h)j = { T−1 h,b (⌈Th,b(hj)⌉) w.p. Th,b(hj) −⌊Th,b(hj)⌋ T−1 h,b (⌊Th,b(hj)⌋) otherwise , 17where Th,b(hj) = (2b−1) hj−minj h maxj h−minj h. Since each dimension is quantized independently, B1 is met. Moreover, Var [Q(h)j] ≤1 4 (maxjh−minjh (hj −minjh) )2 (2b −1)−2 = Rj(h)S(b), where Rj(h) = 1 4 (maxjh−minjh (hj −minjh) )2 , S (b) = (2b −1)−2. Proof. By deﬁnition, J(h; θ)∆h= L∑ l=1 Dl∑ j=1 J(l) j (h; θt)∆h(l) j . Using Assumption B1, we have VarQ[J(h; θ)∆h] = EQ    L∑ l=1 Dl∑ j=1 J(l) j (h; θt)∆h(l) j  2  = L∑ l=1 Dl∑ j=1 EQ [J(l) j (h; θt)∆h(l) j  2] . = L∑ l=1 Dl∑ j=1 J(l) j (h; θt)  2 VarQ [ ∆h(l) j ] Using Assumption B2, we have VarQ[J(h; θ)∆h] ≤ L∑ l=1 Dl∑ j=1 J(l) j (h; θt)  2 Rl(h)S(bl) = L∑ l=1 cl(h,θ)S(bl), where cl(θ,h) := Rl(h) J(l)(h; θt) 2 F. B Experiment Setup B.1 Node classiﬁcation task on graphs We conduct experiments on four node classiﬁcation datasets with standard splits, including Flickr, Reddit, Yelp from GraphSAINT [39], and ogbn-arxiv from Open Graph Benchmark (OGB) [ 40]. The four datasets cover extensive downstream applications with diﬀerent scales. We use accuracy as the evaluation metric for multi-class classiﬁcation and micro-F1 for multi-label classiﬁcation. We run ten seeds (0 to 9) and report the average accuracy across runs. We evaluate GACT on three representative GNN models, including GCN [ 12], GAT [41], and GCNII [ 42] under the full-batch training setting. All three models are implemented by CogDL [ 43], a toolkit for graph neural networks. B.2 Text classiﬁcation task We select four largest datasets, MNLI, QQP, SST-2, and QNLI, from the GLUE benchmark [ 44]. The four datasets cover diﬀerent aspects of natural language understanding, including sentiment classiﬁcation, natural language inference and paraphrase detection. We use the mainstream transformer implementation [ 38] to train Bert-large [45]. We run three seeds (42, 43, 44) and report F1 for QQP, accuracy for the others. C Training Accuracy of Baselines For all the baselines we compared in Sec. 6.3, only ActNN, Mesa, and ZeRO-Oﬄoad are lossy methods. All other methods are lossless and have the same training accuracy as FP32. For ResNet-50 on ImageNet, the training accuracy for FP32, GACT, ActNN L2, and ActNN L3 are 77.3, 77.0, 77.4, and 76.9. For Bert-Large on SST-2, the accuracy for FP32, GACT, Mesa, and ZeRO-Oﬄoad are 93.7, 93.5, 93.8, and 93.3. For Swin-tiny on ImageNet, the training accuracy for FP32, GACT, and Mesa are 81.2, 81.0, and 81.3 respectively. 18",
      "meta_data": {
        "arxiv_id": "2206.11357v4",
        "authors": [
          "Xiaoxuan Liu",
          "Lianmin Zheng",
          "Dequan Wang",
          "Yukuo Cen",
          "Weize Chen",
          "Xu Han",
          "Jianfei Chen",
          "Zhiyuan Liu",
          "Jie Tang",
          "Joey Gonzalez",
          "Michael Mahoney",
          "Alvin Cheung"
        ],
        "published_date": "2022-06-22T20:06:23Z",
        "pdf_url": "https://arxiv.org/pdf/2206.11357v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "GACT proposes a generic Activation Compressed Training (ACT) framework for diverse Neural Network (NN) architectures to reduce training memory footprint. Key contributions include a general convergence theory for ACT, an algorithm that automatically estimates tensor sensitivity and selects optimal compression strategies, and an efficient PyTorch implementation with an easy-to-use API. GACT achieves up to 8.1x activation memory reduction, enabling 4.2x to 24.7x larger batch sizes with negligible accuracy loss.",
        "methodology": "GACT introduces a general convergence theory by analyzing a linearized version of ACT's approximate gradient, proving convergence without prior knowledge of operator type or model architecture. It uses an unbiased stochastic compressor Q(·) and approximates the AC gradient via a first-order Taylor expansion. An adaptive algorithm dynamically determines the compression ratio for each context tensor by estimating its impact on the gradient variance at runtime. This algorithm approximately solves an integer programming problem to minimize variance under a total bits budget, using empirical variance computations. The framework is implemented as a PyTorch library, leveraging low-level hooks to capture context tensors and supporting various memory optimizations like swapping, prefetching, and gradient checkpointing.",
        "experimental_setup": "GACT was evaluated across various machine learning tasks: image classification (VGG11, ResNet-50, Swin-Tiny on ImageNet), object detection (RetinaNet, Faster R-CNN on COCO), text classification (Bert-large on GLUE benchmark datasets: MNLI, QQP, SST-2, QNLI), and graph node classification (GCN, GAT, GCNII on Flickr, Reddit, Yelp, ogbn-arxiv). Experiments were conducted on an AWS g4dn.4xlarge instance (16GB NVIDIA T4 GPU, 64GB CPU memory). Performance was measured in terms of validation accuracy/F1-score, activation compression rate, training throughput, and maximum trainable model size. Comparisons were made against full precision (FP32) training and other memory-saving baselines like ActNN, DTR, simple swapping, Mesa, and ZeRO-Offload.",
        "limitations": "GACT Adapt 2-bit can lead to training divergence in certain tasks (e.g., object detection) if the learning rate is not appropriately reduced to account for larger compression errors. The memory reduction varies among networks because GACT does not quantize intermediate states, and the size of these states differs. Combining GACT with gradient checkpointing might introduce more training noise due to recomputation starting from quantized segment inputs. Also, while GACT avoids quantizing intermediate states, it does not quantize internal states like running mean/variance of BatchNorm layers, which can slightly increase peak memory compared to simple FP32+swap in certain scenarios.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "GACT: Activation Compressed Training for Generic Network Architectures",
      "abstract": "Training large neural network (NN) models requires extensive memory\nresources, and Activation Compressed Training (ACT) is a promising approach to\nreduce training memory footprint. This paper presents GACT, an ACT framework to\nsupport a broad range of machine learning tasks for generic NN architectures\nwith limited domain knowledge. By analyzing a linearized version of ACT's\napproximate gradient, we prove the convergence of GACT without prior knowledge\non operator type or model architecture. To make training stable, we propose an\nalgorithm that decides the compression ratio for each tensor by estimating its\nimpact on the gradient at run time. We implement GACT as a PyTorch library that\nreadily applies to any NN architecture. GACT reduces the activation memory for\nconvolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training\nwith a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We\nimplement GACT as a PyTorch library at\nhttps://github.com/LiuXiaoxuanPKU/GACT-ICML.",
      "full_text": "GACT: Activation Compressed Training for Generic Network Architectures Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael Mahoney, Alvin Cheung University of California, Berkeley xiaoxuan liu@berkeley.edu, jianfeic@tsinghua.edu.cn Abstract Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT’s approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1×, enabling training with a 4.2× to 24.7× larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML. 1 Introduction In recent years, we have witnessed the trend of using larger and larger neural network (NN) models to deliver improved accuracy and generalization in various machine learning tasks [1, 2]. However, training these models requires a considerable amount of on-device GPU memory. Unfortunately, the increase of GPU memory capacity has been relatively slow, leading to a fundamental barrier to the development of large NN models. Activation Compressed Training (ACT) is a promising approach to reduce the memory footprint of models during training. As all layers’ activations need to be kept in the memory for computing the gradients during training, ACT reduces memory consumption by compressing these saved activations. Prior work [3, 4, 5, 6] has shown the eﬀectiveness of ACT by reducing activation footprint by up to 12 ×with 2-bit activations. Although ACT has already demonstrated impressive compression capabilities, previous work on ACT is restricted to speciﬁc NN architectures. For example, ActNN [ 5] is a quantization framework for convolutional NNs only; Mesa [ 7] proposes a per head/layer quantization method for vision transformers; and AC-GC [ 6] derives convergence error bound for diﬀerent types of operators separately. Developing a generic ACT framework is challenging. Theoretically, convergence guarantees must be made without assumptions on the network architecture. Algorithmically, the framework should ﬁnd eﬀective compression strategies for all kinds of networks automatically. From the system perspective, the framework should support arbitrary NN operations, including user-deﬁned ones. In this work, we propose GACT, a general framework for ACT that is agnostic to the NN architecture. Neither specialized mathematical derivations nor customized implementation is needed to support diﬀerent operators. To enable this, we develop a general convergence theory by analyzing the stochastic gradient (SG) introduced by ACT. We show that the SG can be well approximated by a linearized version, which is unbiased to stochastic compressors. The variance of the linearized gradient has a particularly simple structure that allows a numerical algorithm to predict the variance given a compression strategy. Then, we generate the strategy by approximately solving an integer program. 1 arXiv:2206.11357v4  [cs.LG]  3 Sep 2022We implement our method as a library based on PyTorch that can be quickly integrated into real-world machine learning systems. The library also provides several optimization levels to explore the trade-oﬀ between memory and speed. We demonstrate the ﬂexibility and eﬃciency of GACT on various tasks, including image classiﬁcation, object detection, text, and graph node classiﬁcation. Our evaluation shows that GACT can reduce activation memory by up to 8.1 ×, enabling training with a 24.7 ×larger batch size on the same GPU. In sum, our main contributions are as follows: • We propose a general convergence theory for ACT. • We develop an algorithm that automatically estimates the sensitivity of each compressed tensor and selects the optimal compression strategy. • We build eﬃcient implementation of GACT in PyTorch with an easy-to-use API that can also be combined with other memory-saving techniques seamlessly. 2 Related Work Activation Compressed Training.ACT has been applied to convolutional NNs using diﬀerent compressors, such as quantizers [3, 4, 5], JPEG [8], or scientiﬁc data compression algorithms [ 9, 6]. ACT is also applied to transformers [7] and graph NNs [10]. However, the existing theory for ACT [ 3, 4, 5, 6] relies on the case-by-case analysis of speciﬁc network operators, such as convolution, ReLU, and batch normalization. It also requires dedicated implementations for each operator. On the contrary, GACT focuses on the generality of activation compressed training, not a speciﬁc quantizer design, which is the main topic of previous work. Instead of assuming that the network is a stack of layers, GACT formulates the problem as a computational graph of operators. This is general enough to cover transformers [11], graph NNs [12], second-order derivatives, and unknown future architectures. Reduced Precision Training. Apart from ACT, reduced precision training [13, 14, 15, 16, 17, 18] performs calculations directly on low precision data, reducing the computation cost and memory footprint simultaneously. To achieve this, specialized kernels are used to calculate on low precision data. In contrast, ACT only considers storage, and it can thus use more ﬂexible compression strategies and achieve a much better compression ratio with the same accuracy loss. Memory-Eﬃcient Training. Gradient checkpointing [19, 20] trades computation for memory by dropping some of the activations in the forward pass from memory and recomputing them in the backward pass. Swapping [21, 22, 23, 24] oﬄoads activation or model parameters to an external memory (e.g., CPU memory). Recent work [25] explores the possibility of combining the gradient checkpointing and swapping. All these methods save memory by storing fewer tensors on the GPU. In contrast, GACT compresses the saved tensors and is complementary to these approaches. Moreover, the generality of GACT enables easy combination with these methods, which we explore in this paper. 3 Formulation We ﬁrst present the mathematical formulation of our activation compressed training (ACT) framework. As we would like to develop a general ACT algorithm, applicable to a wide range of NN architectures, we make minimal assumptions on our formulation. Throughout the paper, we deﬁne the variance of a vector x as Var [x] = E [ ∥x∥2 ] −∥E[x]∥2. 3.1 Activation Compressed Training In this work, we abstract the forward propagation as two functions ℓ(x; θ) and h(x; θ). Both take a datum x and the model parameter θ as the input. The loss function ℓ(x; θ) outputs the loss of the network θ on datum x. The context function h(x; θ) outputs tensors to be stored in the memory for computing the gradients, which are referred as the context. Assume that the context consists of L tensors, where each tensor h(l)(x; θ) is represented by a ﬂattened Dl-dimensional vector. Denote h(x; θ) = ( h(l)(x; θ))L l=1. Our notations are somewhat unconventional in the sense that we do not explicitly deﬁne each layer’s activation. We do not even assume that there is a NN. It could be any computational graph that saves context tensors. Given a dataset X = {xn}N n=1, deﬁne the batch loss L(θ) := 1 N ∑N n=1 ℓ(x; θ). The dataset can be equivalently represented as an empirical data distribution pX(x) := 1 N ∑N n=1 δ(x−xn), where δ is the Dirac 2Type\tequation\there. ConvBNConv……ReLU ReLU……BNConvConv Compressed context tensors (GPU) ℓ(𝑥;𝜃) Pack hook Unpackhook context ℎ(𝑥;𝜃) Swap outSwap in 𝑄(ℎ(𝑥;𝜃)) Forward Computation Graph Backward Computation Graph CPU Memory bits 𝑔(𝑄ℎ(𝑥;𝜃);𝜃) Adaptive AlgorithmCompressorGACT Decompressor 𝑥,\t gradient𝑔(𝑄ℎ(𝑥;𝜃);𝜃) (𝜃)model Figure 1: The architecture of GACT. delta function. The batch loss can be written as L(θ) = EX[ℓ(x; θ)], where EX denotes for taking expectation over pX. The network is trained with stochastic gradient descent (SGD) [ 26]. Starting from an initial model θ0, at the t-th iteration, SGD updates the model with: θt+1 ←θt −η∇θℓ(x; θt), (1) where η is a learning rate, and the SG ∇θℓ(x; θ) is computed on a random datum x ∼pX. Notice that EX[∇θℓ(x; θ)] = ∇θL(θ), i.e., the SG is an unbiased estimator of the batch gradient ∇θL(θ). Crucially, the SG can be written in the form ∇θℓ(x; θt) = g(h(x; θt); θt). In other words, the back propagation only depends on the forward propagation through the context h(x; θt). The entire context must be kept in memory for computing the gradients. The context dominates the memory consumption in many applications. ACT reduces the training memory footprint by compressing the context. Let Q(h) be a compressor, which converts h to compact formats while keeping Q(h) ≈h. Then, ACT computes the gradient with compressed context: θt+1 ←θt −ηg(Q(h(x; θt)); θt). (2) We refer to g(Q(h(x; θt); θt) as the activation compressed (AC) gradient. ACT is signiﬁcantly more memory eﬃcient then the plain SGD, Eq. (1), since it only needs to store a compressed version of the context. Suppose the original context h(x; θt) consists of 32-bit ﬂoating point tensors, and Q(·) is a compressor which quantizes tensors to 2-bit integers, ACT will reduce the context memory by 16 ×. Fig. 1 illustrates the computational graph of ACT with these notations. In the following presentation, we might denote h(x,θ) simply by h when there is no confusion. 3.2 Convergence of ACT ACT is a lossy approximation of SGD, as it uses an approximate gradient g(Q(h); θ). Therefore, some kind of theoretical guarantee is required for ACT to be useful. Fortunately, analyzing ACT is made signiﬁcantly simpler by introducing an unbiased stochastic compressor Q(·), such that EQ[Q(x)] = x for any x. EQ[·] means taking expectation over the compressor. In this way, g(Q(h); θ) can be viewed as a stochastic estimator of the batch gradient ∇L(θ), but the randomness comes not only from the datum x but also the compressor Q(·). Therefore, ACT is still an SGD algorithm. Standard analytical tools for SGD [ 27] are applicable for studying ACT. SGD algorithms have particular good properties when the SG is unbiased. In our case, this means EQ[g(Q(h); θ)] = g(h; θ). However, the SG is biased general, even when the stochastic compressor itself is unbiased.1 The key technique in this work is to construct an unbiased approximation of the AC gradient by linearizing 1Consider the exampleg(h) =I(h ≥0.5), whereh ∈[0, 1] and its AC gradientg(Q(h)) =I(Q(h) ≥0.5) with the compressor Q(h) ∼Bernoulli(h). Then, E [g(Q(h))] =P(Q(h) = 1) =h ̸= g(h). 3the gradient function g(·; θ). Consider the ﬁrst-order Taylor expansion of g(·; θ) at h: ˆg(Q(h); h,θ) := g(h; θ) + J(h,θ)∆h, (3) where J(h,θ) := ∂g(h;θ) ∂h is a Jacobian matrix, ∆ h := Q(h) −h is the compression error. We further denote ˆgxθ(Q(h); h) := ˆg(Q(h); h,θ)|h=h(x;θ) and Jxθ(h) := J(h,θ)|h=h(x;θ) for short. Since E[∆h(x; θ)] = 0, ˆgxθ(Q(h); h) is an unbiased SG, Furthermore, the approximation error is small: Proposition 1. Assuming that g(h; θ) is twice diﬀerentiable w.r.t. h, and the second order derivative is bounded, then E[∥g(Q(h); θ) −ˆgxθ(Q(h); h)∥2] = O(VarQ[∆h]). Since ∆h itself is unbiased, VarQ[∆h] = EQ [ ∥∆h∥2 ] is simply the expected compression error. Prop. 1 implies that the linearization error is bounded by the compression error. The linearized gradient ˆg is accurate if the compression is accurate. Using ˆg as a bridge, we arrive in the following convergence theorem: Theorem 1. Assume that: A1. L(θ) is a continuous diﬀerentiable, ∇L(θ) is β-Lipschitz continuous. A2. L(θ) is bounded below by L∗. A3. g(h; θ) is diﬀerentiable w.r.t. h and ∃b> 0, s.t. ∀θ,E∥g(Q(h(x; θ)); θ) −ˆgxθ(Q(h); h)∥≤ b. A4. ∃σ2 >0, s.t., ∀θ, Var [ˆgxθ(Q(h); h)] ≤σ2. Then, for all η <1 2β, if we run ACT deﬁned as Eq. (2) for T iterations, then we have min t=0,...,T−1 E [ ∥∇L(θt)∥2 ] ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2 Remark: The analytical technique used in Thm. 1 is rather standard, see Thm. 4.8 in [ 27]. However, we consider the variance term σ2 of the linearized gradient, rather than the SG itself. This formulation brings better analytical properties and an adaptive algorithm for determining the compression scheme, as we shall see soon in Sec. 4. The convergence of ACT is aﬀected by both the linearization error (A3) and the variance of the unbiased gradient ˆg(·; θ) (A4). The latter is characterized as: Proposition 2. Var [ˆgxθ(Q(h); h)] = VarX[g(h; θ)] + EX[VarQ[ˆgxθ(Q(h); h)]] , where the second term on the RHS equals to EX[VarQ[Jxθ(h)∆h]] = O(VarQ[∆h]) . Prop. 2 separates the variance from diﬀerent noise sources. VarX[g(h(x,θ); θ)] is the variance raised by random sampling of data (“sampling variance”). EX[VarQ[Jxθ(h)∆h(x,θ)]] is the variance raised by compression. Now, the convergence in Thm. 1 is depicted by 3 b2 + ηβσ2. By Prop. 1, b2 = O(VarQ[∆h]2). By Prop. 2, σ2 = O(1) +O(VarQ[∆h]), since the sampling variance is not aﬀected by compression. Therefore, when the compression is accurate (∆ h →0), the impact of the linearization error is negligible, and the variance of the unbiased gradient dominates. ACT behaves as if the AC gradient is unbiased. 4 Adapting the Compression Rate In a network, some context tensors (such as those stored for computing the cross entropy loss) are extremely sensitive, a small amount of compression would result in diverged training, while other tensors are quite robust to compression. Therefore, we must apply diﬀerent amounts of compression for each context tensor. As a general framework, we have no prior knowledge of the users’ model architecture, so we designed an algorithm to infer the sensitivity for each context tensor and determine their compression rate automatically. There is a tradeoﬀ between the compression error and the storage requirement. We represent the storage requirement of the compressed context in bits per dimension . We assume that bl bits/dim. are used for compression h(l), and Qbl(h(l)) be the compression result. Let b = (bl)L l=1 be a compression scheme, Qb(h) := {Qbl(h(l))}L l=1, and ∆bh= Qb(h) −h. 44.1 Structure of Variance As discussed in Sec. 3.2, when the compression is relatively accurate, the variance plays the main role in determining the convergence. Therefore, we would like to investigate how the compression scheme would impact the variance. Formally, we are interested in: V(b; h,θ) := VarQ[ˆg(Qb(h); h,θ)] . Once V(b,h; θ) is known, we can ﬁnd the minimum variance compression scheme under a given total bits budget B, by solving the integer programming problem: min b V(b; h(x; θ),θ), s.t. L∑ i=1 blDl ≤B, (4) where Dl is the dimensionality of h(l). To proceed, we need the following assumptions on the compressor Qb(·): Assumption B1: The compressed result is element-wise uncorrelated. That is, for anyi̸= j, Cov [Qb(h)i,Qb(h)j] = 0. Assumption B2: For compressing h(l)(x; θ) to bl bits/dim., the compression error can be written in the form Var [ ∆blh(l)(x; θ)j ] ≤Rlj(x; θ)S(bl), where S(bl) is a known function. This isolates the eﬀect of bl through the unary factor S(bl). Both assumptions can be achieved by a stochastic rounding quantizer [28], where Rlj(x; θ) = 1 4 ( maxkh(l) k −minkh(l) k )2 and S(b) = (2bl −1)−2. See Appendix A.4 for the derivations. The following theorem reveals the structure of the variance: Theorem 2. Under assumptions B1, B2, there exists a family of functions {cl(h,θ)}L l=1, such that the compression variance can be written in the form V(b; h,θ) ≤ L∑ l=1 cl(h,θ)S(bl). (5) 4.2 Computing Sensitivity Thm. 2 reveals two good properties of the variance: (1) the impact of compressing diﬀerent context tensors simply sums up, without aﬀecting each other; and (2) the compression scheme only impacts the variance through S(bl). Both properties are brought about by linearization. Since S(·) is a known function, we only need to know cl(h,θ) to solve problem Eq. (4). cl(h,θ) can be understood as the sensitivity of the AC gradient to the compression of the l-th tensor. We can compute cl(h,θ) numerically by leveraging the idempotence of compressing a tensor: Assumption B3: If h= Q(h′) for some h′with non-zero probability, then Q(h) = h and VarQ[Q(h)] = 0. Let Q¬(l) b (h) = {Qb1 (h(1)),...,h (l),...,Q bL(h(L))}be some tensors, where every tensor except h(l) is compressed. Plug h= Q¬(l) b (h) into Eq. (5), and use B3, we have V(b; Q¬(l) b (h),θ) ≤cl(Q¬(l) b (h),θ)S(bl). The left hand side can be approximated by taking ˆg(Qb(h); h,θ) ≈g(Qb(h); θ). Assume that cl(·,θ) is reasonably continuous, we have cl(h,θ) ≈VarQ[g(Qb(h); θ)] |h=Q¬(l) b (h)/S(bl). The variance can be replaced by empirical variance. Alg. 1 illustrates this idea. To compute VarQ[g(Qb(h); θ)] at h= Q¬(l) b (h), we keep the random seeds ﬁxed for all the compressors except the l-th one. We compute the empirical variance by two evaluations of g(Qb(h); θ), which are two NN iterations (forward + backward propagation). Finally, we assume thatc(h,θ) remains stable for diﬀerent mini-batches h, and along the training trajectory (θt). Therefore, we maintain a cl for each tensor l, which is updated by periodically running Alg. 1. Eq. (4) is approximately solved by the O(Llog2 L) greedy algorithm [5]. Another useful feature of this approach is predicting failure (in an a posteriori manner). If the compression variance V(b; h,θ) is dominating the overall gradient variance Var [g(Q(h); θt)], compression is adding too much noise to the gradient, and the convergence might be aﬀected. The overall gradient variance can be 5Algorithm 1 Numerical algorithm for computing cl(h,θ). Require: A gradient evaluation function g(·; θ) Require: A series of L+ 1 random seeds (rl)L+1 l=1 . Require: Any compression scheme b= (bl)L l=1 ∀l, seed Q(l) with rl g0 ←g(Qb(h); θ) {First iteration} ∀l, seed Q(l) with rl seed Q(l) with rL+1 g1 ←g(Qb(h); θ) {Second iteration, with another seed } Return 1 2 ∥g0 −g1∥2 /S(bl) 1importtorch2 importgact3 4model = ... # user defined model5 controller = gact.controller(model, opt_level='L2')6  controller.install_hook()78  # trainingloop9forepochin...10 foriterin...11......12 # instruct gact how to perform forward and backward13deffwdbwdprop():14output = model(data)15loss =loss_func(output,target)16optimizer.zero_grad()17loss.backward()1819controller.iterate(fwdbwdprop) Figure 2: Usage example of GACT computed by maintaining a running mean of the gradient. If V(b; θ)/Var [ˆg(Q(h); θt)] is too large, we can raise an alert to the user to increase the storage budget. 5 System Implementation We implemented GACT as a lightweight library in PyTorch. Users can use GACT for any NN architecture with several lines of code change. GACT uses low-level PyTorch hooks to capture context tensors, so it supports arbitrary operators, including custom operators deﬁned by users. We implemented eﬃcient CUDA kernels to infer tensor sensitivity and to perform compression during run time. GACT uses the same per-group quantizer in ActNN [5] as the compressor. However, GACT diﬀers from ActNN in several aspects. ActNN relies on manual analytical deduction to compute the sensitivity for diﬀerent operators, while GACT infers tensor sensitivity automatically, as described in Sec. 4.2. Moreover, ActNN performs layer-level quantization. It has to implement an activation compressed version for each operator and substitute operators during the training (e.g., replace torch.nn.Conv2d with actnn.Conv2d). In contrast, GACT runs at tensor level and uses a single hook interface to compress saved tensors for all operators. 5.1 General API As shown in Fig. 2, the interface of GACT is straightforward and intuitive, requiring the user to (i) initialize the GACT controller and specify an optimization level (Line 5); (ii) install hooks (Line 6); and (iii) instruct GACT how to perform forward and backward propagation (Lines 13-17) and pass it as a function (fwdbwdprop) to the controller (Line 19). We require users to specify (iii) because GACT needs to numerically run the forward and backward pass to infer tensor sensitivity. Although fwdbwdprop is passed to the controller every iteration, it is only called internally every adapt interval iterations when tensor sensitivity changes. As shown in Sec. 6.1, tensor sensitivity stabilizes quickly after the ﬁrst several epochs, adapt interval can thus be set to a large number, introducing negligible impact on training speed. 65.2 System Architecture Fig. 1 shows an overview of GACT. The GACT controller has three modules: Adaptivate Algorithm; Compressor; and Decompressor. In the forward pass, the controller uses PyTorch pack hook to capture all context tensors. Then Adaptive Algorithm infers tensor sensitivity based on gradients and assigns higher bits to more sensitive tensors, as described in Sec. 4.2. The bits information is used to instruct Compressor to perform quantization. In the backward pass, Decompressor dequantizes context tensors and uses unpack hook to send the dequantized results back to the PyTorch’s auto diﬀerentiation engine. The controller is also responsible for swapping quantized tensors to the CPU and prefetching them back during the backward propagation if swapping is enabled. 5.3 Identifying Tensors to Quantize The pack hook and unpack hook process all types of context tensors, including activation, parameters trained by the optimizer, and training states such as running mean/variance used by batch normalization. To guarantee that only the activations are quantized, we ﬁlter out saved parameters by recording the data pointers of all the model parameters before training, and we skip quantization if the input tensor pointer exists in the parameter pointer set. Similarly, GACT does not quantize training states by checking if the input tensor requires gradients. However, using hooks blindly disables some memory-saving optimization. For example, in a transformer’s self-attention layer, the keys, query, value tensors are all calculated from the same input tensor. The saved objects of the three operations thus all refer to the same tensor. In this case, PyTorch triggers the pack hook three times. If we perform quantization blindly, we waste computation resources and introduce extra memory consumption because the same underlying tensor is quantized and saved more than once. GACT avoids duplication by generating footprints for each input context tensor. We use the CUDA data pointer, sampled data points, and the tensor statistics (e.g., sum) as the footprint. GACT manages all quantized context tensors and uses the footprint to diﬀerentiate them. If a tensor is already quantized, GACT will skip quantization and return previous results directly. 5.4 Parallel Swap and Prefetch To further reduce activation memory, we combine GACT with swapping. All compressed tensors are oﬄoaded to the CPU during the forward pass and swapped back in the backward pass. Here, we replace the original tensor with quantized activation, as data movement is more expensive than computation. Swapping the original tensor saves the quantization overhead but adds more data movement cost between CPU and GPU. As shown in Sec. 6.4, quantization overhead is much smaller than copying full-precision data to CPU in modern GPU architecture. Furthermore, we create two new streams (swap in/out) to parallelize the computation and swapping operation to reduce the swap overhead. The forward computation and swap-out process happen in parallel during the forward pass. During the backward pass, in each layer the swap-in stream is responsible for prefetching the compressed activation of the previous layer to avoid synchronization overhead. We leverage the CUDA event to ensure tasks in diﬀerent streams are executed in the correct order. 5.5 Other Memory Optimizations Gradient checkpointing. Gradient checkpointing [ 19] works by dividing the NN into segments. The algorithm only stores the inputs of each segment and recomputes the dropped activations segment by segment during backpropagation. The memory consumption is thus the cost of storing the inputs of all segments plus the maximum memory cost to backpropagate each segment. When combined with gradient checkpointing, GACT can reduce the memory consumption of both parts. GACT reduces the memory consumption of the ﬁrst part by quantizing the segment inputs. Moreover, the activations saved during the recompute phase are also quantized, reducing the memory cost of the second part. Combining GACT with gradient checkpointing might introduce more training noise because the recompute starts from quantized segment inputs, making the forward pass of recompute phase not exact. However, in Sec. 6.4, we show the noise introduced by forwarding from the quantized tensors is negligible. Memory eﬃcient self-attention. When the batch size is very large, the single layer after dequantization occupies a large amount of memory and prevents the batch size from increasing further. We observe this 7Table 1: Optimization levels for GACT. Level Compression Strategy Bits L0 Do not compress 32 L1 per-group quantization with auto-precision 4 L2 L1 + swapping/prefetching 4 CB1 L1 + gradient checkpointing 4 CB2 CB1 + eﬃcient self-attention 4 relu conv *pool conv *conv *pool conv *conv *pool conv *conv *pool linear *drop linear *drop linear loss 1 2 4 8 32bits 10−1 100 101 cl (a) Inferred per-tensor cl (line) and bits/dim. (bar) for VGG-11. Layers with * have a preceding ReLU layer with shared context. drop=dropout, loss=cross entropy loss. 1 2 4 8 bits/dim. 10−2 10−1 100 101 grad. var. uniform adapt (b) Gradient variance. 0 50 100 epoch 10−2 100 102 104 cl (c) Evolution of the per- tensor sensitivity. Each line iscl for a tensor. Figure 3: Eﬀectiveness of the adaptive algorithm. problem in transformer-based models where self-attention has quadratic space complexity in terms of sequence length. To reduce the memory footprint of the self-attention layer, we implement the algorithm introduced in [29] that achieves linear space complexity, and combines it with GACT. 5.6 Optimization level To exploit the trade-oﬀ between memory saving and training speed, GACT provides several optimization levels. Higher levels can save more memory but with more overhead. Tab. 1 lists these optimization levels. L1 uses per-group quantization with the adaptive algorithm. L2 combines per-group quantization with swapping and prefetching. For transformer-based models, CB1 combines GACT with gradient checkpointing. CB2 further reduces the peak memory by adding eﬃcient self-attention to CB1. 6 Experiments We ﬁrst demonstrate the eﬀectiveness of the GACT adaptive algorithm. We further apply GACT to a wide range of machine learning tasks, including image classiﬁcation, object detection, text, and graph node classiﬁcation. We compare the training accuracy and activation compression rate for full precision, adaptive 4/3/2 (using GACT to adaptively decide quantization bits with an average of 4/3/2 bit) and ﬁx-4 bit (quantizating all tensors uniformly with 4 bits). Next, we study the trade-oﬀ between compression rate and training throughput and compare GACT with other state-of-the-art memory-saving methods. Lastly, we demonstrate the ﬂexibility of GACT by exploring the possibility of combining it with other memory optimization methods (CB1, CB2 as listed in Table 1). We use open-source model implementations for all tasks. 6.1 Compression Strategy We ﬁrst test the eﬀectiveness of our adaptive compression rate algorithm for training VGG-11 [ 30] on ImageNet. Fig. 3(a) plots the inferred per-tensor sensitivity cl and the corresponding optimal bits/dim. GACT assigns more bits to more sensitive layers. The context tensor saved by the cross-entropy loss operator is most sensitive. A small amount of compression leads to a huge gradient variance. This makes sense since the loss is the ﬁrst operator to back-propagate through, where the error accumulates. Therefore, GACT assigns 32 bits/dim. for the tensors in the classiﬁcation head. With the adaptive algorithm, GACT with an average of 4 bits/dim. achieves smaller gradient variance than uniformly assigning 8 bits/dim. for all the tensors, as shown in Fig. 3(b). Finally, Fig. 3(c) shows that the sensitivity cl(h; θt) remains stable during training.Therefore, periodically updating cl at a large interval is reasonable, and this introduces negligible impact on training speed. 8Table 2: For classiﬁcation, we train VGG11 [ 30], ResNet-50 [31], and Swin-Tiny [ 32] on ImageNet [ 33]. For object detection, we train RetinaNet [ 34], Faster R-CNN [35] on Coco [ 36]. We report accuracy on validation sets (Div. indicates diverge) and the compression rate of context tensors (numbers in brackets) for both tasks. Task Model FP32 GACT Adapt 4bit (L1) GACTAdapt 2bit Cls. VGG11 68.75 68.77 (2.84×) 68.49 (3.34×) ResNet-50 77.29 76.96 (6.69×) 76.13 (11.39×) Swin-tiny 81.18 80.92 (7.44×) 77.91 (13.73×) Det. Faster RCNN37.4 37.0 (4.86×) 36.1 (6.81×) RetinaNet 36.5 36.3 (3.11×) Div. 6.2 Optimization level We apply GACT on various computer vision tasks, including image classiﬁcation and object detection, as shown in Fig. 2. We also vary the average bits used by the adaptive algorithm to explore the memory accuracy trade-oﬀ. On both tasks, GACT L1 achieves comparable ( <0.5% accuracy drop) or even better results than the full precision training, while reducing activation memory by up to 7.44 ×. Here, we list the accuracy of FP32 as the strongest accuracy baseline. For other lossy methods we consider in Sec. 6.3, the accuracy is no better than FP32, and we list their training accuracy in Appendix C. Notice that here GACT Adapt 2bit diverges on the detection task. This is because, as shown in Sec.3.2, although ACT has unbiased gradients, the compression error and learning rate aﬀect the convergence. When using 2 bit, the compression error is large and the learning rate has to be reduced accordingly to guarantee convergence. However, we do not want to slow training by decreasing the learning rate. All experiments are run with the same learning rate as the full precision. Therefore when compression error is large, the training diverges. Furthermore, we observe that the memory reduction varies among networks because GACT does not quantize intermediate states, and the size of intermediate states diﬀers between networks. For example, in VGG11, when the batch size is 128, GACT reduces the saved tensor size from 5889MB to 2080MB, among which 78% (1494MB) is used to store the intermediate index for the max-pooling layer that is not quantized by GACT. Next, we demonstrate the ﬂexibility of GACT by applying it to a wider variety of natural language processing (NLP) and graph machine learning (Graph) tasks. We run multiple seeds for each task, and we report the mean ±std of accuracy/F1 across runs as shown in Tab. 3. We include the detailed experimental setup in Appendix B. For both NLP and Graph tasks, GACT L1 achieves comparable training results with FP32, introducing less than 0.3% accuracy/F1-score drop, while reducing activation memory by 4.18 ×to 7.93×. Moreover, the results are stable across runs, introducing similar accuracy variance as FP32. We also show the training results of ﬁx-4bit quantization, where all tensors are uniformly quantized with 4 bits. As shown in Tab. 3, ﬁx-4 bit quantization causes signiﬁcant accuracy/F1-score loss on various graph models. For Bert-large, ﬁxed-4 bit quantization works ﬁne because all the context tensors have similar sensitivity. On the other hand, GACT L1, using a similar amount of memory as always quantizing each layer to 4 bits, still performs on par with full precision training on all the models. This shows the necessity of using adaptive algorithms to assign bits based on tensor sensitivity for stabilized training. Moreover, for Bert-large and three graph models (GCN/GAT/GCNII), GACT converges and gives lossless results with 3 bits. Remarkably, across all the graph models, training with 2-bit GACT causes little accuracy loss ( <1%). This shows the robustness of our adaptive algorithm. 6.3 Memory Saving and Computational Overhead Settings and baselines. We implement the benchmark with PyTorch 1.10 and measure the memory saving and overhead of GACT on an AWS g4dn.4xlarge instance, which has a 16GB NVIDIA T4 GPU and 64GB CPU memory. On ResNet-50, we compare with ActNN [ 5], a dedicated quantization framework for convolutional NNs, and DTR [ 21], a state-of-the-art rematerialization method for dynamic graphs. “swap” is a simple swapping strategy that swaps all activations to the CPU. For Bert-large, we also show the results on Mesa [ 7], a memory-saving resource-eﬃcient training framework for transformers, and ZeRO-Oﬄoad [ 37], a highly optimized system for training large-scale language models. Gradient checkpointing uses the default checkpointing policy provided by the transformer library [ 38], where only the input to each transformer block is saved before the backward pass. On Swin-tiny, we only include Mesa and swap because other baselines 9Table 3: Accuracy and activation compression rate for NLP and Graph tasks. Accuracy that drops > 1% is in italic font. Model Dataset FP32 Fix 4bit GACT Adapt 4bit (L1)GACT Adapt 3bitGACT Adapt 2bit GCN Flickr 51.17±0.19 50.93±0.16 (7.56×) 51.08±0.18 (7.93×) 51.14±0.18 (11.34×) 51.20±0.18 (17.56×) Reddit 95.33±0.07 94.42±0.11 (7.55×) 95.32±0.07 (7.90×) 95.31±0.07 (9.70×) 95.34±0.06 (13.68×) Yelp 39.86±0.94 39.85±1.22 (5.94×) 40.06±0.74 (6.42×) 40.21±0.82 (7.46×) 39.89±1.45 (9.00×) ogbn-arxiv71.51±0.65 68.61±0.77 (7.54×) 71.35±0.36 (8.09×) 70.82±0.95 (10.45×) 70.87±0.66 (13.75×) GAT Flickr 52.40±0.28 35.24±11.90 (4.23×) 52.26±0.31 (4.34×) 51.68±1.13 (5.04×) 51.62±1.19 (5.46×) Reddit 95.95±0.06 59.37±11.48 (4.12×) 96.02±0.09 (4.29×) 95.96±0.06 (4.64×) 95.82±0.06 (5.24×) Yelp 52.41±0.69 36.09±13.70 (4.04×) 52.18±0.38 (4.18×) 51.63±0.83 (4.53×) 51.15±0.53 (5.24×) ogbn-arxiv71.68±0.54 54.64±5.62 (5.04×) 71.80±0.47 (5.09×) 71.47±0.50 (6.14×) 71.21±0.68 (6.98×) GCNII Flickr 52.37±0.16 52.28±0.16 (4.84×) 52.31±0.16 (4.91×) 52.36±0.16 (5.54×) 52.23±0.15 (6.44×) Reddit 96.32±0.24 86.50±1.08 (4.51×) 96.11±0.22 (4.52×) 96.01±0.33 (5.16×) 95.54±0.29 (5.92×) Yelp 62.33±0.20 62.21±0.22 (5.26×) 62.28±0.26 (5.34×) 62.53±0.36 (6.29×) 62.33±0.37 (7.28×) ogbn-arxiv72.52±0.12 44.57±5.01 (6.54×) 72.28±0.35 (6.74×) 72.22±0.28 (7.98×) 71.74±0.26 (10.24×) Bert- large MNLI 86.74±0.24 85.98±0.16 (7.55×) 86.61±0.11 (7.38×) 86.68±0.08 (9.13×) 84.24±0.74 (12.87×) SST-2 93.69±0.30 93.46±0.23 (7.55×) 93.54±0.52 (7.30×) 93.20±0.37 (9.05×) 91.90±1.04 (12.91×) MRPC 88.20±0.02 87.36±0.19 (7.55×) 87.90±0.10 (7.40×) 87.69±0.07 (9.19×) 82.54±0.38 (12.91×) QNLI 92.29±0.14 92.34±0.07 (7.55×) 92.44±0.07 (7.42×) 92.43±0.31 (9.19×) 90.74±0.13 (12.95×) Table 4: Largest models GACT can train with 16G GPU memory. In ResNet (batch size=64), D (depth): number of layers, W (width): base width of the bottleneck block, R (resolution): width and height of input images. In Bert-large (batch size=16) and GCN, D (depth): number of transformer/gcn blocks, W (width): hidden size. Dim Maximum Value Throughput (TFLOPS) FP L1 L2 FP L1 L2 ResNet- 152 D 160 460 1124 0.43 0.47 0.41 W 88 304 320 0.44 0.89 0.6 R 232 548 716 0.41 0.39 0.44 Bert- large D 32 56 64 0.67 0.56 0.53 W 1280 1488 2032 0.68 0.61 0.60 GCN D 24 152 240 0.20 0.14 0.15 W 2464 3948 4244 0.36 0.38 0.40 lack the support for this network. Results. We compare the training throughput of GACT against other memory saving systems in Fig. 4. On ResNet-50, GACT achieves similar throughput as ActNN (ActNN optimization L5 is not listed because it optimizes PyTorch memory allocation, which is unrelated to quantization and can also be applied to GACT), but ActNN enables training with a larger batch size. This is expected because ActNN implements eﬃcient, customized layers for diﬀerent operators in convolutional NNs. For Bert-large, Zero-oﬄoad fails quickly because it only oﬄoads optimizer states that occupy a small portion of total memory to CPU. GACT L1 outperforms Mesa because Mesa only compresses tensors to 8 bit. When the batch is bigger, the activation size of each segment becomes the memory bottleneck and prevents gradient checkpointing from increasing the batch size. Moreover, combining GACT with gradient checkpointing and eﬃcient self-attention further reduces the peak memory, increasing the batch size by up to 24.7 ×. Meanwhile, it introduces a small throughput overhead compared with the original gradient checkpointing. Across all the network architectures, GACT enables training with a 4.2 ×to 24.9×larger batch size under the same memory budget. Network scaling. With GACT, we can construct larger models or train with a higher image resolution. Tab. 4 compares the largest model we can train against full precision. With the same batch size and memory budget, GACT can scale a ResNet-152 to 7.0 ×deeper, 3.6 ×wider or 3.0 ×higher resolution. Similarly, Bert-large can be scaled to 2.0 ×deeper or 1.6×wider. In GCN, GACT enables training 10.0 ×deeper and 1.7×wider network. Overall, GACT maintains 75% - 136% original training throughput. 6.4 Other Optimizations We evaluate the idea of combining GACT with swapping on Bert-large-cased. As shown in Tab. 5, swapping compressed tensors is faster than swapping the original ones because communication between CPU and GPU is more time-consuming than computation. Combining GACT with swapping increases training speed by 10(a) 0 200 400 600 800 Batch Size 0 50 100Training Throughput 4.3× L0 L1 L2 ResNet-50 DTR Swap ActNN GACT (b) 0 100 200 300 400 500 600 Batch Size 0 10 20Training Throughput 24.7× L0 L1 L2 CB1 CB2 Bert-large ZeroOff Swap Mesa CKPT GACT (c) 0 100 200 300 400 500 600 Batch Size 0 25 50 75Training Throughput 5.6× L0 L1 L2 Swin-tiny Mesa Swap GACT Figure 4: Training throughput vs batch size. Red cross mark means out-of-memory. The shaded yellow region denotes the batch sizes with full precision training given the memory budget. CKPT: Gradient checkpointing, ZeroOﬀ: ZeRO-Oﬄoad. Table 5: Swap and prefetch speed/memory on Bert-large. Algorithm Speed (sequence/s) Peak Mem. (MB) Total Mem. (MB) FP32 16.41 9573 9527 FP32 + swap 6.02 5215 5093 GACT swap 12.95 5426 5325 GACT swap + prefetch14.02 5426 5324 up to 2.3×. Notice here that the peak memory use of “GACT swap” is slightly higher than “FP32 + swap” because GACT does not quantize and swap intermediate states such as running mean/var of BatchNorm layer. Moreover, prefetch increases the speed by about 7% with negligible memory overhead. We next demonstrate combining GACT with gradient checkpointing (CB1). Gradient checkpointing is performed at the beginning of each transformer block, thus avoiding saving tensors generated within the block. We then apply GACT with gradient checkpointing, where the saved tensors are quantized with 4 bits. As shown in Tab. 6, the accuracy is unaﬀected. We also compare the activation memory and peak memory of CB1 and CB2 in Tab. 7. AM2 denotes the peak activation memory, which is the size of saved tensors after reforwarding the ﬁrst transformer block. When batch size = 288, compared with gradient checkpointing on full precision (FP32), CB1 and CB2 reduce the peak activation size by 4.7 ×and 5.4×respectively. 7 Conclusion This paper presents GACT, an ACT framework for generic NN architectures. We prove the convergence of GACT without prior knowledge about operator type or network architecture by analyzing a linearized Table 6: Accuracy of Bert-large-cased on SST-2 and QNLI datasets Algorithm SST-2 QNLI Algorithm SST-2 QNLI FP32 93.58 92.42 CB1 93.81 92.26 11Table 7: Memory use of diﬀerent algorithms on Bert-large. AM1: Activation size before backward, AM2: Activation size after reforwading the ﬁrst transformer block. When batch size = 288, L0 runs out of memory, and therefore it is not listed below. Batch Size Algorithm AM1(MB) AM2(MB) Peak Mem.(MB) 16 L0 4434 - 9573 FP32 + CKPT210 394 5541 CB1 37 99 5286 CB2 31 79 5269 288 FP32 + CKPT3783 7092 12885 CB1 515 1497 8251 CB2 486 1307 8102 approximation of ATC’s gradients. With the adaptive algorithm, GACT achieves negligible accuracy loss on various tasks, reducing activation memory by up to 8.1 ×and enabling training with up to 24.7 ×batch size compared with full precision training. Acknowledgements This work was supported by the National Key Research and Development Project of China (No. 2021ZD0110502); NSF of China Project (No. 62106120), by the National Science Foundation through grants IIS-1955488, IIS-2027575, CCF-1723352, ARO W911NF2110339, ONR N00014-21-1-2724, and DOE award DE-SC0016260. We would also like to acknowledge partial support from DARPA, IARPA, the Sloan Foundation, NSF, and ONR. Our conclusions do not necessarily reﬂect the position or the policy of our sponsors, and no oﬃcial endorsement should be inferred. References [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. [2] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and eﬃcient sparsity. arXiv preprint arXiv:2101.03961 , 2021. [3] Ayan Chakrabarti and Benjamin Moseley. Backprop with approximate activations for memory-eﬃcient network training. arXiv preprint arXiv:1901.07988 , 2019. [4] Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and Bin Cui. Don’t waste your bits! squeeze activations and gradients for deep neural networks via tinyscript. In International Conference on Machine Learning, pages 3304–3314. PMLR, 2020. [5] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W Mahoney, and Joseph E Gonzalez. Actnn: Reducing training memory footprint via 2-bit activation compressed training. In International Conference on Machine Learning , 2021. [6] R David Evans and Tor Aamodt. AC-GC: Lossy activation compression with guaranteed convergence. Advances in Neural Information Processing Systems , 34, 2021. [7] Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai, and Bohan Zhuang. Mesa: A memory-saving training framework for transformers. arXiv preprint arXiv:2111.11124 , 2021. [8] R David Evans, Lufei Liu, and Tor M Aamodt. Jpeg-act: accelerating deep learning via transform-based lossy compression. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pages 860–873. IEEE, 2020. [9] Sian Jin, Guanpeng Li, Shuaiwen Leon Song, and Dingwen Tao. A novel memory-eﬃcient deep learning training framework via error-bounded lossy compression. In 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming , pages 485–487, 2021. 12[10] Anonymous. EXACT: Scalable graph neural networks training via extreme activation compression. In Submitted to The Tenth International Conference on Learning Representations , 2022. under review. [11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, / Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017. [12] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907 , 2016. [13] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. [14] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. In International Conference on Learning Representations, 2018. [15] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit ﬂoating point numbers. In Advances in Neural Information Processing Systems, pages 7675–7684, 2018. [16] Ron Banner, Itay Hubara, Elad Hoﬀer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems , pages 5145–5153, 2018. [17] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. In Advances in neural information processing systems , 2020. [18] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkatara- mani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. In Advances in Neural Information Processing Systems , volume 33, 2020. [19] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 , 2016. [20] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica, and Joseph E Gonzalez. Checkmate: Breaking the memory wall with optimal tensor rematerialization. arXiv preprint arXiv:1910.02653 , 2019. [21] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic tensor rematerialization. arXiv preprint arXiv:2006.09616 , 2020. [22] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 1341–1355, 2020. [23] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic GPU memory management for training deep neural networks. In 23rd ACM SIGPLAN symposium on principles and practice of parallel programming , pages 41–53, 2018. [24] Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-based gpu memory management for deep learning. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 891–905, 2020. 13[25] Olivier Beaumont, Lionel Eyraud-Dubois, and Alena Shilova. Eﬃcient combination of rematerialization and oﬄoading for training dnns. Advances in Neural Information Processing Systems , 34, 2021. [26] L´ eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP- STAT’2010, pages 177–186. Springer, 2010. [27] L´ eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223–311, 2018. [28] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pages 3123–3131, 2015. [29] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [30] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV) , 2021. [33] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. [34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ ar. Focal loss for dense object detection. In International Conference on Computer Vision (ICCV) , pages 2980–2988, 2017. [35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. Advances in neural information processing systems , 28:91–99, 2015. [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014. [37] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-oﬄoad: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840, 2021. [38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´ emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online, October 2020. Association for Computational Linguistics. [39] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931 , 2019. [40] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. 14[41] Petar Veliˇ ckovi´ c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017. [42] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International Conference on Machine Learning , pages 1725–1735. PMLR, 2020. [43] Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, and Jie Tang. Cogdl: Toolkit for deep learning on graphs. arXiv preprint arXiv:2103.00959 , 2021. [44] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353–355, 2018. [45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, 2019. 15A Proof of Theorems A.1 Theorem 1: Convergence of ACT Assume that: A1. L(θ) is a continuous diﬀerentiable, ∇L(θ) is β-Lipschitz continuous. . A2. L(θ) is bounded below by L∗. A3. g(h; θ) is diﬀerentiable w.r.t. h and ∃b> 0, s.t. ∀θ,E∥g(Q(h(x,θ)); θ) −ˆg(h(x,θ); θ)∥≤ b. A4. ∃σ2 >0, s.t., ∀θ, Var [ˆg(h(x,θ)] ≤σ2. Then, for all η <1 2β, if we run ACT deﬁned as Eq. (2) for T iterations, then we have min t=0,...,T−1 E [ ∥∇L(θt)∥2 ] ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2 Proof. Denote m:= ∇θL(θt), ϵ:= ˆg(h(x,θt); θt) −m, d:= g(Q(h(x; θt)); θt) −ˆg(h(x,θt); θt). Then, by A3 and A4, we have E[ϵ] = E[g(h(x,θt); θt) −∇θL(θt)] + E[⟨J(x,θt),∆Q(h(x,θt))⟩] = ⟨J(x,θt),E[∆Q(h(x,θt))]⟩= 0. (6) E [ ∥ϵ∥2 ] = ∥E[ϵ]∥2 + Var [ϵ] = Var [ˆg(h(x,θt); θt)] ≤σ2. (7) E[∥d∥] ≤b. (8) By the deﬁnitions, the ACT dynamics can be written as θt+1 ←θt −η(m+ d+ ϵ). By A1, we have L(θt+1) ≤L(θt) −η⟨m,m + d+ ϵ⟩+ βη2 2 ∥m+ d+ ϵ∥2 . (9) By Eq.s (6,8) E[⟨m,m + d+ ϵ⟩] ≥∥m∥2 −∥m∥∥d∥+ ⟨m,E[ϵ]⟩≥∥ m∥2 −∥m∥b. (10) By Eq.s (6,7,8), and ∥x+ y∥2 ≤2 ∥x∥2 + 2∥y∥2, E [ ∥m+ d+ ϵ∥2 ] = E [ ∥m+ d∥2 ] + Var [ϵ] ≤2E[∥m∥]2 + 2E[∥d∥]2 + Var [ϵ] = 2E[∥m∥]2 + 2b2 + σ2. (11) Taking expectation on both sides of Eq. (9), plug in Eq.s (10, 11), and use η <1 2β, we have E[L(θt+1)] ≤L(θt) −η(∥m∥2 −∥m∥b) + βη2 2 (2E[∥m∥]2 + 2b2 + σ2). =L(θt) −(η−βη2) ∥m∥2 + η∥m∥b+ βη2 2 (2b2 + σ2) =L(θt) −η 2 ∥m∥2 + η∥m∥b+ βη2 2 (2b2 + σ2). Completing the squares, E[L(θt+1)] ≤L(θt) −η 2(∥m∥−b)2 + βη2 2 (2b2 + σ2). Take expectation on both sides and sum up for t= 0,...,T −1, E[L(θT)] −L(θ0) ≤−η 2 T−1∑ t=0 E(∥∇L(θt)∥−b)2 + βη2T 2 (2b2 + σ2). Reorganize the terms, Et [ E(∥∇L(θt)∥−b)2 ] ≤2(L(θ0) −L(θT)) ηT + ηβ(2b2 + σ2). Let t∗= argmintE[∥∇L(θt)∥], and use A1, we have E(∥∇L(θt∗)∥−b)2 ≤2(L(θ0) −L∗) ηT + ηβ(2b2 + σ2). 16Use (a+ b)2 ≤2a2 + 2b2, we have E [ ∥∇L(θt∗)∥2 ] ≤4(L(θ0) −L∗) ηT + (2βη+ 2)b2 + ηβσ2 ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2. A.2 Proposition 1: The Linearization Error Proof. Consider the gradient function g(Q(h(x; θ); θ)), whose output is a P-dimensional vector. Since it is twice diﬀerentiable, we construct the Taylor’s expansion at h(x; θ) with Lagrange remainder: ∃H1,...,H P,s.t., ∀i, gi(Q(h(x; θ)); θ) = gi(h(x,θ); θ) + Ji(x,θ)∆h(x,θ) + ∆h(x,θ)⊤Hi∆h(x,θ), where Ji(h(x; θ),θ) := ∂gi(h(x;θ);θ) ∂h . By the assumption, there exists P >0, such that the linearization error is ∥g(Q(h(x; θ)); θ) −ˆg(h(x; θ); h(x; θ),θ)∥1 = P∑ i=1 ∆h(x,θ)⊤Hi∆h(x,θ) ≤γP ∥∆h(x,θ)∥2 . Taking expectation, E[∥g(Q(h(x; θ)); h(x; θ),θ) −ˆg(h(x; θ); θ)∥2] ≤E[∥g(Q(h(x; θ)); θ) −ˆg(h(x; θ); h(x; θ),θ)∥1] ≤γPVar [∆h(x,θ)] = O(Var [∆h(x,θ)]). A.3 Proposition 2: The Order of the Variance The following proposition is convenient for isolating the diﬀerent noise sources. Proposition A. (Law of Total Variance) Var [X] = E[Var [X |Y]] + Var [E[X |Y]] . Proof. By deﬁnition Var [ˆg(h(x; θt); h(x; θ),θt)] = Var [g(h(x,θ); θ)] + Var [J(h(x; θ),θ)∆h(x,θ)] , where Var [g(h(x,θ); θ)] is the noise introduced by subsampling the data x. By law of total variance, Var [J(h(x; θ),θ)∆h(x,θ)] = EX[VarQ[J(h(x; θ); θt)∆h(x,θ)]] + VarX[EQ[J(h(x; θ); θt)∆h(x,θ)]]   =0 , where VarQ[J(h(x; θ); θt)∆h(x,θ)] =EQ [ ∥J(h(x; θ); θt)∆h(x,θ)∥2 ] ≤EQ [ ∥J(h(x; θ); θt)∥2 ∥∆h(x,θ)∥2 ] = ∥J(h(x; θ); θt)∥2 EQ [ ∥∆h(x,θ)∥2 ] = O(Var [∆h(x,θ)]) . A.4 Proposition 3: The Structure of the Variance Before investigating the structure of VarQ[J(x; θt)∆h(x,θ)], let’s do some recap: the parameter θt is a P-dimensional vector; the context diﬀerence ∆ h(x,θ) is a D-dimensional vector, and J(x; θt) is a P ×D matrix. Recall that ∆ h(x,θ) is the concatenation of L-vectors, ∆h(l)(x,θ), and let J(l)(x,θ) := ∂g ∂h(l) g ( (h(l)(x; θ))L l=1,θ ) , which is a P ×Dl matrix. Furthermore, let h(l) j (x,θ) be the j-th dimension, and J(l) j (x,θ) be its j-th column. To proceed, we need to make the following assumptions to the compressor Q(·) : RD →RD: B1: The compressed result is element-wise uncorrelated. That is, for any i̸= j, Cov [Q(h)i,Q(h)j] = 0. B2: For compressing a vector h to b bits, the compression variance of each dimension can be written in the form Var [Q(h)j] ≤Rj(h)S(b), where S(·) is a known function. Both assumptions can be achieved by a stochastic rounding [28] quantizer, where Q(h)j = { T−1 h,b (⌈Th,b(hj)⌉) w.p. Th,b(hj) −⌊Th,b(hj)⌋ T−1 h,b (⌊Th,b(hj)⌋) otherwise , 17where Th,b(hj) = (2b−1) hj−minj h maxj h−minj h. Since each dimension is quantized independently, B1 is met. Moreover, Var [Q(h)j] ≤1 4 (maxjh−minjh (hj −minjh) )2 (2b −1)−2 = Rj(h)S(b), where Rj(h) = 1 4 (maxjh−minjh (hj −minjh) )2 , S (b) = (2b −1)−2. Proof. By deﬁnition, J(h; θ)∆h= L∑ l=1 Dl∑ j=1 J(l) j (h; θt)∆h(l) j . Using Assumption B1, we have VarQ[J(h; θ)∆h] = EQ    L∑ l=1 Dl∑ j=1 J(l) j (h; θt)∆h(l) j  2  = L∑ l=1 Dl∑ j=1 EQ [J(l) j (h; θt)∆h(l) j  2] . = L∑ l=1 Dl∑ j=1 J(l) j (h; θt)  2 VarQ [ ∆h(l) j ] Using Assumption B2, we have VarQ[J(h; θ)∆h] ≤ L∑ l=1 Dl∑ j=1 J(l) j (h; θt)  2 Rl(h)S(bl) = L∑ l=1 cl(h,θ)S(bl), where cl(θ,h) := Rl(h) J(l)(h; θt) 2 F. B Experiment Setup B.1 Node classiﬁcation task on graphs We conduct experiments on four node classiﬁcation datasets with standard splits, including Flickr, Reddit, Yelp from GraphSAINT [39], and ogbn-arxiv from Open Graph Benchmark (OGB) [ 40]. The four datasets cover extensive downstream applications with diﬀerent scales. We use accuracy as the evaluation metric for multi-class classiﬁcation and micro-F1 for multi-label classiﬁcation. We run ten seeds (0 to 9) and report the average accuracy across runs. We evaluate GACT on three representative GNN models, including GCN [ 12], GAT [41], and GCNII [ 42] under the full-batch training setting. All three models are implemented by CogDL [ 43], a toolkit for graph neural networks. B.2 Text classiﬁcation task We select four largest datasets, MNLI, QQP, SST-2, and QNLI, from the GLUE benchmark [ 44]. The four datasets cover diﬀerent aspects of natural language understanding, including sentiment classiﬁcation, natural language inference and paraphrase detection. We use the mainstream transformer implementation [ 38] to train Bert-large [45]. We run three seeds (42, 43, 44) and report F1 for QQP, accuracy for the others. C Training Accuracy of Baselines For all the baselines we compared in Sec. 6.3, only ActNN, Mesa, and ZeRO-Oﬄoad are lossy methods. All other methods are lossless and have the same training accuracy as FP32. For ResNet-50 on ImageNet, the training accuracy for FP32, GACT, ActNN L2, and ActNN L3 are 77.3, 77.0, 77.4, and 76.9. For Bert-Large on SST-2, the accuracy for FP32, GACT, Mesa, and ZeRO-Oﬄoad are 93.7, 93.5, 93.8, and 93.3. For Swin-tiny on ImageNet, the training accuracy for FP32, GACT, and Mesa are 81.2, 81.0, and 81.3 respectively. 18",
      "meta_data": {
        "arxiv_id": "2206.11357v4",
        "authors": [
          "Xiaoxuan Liu",
          "Lianmin Zheng",
          "Dequan Wang",
          "Yukuo Cen",
          "Weize Chen",
          "Xu Han",
          "Jianfei Chen",
          "Zhiyuan Liu",
          "Jie Tang",
          "Joey Gonzalez",
          "Michael Mahoney",
          "Alvin Cheung"
        ],
        "published_date": "2022-06-22T20:06:23Z",
        "pdf_url": "https://arxiv.org/pdf/2206.11357v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "GACT is proposed as a generic Activation Compressed Training (ACT) framework designed for a broad range of machine learning tasks and neural network architectures. It addresses the challenge of extensive memory requirements for training large models by reducing activation memory footprint. Key contributions include a general convergence theory for ACT, an adaptive algorithm that automatically estimates tensor sensitivity to select optimal compression ratios at runtime, and an efficient PyTorch library implementation. GACT reduces activation memory by up to 8.1x, enabling training with 4.2x to 24.7x larger batch sizes with negligible accuracy loss across diverse tasks.",
        "methodology": "GACT formulates ACT as a computational graph problem, making it agnostic to specific network architectures. It develops a general convergence theory by analyzing a linearized version of ACT's approximate gradient, proving convergence without prior knowledge of operator type or model architecture. The framework employs an algorithm that dynamically determines the compression ratio (bits/dimension) for each context tensor by numerically estimating its sensitivity (impact on gradient variance) via empirical variance computed from two forward+backward passes. This adaptive strategy approximately solves an integer programming problem to minimize variance under a total bits budget. GACT is implemented as a PyTorch library using low-level hooks to capture context tensors, supporting arbitrary operators. It leverages efficient CUDA kernels for runtime sensitivity inference and compression using a per-group quantizer. The system includes features to identify and filter only activations for quantization, avoid duplicate compression, and support parallel swap/prefetch of compressed tensors to/from CPU. It is also designed to seamlessly integrate with other memory optimizations like gradient checkpointing and memory-efficient self-attention, offering different optimization levels (L0-CB2) to balance memory saving and speed.",
        "experimental_setup": "Experiments were conducted on an AWS g4dn.4xlarge instance (16GB NVIDIA T4 GPU, 64GB CPU memory) using PyTorch 1.10. GACT was evaluated on various tasks: image classification (VGG-11, ResNet-50, Swin-Tiny on ImageNet), object detection (RetinaNet, Faster R-CNN on COCO), natural language processing (Bert-large on GLUE benchmarks: MNLI, QQP, SST-2, QNLI), and graph node classification (GCN, GAT, GCNII on Flickr, Reddit, Yelp, ogbn-arxiv). Evaluation metrics included accuracy/F1-score, activation memory compression rate, training throughput, and maximum model scaling capabilities (depth, width, resolution). Baselines included Full Precision (FP32), ActNN, DTR, simple swapping, Mesa, ZeRO-Offload, and Gradient Checkpointing. GACT was tested with adaptive 4/3/2-bit quantization and compared to fixed 4-bit uniform quantization. Multiple runs (10 for graph, 3 for text) were performed to report mean ± standard deviation for accuracy/F1-score.",
        "limitations": "GACT Adapt 2bit can diverge on sensitive tasks like object detection if the compression error is too large and the learning rate is not reduced accordingly. The achieved memory reduction varies between networks because GACT does not quantize all intermediate states (e.g., max-pooling layer indices or BatchNorm running mean/variance), leading to differences in unquantized memory footprint. When combining GACT with swapping, the peak memory use can be slightly higher than FP32 + swap because GACT does not quantize and swap all intermediate states.",
        "future_research_directions": "Future research could explore enhancing stability for ultra-low bit compression (e.g., 2-bit) in sensitive tasks without compromising training speed, potentially by investigating advanced learning rate adaptation strategies or more robust compression techniques. Further work could also focus on extending the compression scope to currently unquantized intermediate states (like max-pooling indices or BatchNorm statistics) to achieve even greater memory savings. Additionally, new stochastic compressor designs and exploring theoretical guarantees under weaker assumptions or for more complex non-differentiable operations could be areas of interest."
      }
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The research identifies and explains a fundamental issue in Graph Attention Networks (GATs) where a high portion of parameters with standard initialization struggle to change during training, particularly in deeper networks, leading to significant performance degradation. The core contribution is the derivation of a conservation law for GAT gradient flow dynamics, which elucidates this lack of trainability. To address this, the study proposes a novel balanced initialization scheme that allows for more effective propagation of gradients, thereby enabling the trainability of deeper GATs and achieving considerable speedups in training and convergence time. This work also establishes a causal link between parameter balancedness and trainability empirically and serves as a foundation for studying the learning dynamics of positive homogeneous models with attention mechanisms.",
        "methodology": "The methodology centers on a theoretical derivation of a conservation law for GAT gradient flow dynamics, extending the concept of neuron-wise balancedness from traditional deep neural networks. This law describes how the difference between the squared l2-norms of incoming feature weights and attention parameters, and outgoing feature weights, remains approximately constant during training under gradient flow. Based on this insight, the authors devise a balanced initialization scheme. This scheme involves setting attention parameters to zero and scaling feature weights (Wl and Wl+1) layer-wise to ensure that the initial 'degree of balancedness' (c) is zero, thus promoting effective gradient propagation. Furthermore, a 'Balanced Orthogonal Initialization' is introduced, which first initializes feature weights with a looks-linear (LL) mirrored block orthogonal structure before applying the balancing procedure, aimed at achieving dynamical isometry for enhanced trainability, especially in deeper models.",
        "experimental_setup": "The GAT models, often referred to as GATv2 in the paper, were trained using ReLU activation, weight sharing, and no biases (unless specified). Experiments primarily utilized SGD and Adam optimizers, running for up to 5000 epochs (or until training loss fell below 10^-4). Learning rates were set based on network depth and dataset, with fine-tuning for deeper networks in Adam. Model selection was based on the highest validation accuracy, with mean ±95% confidence intervals reported over five runs. Four initialization schemes were compared: standard Xavier (Xav), Xavier with zero attention (XavZ) for ablation, Xavier with balanced parameters (BalX), and Looks-Linear Orthogonal with balanced parameters (BalO). The evaluation involved nine benchmark datasets for semi-supervised node classification, including Cora, Citeseer, Pubmed, Cornell, Texas, Wisconsin, Squirrel, Chameleon, and Actor. Additional architectural variations (ELU activation, multiple attention heads, no weight sharing, dropout, weight decay) and comparisons with Lipschitz normalization were explored. Experiments were conducted on Nvidia T4 Tensor Core GPU (15 GB RAM) or Nvidia GeForce RTX 3060 Laptop GPU (6 GB RAM).",
        "limitations": "The derived conservation law is specifically applicable to the self-attention mechanism found in the original GAT and GATv2 models, and certain architectural variations like ωGAT. It requires modifications to be extended to other forms of self-attention, such as dot-product self-attention used in models like SuperGAT, or other Transformer-based architectures. The theory's assumption of positive homogeneous activation functions means that non-homogeneous functions like ELU can negatively impact the effectiveness of certain balanced initializations (e.g., BalO, although BalX might still perform well). Furthermore, standard regularization techniques like dropout were observed to be less effective for very deep networks (e.g., L=10) regardless of the initialization scheme, indicating limitations in their ability to completely resolve deep network trainability issues.",
        "future_research_directions": "Future research directions include extending the study of learning dynamics to other positive homogeneous models that incorporate attention mechanisms, such as Transformers (especially Vision Transformers that often require greater depth than NLP Transformers). Another promising area is exploring how dynamical isometry can be achieved or approximated in more general Graph Neural Networks (GNNs). The paper also suggests deriving modifications to the conservation law for other attention-based models, specifically those utilizing dot-product self-attention mechanisms (e.g., SuperGAT) and other Transformer architectures adapted for graph learning. Finally, further investigation into the observation that width-overparameterized models aid in training deeper models could provide valuable insights into the role of overparameterization in GNNs."
      }
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of accelerating Graph Neural Network (GNN) training by optimizing neighbor sampling. Existing sampling algorithms for Graph Convolutional Networks (GCNs) are suboptimal and not applicable to more general attentive GNNs because optimal sampling distributions depend on constantly changing neighbor embeddings and learned weights, making direct computation intractable. This work reformulates the sampling variance optimization as an adversary bandit problem, where rewards relate to node embeddings and learned weights. Key contributions include proposing two bandit algorithms (GNN-BS and GNN-BS.M) that asymptotically approach the optimal variance within a factor of 3, applicable to both GCNs and attentive GNNs. Empirically, the proposed methods demonstrate superior convergence rates, higher accuracy (Micro F1 scores), and lower sampling variances compared to state-of-the-art approaches across multiple datasets.",
        "methodology": "The core methodology involves formulating the optimization of sampling variance as an adversary bandit problem. Instead of explicitly calculating the intractable optimal sampling distribution, the approach maintains nonparametric estimates of the sampler and updates it iteratively based on partial knowledge acquired from sampled neighbors. Rewards are defined as the negative derivatives of the effective sampling variance. Two bandit algorithms are proposed: 1) GNN-BS, based on the adversary Multi-Armed Bandit (MAB) setting, where a single neighbor (arm) is chosen and repeated k times, and the sampling distribution is updated using EXP3. 2) GNN-BS.M, based on MAB with multiple plays, which uses an efficient k-combination sampler (DepRound) to select a k-element subset of neighbors once, updating the distribution via EXP3.M. For attentive GNNs, the algorithms are extended by defining adjusted feedback attention values to approximate true attention values from sampled unnormalized attentions.",
        "experimental_setup": "Experiments were conducted on five benchmark datasets: Cora, Pubmed, PPI, Reddit, and Flickr, covering multi-class and multi-label node classification tasks. An additional experiment on the OGB protein dataset was reported. All GNN architectures (GCN, GAT, GeniePath) were configured with two layers. Hidden embedding dimensions were set to 16 for Cora/Pubmed, 256 for PPI/Reddit/Flickr, and 64 for OGB protein. No normalization layers were used for fair comparison, and GAT multi-heads were set to 1. Comparison algorithms included various layer sampling methods (GraphSAGE, FastGCN, LADIES, AS-GCN), variance reduction techniques (S-GCN), and graph sampling techniques (ClusterGCN, GraphSAINT), along with their attentive GNN variants (AS-GAT, GraphSAINT-GAT). Hyperparameters such as learning rate, L2-norm regularizers, and dropout rate were optimized via grid search. Sample sizes (k) for node-wise samplers were 1 for Cora/Pubmed, 5 for Flickr, and 10 for PPI/Reddit. Batch sizes were 256. Performance was evaluated using Micro F1 scores, convergence rates (validation F1 vs. epochs/time), and analysis of sample variances. Each experiment was run multiple times (3 for benchmarks, 10 for OGB) to report mean and standard deviation.",
        "limitations": "The current derivation of bandit samplers primarily follows node-wise sampling approaches, with the extension to layer-wise sampling left for future work. The GNN-BS algorithm, based on repeating a 1-arm selection k times, is acknowledged as not strictly rigorous in the formal MAB setting, unlike the GNN-BS.M with multiple plays. In practice, the implementation maintains a single sampling distribution (qi) updated only from rewards of the first layer, rather than multiple qi's for each layer, which would be more principled. The variance approximation for GNN-BS.M uses Jensen's inequality, indicating it is an upper bound rather than the exact effective variance. Furthermore, some comparative methods like 'graph sampling' approaches are not applicable to scenarios where only partial vertices have labels, a common practical limitation. Memory issues prevented the completion of AS-GAT training on the Reddit dataset.",
        "future_research_directions": "Future research could explore extending the bandit samplers to layer-wise sampling approaches, building upon the current node-wise derivations. Investigating other bandit settings beyond the adversary bandit problem, such as stochastic bandits, is also suggested. A potential refinement for the current approach involves maintaining and updating separate sampling distributions (qi's) for each layer, rather than a single one updated from the first layer, to potentially improve performance or theoretical rigor."
      }
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors",
      "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood\naggregation that assigns weights to neighbors according to their importance. In\npractice, however, GATs are often unable to switch off task-irrelevant\nneighborhood aggregation, as we show experimentally and analytically. To\naddress this challenge, we propose GATE, a GAT extension that holds three major\nadvantages: i) It alleviates over-smoothing by addressing its root cause of\nunnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits\nfrom higher depth as it can still utilize additional layers for (non-)linear\nfeature transformations in case of (nearly) switched-off neighborhood\naggregation. iii) By down-weighting connections to unrelated neighbors, it\noften outperforms GATs on real-world heterophilic datasets. To further validate\nour claims, we construct a synthetic test bed to analyze a model's ability to\nutilize the appropriate amount of neighborhood aggregation, which could be of\nindependent interest.",
      "full_text": "GATE: How to Keep Out Intrusive Neighbors Nimrah Mustafa 1 Rebekka Burkholz 1 Abstract Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neigh- borhood aggregation, as we show experimen- tally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over- smoothing by addressing its root cause of un- necessary neighborhood aggregation. ii) Simi- larly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non- )linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neigh- bors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to an- alyze a model’s ability to utilize the appropri- ate amount of neighborhood aggregation, which could be of independent interest. 1. Introduction Graph neural networks (GNNs) (Gori et al., 2005) are a standard class of models for machine learning on graph- structured data that utilize node feature and graph struc- ture information jointly to achieve strong empirical per- formance, particularly on node classification tasks. In- put graphs to GNNs stem from various domains of real- world systems such as social (Bian et al., 2020), com- mercial (Zhang & Chen, 2020), academic (Hamaguchi et al., 2017), economic (Monken et al., 2021), biochem- ical(Kearnes et al., 2016), physical (Shlomi et al., 2021), and transport (Wu et al., 2019) networks that are diverse in their node feature and graph structure properties. 1CISPA Helmholtz Center for Information Security, 66123 Saarbr¨ucken, Germany. Correspondence to: Nimrah Mustafa <nimrah.mustafa@cispa.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). The message-passing mechanism of GNNs (Kipf & Welling, 2017; Xu et al., 2019) involves two key steps: a transformation of the node features, and the aggregation of these transformed features from a node’s neighborhood to update the node’s representation during training. While this has proven to be largely successful in certain cases, it gen- erally introduces some problems for learning with GNNs, the most notorious of which is over-smoothing (Li et al., 2018). The enforced use of structural information in ad- dition to node features may be detrimental to learning the node classification task, as shown by recent results where state-of-the-art GNNs perform the same as or worse than multi-layer perceptrons (MLPs) (Gomes et al., 2022; Yan et al., 2022; Ma et al., 2022). One such task is where node labels can be easily determined by informative node fea- tures and require no contribution from the neighborhood. Here, standard neighborhood aggregation, as in most GNN architectures, would impair model performance, particu- larly with an increase in model depth. A popular standard GNN architecture that, in principle, tries to resolve this problem is the Graph Attention Net- work (GAT) (Veliˇckovi´c et al., 2018; Brody et al., 2022). By design, neighborhood aggregation in GATs is charac- terized by learnable coefficients intended to assign larger weights to more important neighboring nodes (including the node itself) in order to learn better node representations. Therefore, in the above example, GATs should ideally re- sort to assigning near-zero importance to neighbor nodes, effectively switching off neighborhood aggregation. How- ever, we find that, counter-intuitively, GATs are unable to do this in practice and continue to aggregate the uninforma- tive features in the neighborhood which impedes learning. One may ask why one would employ a GAT (or any GNN architecture) if an MLP suffices. In practice, we do not know whether neighborhood aggregation (of raw features or features transformed by a perceptron or MLP), would be beneficial or not beforehand. This raises a pertinent ques- tion for the GNN research community: How much neigh- borhood aggregation is needed for a given task? . Ideally, it is what we would want a model to learn. Otherwise, the right task-specific architecture would need to be identified by time and resource-intensive manual tuning. We address the challenge faced by GAT to effectively de- 1 arXiv:2406.00418v2  [cs.LG]  29 Jul 2024GATE: How to Keep Out Intrusive Neighbors termine how well a node is represented by its own features compared to the features of neighboring nodes, i.e., dis- tinguish between the relative importance of available node features and graph structure information for a given task. Firstly, we provide an intuitive explanation for the problem based on a conservation law of GAT gradient flow dynam- ics derived by Mustafa & Burkholz (2023). Building on this insight, we present GATE, an extension of the GAT architecture that can switch neighborhood aggregation on and off as necessary. This allows our proposed architecture to gain the following advantages over GAT: 1. It alleviates the notorious over-smoothing problem by addressing the root cause of unnecessarily repeated neighborhood aggregation. 2. It allows the model to benefit from more meaningful representations obtained solely by deeper non-linear transformations, similarly to perceptrons, in layers with little to no neighborhood aggregation. 3. It often outperforms GATs on real-world heterophilic datasets by weighing down unrelated neighbors. 4. It offers interpretable learned self-attention coeffi- cients, at the node level, that are indicative of the rel- ative importance of feature and structure information in the locality of the node. In order to validate these claims, we construct a syn- thetic test bed of two opposite types of learning problems for node classification where label-relevant information is completely present only in a node’s i) own features and ii) neighboring nodes’ features (see Fig. 2). GATE is able to adapt to both cases as necessary. On real-world datasets, GATE performs competitively on homophilic datasets and is substantially better than GAT on heterophilic datasets. Furthermore, up to our knowledge, it achieves a new state of the art on the relatively large OGB-arxiv dataset (Hu et al., 2021) (i.e., 79.57 ± 0.84% test accuracy). In sum- mary, our contributions are as follows: • We identify and experimentally demonstrate a struc- tural limitation of GAT, i.e., its inability to switch off neighborhood aggregation. • We propose GATE, an extension of GAT, that over- comes this limitation and, in doing so, unlocks several benefits of the architecture. • We update an existing conservation law relating the structure of gradients in GAT to GATE. • We construct a synthetic test bed to validate our claims, which could be of independent interest to mea- sure progress in developing adaptive neighborhood aggregation schemes. Figure 1: MLP only performs node feature transforma- tions, whereas GAT also always aggregates over the neigh- borhood. With the ability to switch off neighborhood ag- gregation, GATE can learn to emulate MLP behavior and potentially interleave effective perceptron and standard lay- ers in a flexible manner. This allows for more expressive power that we find to benefit real-world tasks (see Table 3). 2. Related Work To relieve GNNs from the drawbacks of unnecessarily re- peated neighborhood aggregation in deeper models, ini- tial techniques were inspired by classical deep learning of MLPs such as normalization (Cai et al., 2021; Zhao & Akoglu, 2020; Zhou et al., 2020; 2021) and regularization (Papp et al., 2021; Rong et al., 2020; Yang et al., 202; Zou et al., 2019). More recently, the need for deeper models and architectural changes to limit neighborhood aggregation as necessary has been recognized. Some approaches use linear combi- nations of initial features and current layer representation (Gasteiger et al., 2019), add skip connections and identity mapping (Chen et al., 2020; Cong et al., 2021), combine representations of all previous layers at the last layer (Xu et al., 2018), aggregate information from a node-wise de- fined range of k-hop neighbors(Liu et al., 2020), and limit the number of aggregation iterations based on node influ- ence scores (Zhang et al., 2021). However, these architec- tures are not flexible enough to utilize additional network layers to simulate perceptron behavior, which, as we find, benefits heterophilic tasks. (Ma et al., 2023) discuss ‘good’ and ‘bad’ heterophily, which are also task-dependent. Other contemporary works for general GNNs propose the use of bilevel optimization to determine a node’s strategic discrete action to a received message (Finkelshtein et al., 2023) and a variational inference framework for adaptive message passing (Errica et al., 2023). While these non- attentive architectures improve message passing in GNNs, we focus on identifying and explaining a structural limita- tion of self-attention in GAT, that continues to be used as a strong baseline architecture. An orthogonal line of research uses graph structural learn- ing (Yang et al., 2019; Stretcu et al., 2019; Franceschi et al., 2020) to amend the input graph structure such that neighborhood aggregation benefits the given task. Such 2GATE: How to Keep Out Intrusive Neighbors approaches are difficult to scale, more susceptible to over- smoothing, and potentially destroy any inherent informa- tion in the original graph structure. On the contrary, a standard GNN architecture equipped to selectively per- form neighborhood aggregation avoids these pitfalls. Self- supervision of the attention mechanism has also been pro- posed (Wang et al., 2019; Kim & Oh, 2021). Methods such as graph rewiring (Deac et al., 2022) to overcome problems such as over-squashing (Alon & Yahav, 2021) are comple- mentary and may also be combined with GATE. While we focus our insights on GAT, architectures based on GAT such as ωGAT (Eliasof et al., 2023) also suffer from the same problem (see Fig. 14 in Appendix C). This further confirms that the universal problem with GAT has been correctly identified. In general, recent works direct ef- fort to understand the current limitations of graph attention (Lee et al., 2023; Fountoulakis et al., 2023). 3. Architecture Notation Consider a graph G = ( V, E) with node set V and edge set E ⊆ V × V, where for a node v ∈ V the neighborhood is N(v) = {u|(u, v) ∈ E} and input features are h0 v. A GNN layer updates each node’s repre- sentation by aggregating over its neighbors’ representation and combining it with its own features. The aggregation and combination steps can be performed together by in- troducing self-loops in G such that, ∀v ∈ V, (v, v) ∈ E. We assume the presence of self-loops in G unless spec- ified otherwise. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indi- cate the importance of node u for v. A network is con- structed by stacking L layers, defined as follows, using a non-linear activation function ϕ that is homogeneous (i.e ϕ(x) = xϕ′(x)) and consequently, ϕ(ax) = aϕ(x) for positive scalars a) such as ReLU ϕ(x) = max {x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. GAT Given input representations hl−1 v for v ∈ V, a GAT 1 layer l ∈ [L] transforms those to: hl v = ϕ   X u∈N(v) αl uv · Wl shl−1 u  , where (1) αl uv = exp \u0000 el uv \u0001 P u′∈N(v) exp \u0000 el u′v \u0001, and (2) el uv = \u0000 al\u0001⊤ · ϕ \u0000 Wl shl−1 u + Wl thl−1 v \u0001 (3) The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that Ws = Wt. We denote this variant of GAT by GATS. 1Throughout, we refer to GATv2 (Brody et al., 2022) as GAT. GATE In addition, we propose GATE, a GAT variant that flexibly weights the importance of node features and neigh- borhood features. A GATE layer is also defined by Eq. (1) and (2) but modifies euv in Eq. (3) to Eq. (4). el uv = \u0000 1u̸=val s + 1u=val t \u0001⊤ ·ϕ \u0000 Ulhl−1 u + Vlhl−1 v \u0001 . (4) We denote euv in Eq. (3) and (4) as el vv if u = v. For GATE, Wl s in Eq. (1) is denoted as Wl. A weight- sharing variant of GATE, GATE S, is characterized by all feature transformation parameters being shared in a layer (i.e. Wl = Ul = Vl). Note that, then, for a d-dimensional layer, GATE adds onlyd more parameters to GAT. We next present theoretical insights into the reasoning be- hind the inability of GATs to switch off neighborhood ag- gregation, which is rooted in norm constraints imposed by the inherent conservation law for GATs. The gradients of GATE fulfill an updated conservation law (Thoerem 4.3) that enables switching off neighborhood aggregation in a parameter regime with well-trainable attention. 4. Theoretical Insights For simplicity, we limit our discussion here to GATs with weight sharing. We derive similar arguments for GATs without weight sharing in Appendix A.1. The following conservation law was recently derived for GATs to explain trainability issues of standard initialization schemes. Even with improved initializations, we argue that this law lim- its the effective expressiveness of GATs and their ability to switch off neighborhood aggregation when necessary. Theorem 4.1 (Thm. 2.2 by Mustafa & Burkholz (2023)) . The parameters θ of a layer l ∈ [L − 1] in a GAT network and their gradients ∇θL w.r.t. loss L fulfill: ⟨Wl [i,:], ∇Wl [i,:] ⟩ = ⟨Wl+1 [:,i] , ∇Wl+1 [:,i] ⟩ + ⟨al [i], ∇al [i] ⟩. (5) Intuitively, this equality limits the budget for the relative change of parameters and imposes indirectly a norm con- straint on the parameters. Under gradient flow that assumes infinitesimally small learning rates, this law implies that the relationship \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c stays constant during training, where c is defined by the initial norms. Other gradient-based optimizers fulfill this norm balance also approximately. Note that the norms\r\rWl[i, :] \r\r generally do not assume arbitrary values but are determined by the required scale of the output. Deeper models are especially less flexible in varying these norms as deviations could lead to exploding or diminishing out- puts and/or gradients. In consequence, the norms of the attention parameters are also bounded. Furthermore, a pa- rameter becomes harder to change during training when its 3GATE: How to Keep Out Intrusive Neighbors magnitude increases. This can be seen by transforming the law with respect to the relative change of a parameter de- fined as ∆θ = ∇θL/θ for θ ̸= 0 or ∆θ = 0 for θ = 0. nl+1X j=1 Wl ij 2 ∆Wl ij = nl+2X k=1 Wl+1 ki 2 ∆Wl+1 ki + al i 2 ∆al i. (6) The higher the magnitude of an attention parameter (al i)2, the smaller will be the relative change ∆al i and vice versa. This restricts the attention mechanism in the network to a less-trainable regime without converging to a meaningful model. We next explain why large \r\ral\r\r are required to switch off neighborhood aggregation in a layer. Insight 4.2 (Effective expressiveness of GATs). GATs are challenged to switch off neighborhood aggregation during training, as this requires the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. An intuitive derivation of this insight is presented in the appendix. Here, we outline the main argument based on the observation that to make the contribution of neighbor j insignificant relative to node i, we require αij/αii << 1. We use relativeαij/αii instead of αij and αii to cancel out normalization constants and simplify the analysis. Our key observation is that, given an insignificant link (i, j), its relative contribution to its two neighborhoods αij/αii << 1 and αji/αjj << 1 are affected in oppo- site ways by a feature f of the attention parameters a, i.e. if a[f] contributes to reducing αij/αii, it automatically in- creases αji/αjj . However, we require multiple features that contribute to reducing only αij without strengthen- ing αji that may only be possible in a high dimensional space requiring large norms of a. Yet, the norms ∥a∥2 are constrained by the parameter initialization and cannot in- crease arbitrarily due to the derived conservation law. Note that to switch off all neighborhood aggregation, we require αij/αii << 1, ∀ j ∈ N(i), further complicating the task. To address this challenge, we modify the GAT architecture by GATE that learns separate attention parameters for the node and the neighborhood contribution. As its conserva- tion law indicates, it can switch off neighborhood aggrega- tion in the well-trainable parameter regime. Theorem 4.3 (Structure of GATE gradients). The param- eters and gradients of a GATE network w.r.t. to loss L for layer l ∈ [L − 1] are conserved according to the following laws. Given Θ(θ) = ⟨θ, ∇θL⟩, it holds that: Θ(Wl [i,:]) − Θ(al+1 s [i]) − Θ(al+1 t [i]) = Θ(Wl+1 [:,i] ). (7) and, if additional independent matrices Ul and Vl are trainable, it also holds that: Θ(al s[i]) + Θ(al t[i]) = Θ(Ul [i,:]) + Θ(Vl [i,:]). (8) The proof is provided in the appendix. We utilize this the- orem for two purposes. Firstly, it induces an initialization that enables at least the initial trainability of the network. Similarly to GAT (Mustafa & Burkholz, 2023), we initial- ize all a parameters with zero and matrices W with ran- dom orthogonal looks-linear structure in GATE. This also ensures that we have no initial inductive bias or preference for specific neighbor or node features. As an ablation, we also verify that the initialization of the attention parameters in GAT with zero alone can not switch off neighborhood aggregation in GAT (see Fig. 7 in Appendix C). Secondly, the conservation law leads to the insight that a GATE network is more easily capable of switching off neighborhood aggregation or node feature contributions in comparison with GAT. Insight 4.4 (GATE is able to switch off neighborhood ag- gregation.). GATE can flexibly switch off neighborhood aggregation or node features in the well-trainable regime of the attention parameters. This insight follows immediately from the related conser- vation law for GATE that shows that at and as can inter- change the available budget for relative change among each other. Furthermore, the contribution of neighbors and the nodes are controlled separately. We show how the respec- tive switch-off can be achieved with relatively small atten- tion parameter norms that correspond to the well-trainable regime in Appendix A.3. To verify these insights in exper- iments, we next design synthetic data generators that can test the ability of GNNs to take structural infromation into account in a task-appropriate manner. 5. Experiments We validate the ability of GATE to perform the appropri- ate amount of neighborhood aggregation, as relevant for the given task and input graph, on both synthetic and real- world graphs. In order to gauge the amount of neighbor- hood aggregation, we study the distribution of αvv values (over the nodes) at various epochs during training and lay- ers in the network. This serves as a fair proxy since ∀ v ∈ V, αvv = 1 − P u∈N(v),u̸=v αuv. Thus, αvv = 1 implies no neighborhood aggregation (i.e. only hv is used) whereas αvv = 0 implies only neighborhood aggregation (i.e. hv is not used). Figure 2 shows an exemplary con- struction of both these cases. We defer a discussion of the experimental setup to Appendix B. 5.1. Synthetic Test Bed We construct the synthetic test bed as a node classification task for two types of problems: self-sufficient learning and neighbor-dependent learning. In the self-sufficient learning problem, complete label-relevant information is present in 4GATE: How to Keep Out Intrusive Neighbors (a) No neighborhood contribution required. (b) Only neighborhood contribution required. Figure 2: Examples of synthetic input graphs constructed for learning tasks that are (a) self-sufficient and can be bet- ter solved by switching off neighborhood aggregation, i.e. αvv = 1 and (b) neighbor-dependent that benefit from ig- noring the node’s own features, i.e.αvv = 0. In both cases, ∀ v ∈ V, P u∈N(v),u̸=v αuv + αvv = 1. These represent opposite ends of the spectrum whereas real-world tasks of- ten lie in between and require αii ∈ [0, 1]. GATE’s atten- tion mechanism is more flexible than GAT’s in learning the level of neighborhood aggregation required for a task. a node’s own features. On the contrary, in the neighbor- dependent learning problem, label-relevant information is present in the node features of the k-hop neighbors. We discuss both cases in detail, beginning with the simpler one. Learning self-sufficient node labels In order to model this task exactly, we generate an Erd ˝os–R´enyi (ER) graph structure G with N = 1000 nodes and edge probabil- ity p = 0 .01. Node labels yv are assigned uniformly at random from C = [2 , 8] classes. Input node features h0 v are generated as one-hot encoded node labels in both cases, i.e., h0 v = 1yv . Nodes are divided randomly into train/validation/test split with a 2 : 1 : 1 ratio. We also use a real-world graph structure of the Cora dataset. Two cases using this graph structure are tested: i) using the original node labels consisting of7 classes, and ii) randomized labels of 7 classes. Input node features are generated as one-hot encoding of node labels in both cases. The standard splits of Cora are used. Table 1: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of net- work layers, respectively. Original (Orig.) and randomized (Rand.) labels are used for the Cora structure. In all cases, 100% train accuracy is achieved except in ones marked with ∗ and GATE eventually achieves 100% test accuracy a few epochs later except in one marked with ‡. GAT S and GAT models marked with † also eventually achieve 100% test accuracy. GATE S behaves similarly to GATE and achieves 100% train and test accuracy. . S C L Test Acc.@Epoch of Min. Train Loss GATS GAT GATE Cora O,7 1 99.1@215† 97.7@166† 99.0@127 2 93.4@218 94.5@158 99.6@35 5 85.9@92 85.5@72 98.4@36‡ R,7 1 99.4@263 † 99.8@268† 100@104 2 61.7@2088 ∗ 52.8@341∗ 99.9@36 5 35.1@609 32.1@1299 99.9@23 ER(p = 0.01) R,2 1 100@341 † 100@182† 100@1313 2 99.2@100 † 99.2@119† 99.6@79 5 64.0@7778 ∗ 99.6@239 100@45 R,8 1 88.8@9578 ∗ 98.4@3290 99.2@1755 2 90.4@2459 ∗ 94.8@2237 99.6@44 5 23.6@8152 26.0@8121 100@28 As evident in Table 1, GAT is unable to perfectly learn this task whereas GATE easily achieves100% train and test ac- curacy, and often in fewer training epochs. In line with the homophilic nature of Cora, GAT achieves reasonably good accuracy when the original labels of the Cora graph structure are used as neighborhood aggregation is relatively less detrimental, particularly in a single-layer model. Nevertheless, in the same case, GATE generalizes better than GAT with an increase in model depth. This in- dicates that over-smoothing, a major cause of performance degradation with model depth in GNNs, is also alleviated due to reduced neighborhood aggregations (see Fig. 3). On the contrary, random labels pose a real challenge to GAT. Since the neighborhood features are fully uninforma- tive about a node’s label in the randomized case, aggrega- tion over such a neighborhood distorts the fully informa- tive features of the node itself. This impedes the GAT net- work from learning the task, as it is unable to effectively switch off aggregation (see Fig. 3), whereas GATE is able to adapt to the required level of neighborhood aggregation (i.e. none, in this case). Interestingly, note that a single 5GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 3: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 1 and 2 layer models. Due to space limitation, we defer the plots of 5 layer networks to Fig. 8 in Appendix C. layer GAT in the case of random labels can almost, though not completely, switch off neighborhood aggregation (see Fig. 3) and achieve (near) perfect accuracy in the simpler cases. This is in line with our theoretical analysis (see In- sight 4.2), as the norms of a single-layer model are not con- strained and thus the attention parameters have more free- dom to change. Overall, the accuracy of GAT worsens drastically along two dimensions simultaneously: i) an increase in the depth of the model (due to increased unnecessary aggregation), and ii) an increase in the complexity of the task (due to an in- crease in the number of classes in an ER graph and conse- quently in node neighborhoods). In the interest of space, we defer results for GATE S to Fig. 9 in Appendix C as aggregation patterns similar to GATE are observed. Having established that GATE excels GAT in avoiding task-irrelevant neighborhood aggregation, it is also impor- tant to verify whether GATE can perform task-relevant neighborhood aggregation when required, and as much as required. We answer this question next by studying the be- havior of GATE, in comparison to GAT, on a synthetically constructed neighbor-dependent learning problem. Learning neighbor-dependent node labels To model this task, we generate an ER graph structure with N = 1000 nodes and edge probability p = 0 .01. Input node features h0 v ∈ Rd are sampled from a multivariate normal distribution N(0d, Id). For simplicity, d = 2. This input graph G is fed to a random GAT network Mk with k layers of width d. Note that this input graph G has no self-loops on nodes (i.e. v /∈ N(v)).The parameters of Mk are initialized with the standard Xavier (Glorot & Ben- gio, 2010) initialization. Thus, for each node v, the node embedding output by Mk, hMk v is effectively a function f Table 2: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Mean test accuracy ±95% confi- dence interval over 5 runs is reported. In all cases, a GATE variant outperforms the GAT variants. We further analyze one experimental run in detail in Table 8 in Appendix C. k L GATS GAT GATE S GATE 1 1 93 .6±1.3 92 .3±1.3 96.4 ± 0.7 93.5±1.3 2 93 .5±0.7 92 .7±2.7 97.9 ± 0.8 94.6±2.1 3 88 .2±4.9 91 .8±3.4 92 .1±4.6 94.0 ± 1.5 2 2 90 .4±1.3 87 .7±1.6 93.8 ± 0.5 88.7±2.5 3 82 .2±4.5 88 .9±2.1 85 .8±2.5 93.4 ± 3.3 4 84 .0±5.0 83 .0±4.8 89.2 ± 2.3 87.8±2.4 3 3 84 .3±3.2 83 .8±2.7 87 .5±1.8 88.6 ± 2.0 4 71 .4±3.9 75 .9±7.6 89.2 ± 1.0 89.0±0.5 5 80 .2±4.8 83 .9±2.2 86 .1±0.8 87.8 ± 1.6 of the k-hop neighboring nodes of node v represented by a random GAT network. Let Nk(v) denote the set of k-hop neighbors of v and v /∈ Nk(v). Finally, we run K-means clustering on the neighborhood aggregated representation of nodes hMk v to divide nodes into C clusters. For simplicity, we set C = 2. This cluster- ing serves as the node labels (i.e. yv = argc∈[C](v ∈ c) for our node classification task. Thus, the label yv of a node v to be learned is highly dependent on the input features of the neighboring nodes h0 u ∈ Nk(v) rather than the node’s own input features h0 v. The generated input data and the real decision boundary for varying k are shown in Fig. 4. Corresponding results in Table 2 and Fig. 5 exhibit that GATE can better detect the amount of necessary neighborhood aggregation than GAT. However, this task is more challenging than the previous one, and GATE too can not achieve perfect100% test accu- racy. This could be attributed to data points close to the real decision boundary which is not crisply defined (see Fig. 4). 5.2. Real-World Data To demonstrate that the ability of GATE to switch-off neighborhood aggregation has real application relevance, we evaluate GATE on relatively large-scale real-world node classification tasks, namely on five heterophilic benchmark datasets (Platonov et al., 2023) (see Table 3) and three OGB datasets (Hu et al., 2021) (see Table 5). We defer results and discussion on five small-scale datasets with varying homophily levels to Table 7 in Appendix C. To analyze 6GATE: How to Keep Out Intrusive Neighbors (a) k = 1  (b) k = 2  (c) k = 3 Figure 4: (a)-(c): Distribution of node labels of a synthetic dataset, with neighbor-dependent node labels, based on nodes’ own random features (left) and neighbors’ features aggregated k times (right). Figure 5: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 3. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 3, 4, and 5 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 3 layers of the 4 and 5 layer models perform neighborhood aggregation. the potential benefits of combining MLP and GAT layers in GATE, we compare its behavior with GAT and MLP. We argue that the better performance of GATE, by a large margin in most cases, can be attributed to down-weighting unrelated neighbors, leveraging deeper non-linear feature transformations, and reducing over-smoothing. While we focus our exposition on the neighborhood aggre- gation perspective of GATs, we also consider the FAGCN architecture (Bo et al., 2021), which relies on a similar at- tention mechanism and, in theory, could switch off neigh- borhood aggregation when positive and negative contribu- tions of neighbors cancel out. In contrast to GATE, it re- quires tuning a hyperparameter ϵ, which controls the con- tribution of raw node features to each layer. Furthermore, on our synthetic tasks, we find that, like GAT, FAGCN is also unable to limit neighborhood contribution. We also provide a detailed qualitative and quantitative discussion comparing GATE and FAGCN using the synthetic testbed in Appendix C. Next, we analyze GATE’s ability to mix MLP and GAT layers. To this end, we evaluate a GNN architecture con- structed by alternately placing GAT and MLP layers in the network that we denote by MLP +GAT on various het- erophilic tasks. The purpose of this experiment is twofold. Firstly, we observe in Table 3 that MLP+GAT outperforms both GAT and MLP in most cases. This highlights the benefit of only performing non-linear transformations on raw or aggregated neighborhood features without immedi- ate further neighborhood aggregation to learn potentially more complex features. Secondly, we find GATE to outper- form MLP+GAT (see Table 3). This illustrates that rigidly embedding MLP layers in a GNN with arbitrary predefined roles is not ideal as the appropriate degree and placement of neighborhood aggregation is unknown a-priori. In con- trast, GATE offers more flexibility to learn intricate com- binations of GNN layers and nonlinear feature transforma- tions that define more adequate models for a given task, as exemplified in Fig. 6. The distributions ofαvv in Fig. 6 across layers in GATE re- veal information about the relative importance of node fea- ture and graph structure at the node level, which allows us to analyze the question to which degree graph information is helpful for a task. For example, in Fig. 6, we observe that the αvv values are mostly lower in the minesweeper dataset than the roman-empire dataset. This indicates that aggregation, particularly over the input node features and the final layer’s learned representations, is more beneficial compared to the node’s own features for the minesweeper dataset. On the other hand, for roman-empire, the model has a higher preference to utilize features of the node it- self (as most values of αvv approach 1) over features of the neighbors. This aligns with the homophily levels, 0.05 and 0.68, of the roman-empire and minesweeper datasets, re- spectively. A similar analysis for datasets Texas and Actor can be found in Fig. 13 in Appendix C. We also observe in Fig. 6 that when neighborhood aggrega- 7GATE: How to Keep Out Intrusive Neighbors Table 3: We report mean test accuracy ±95% confidence interval for roman-empire and amazon-ratings and AUC-ROC for the other three datasets over the standard 10 splits, following (Platonov et al., 2023). All architectures were run with networks of depth 5 and 10 layers. The better performance for each architecture is shown with the number of network layers used in parentheses. GATE outperforms GAT and other baselines on all datasets, mostly by a significant margin. roman-empire amazon-ratings questions minesweeper tolokers GAT 26.10 ± 1.25 (5) 45 .58 ± 0.41 (10) 57 .72 ± 1.58 (5) 50 .83 ± 0.41 (5) 63 .57 ± 1.03 (10) MLP 65.12 ± 0.25 (5) 43 .26 ± 0.34 (5) 59 .44 ± 0.94 (10) 50 .74 ± 0.56 (5) 62 .67 ± 1.06 (10) MLP+GAT 70.83 ± 0.39 (5) 45 .25 ± 0.17 (10) 59 .12 ± 1.57 (10) 60 .07 ± 1.11 (5) 65 .85 ± 0.64 (10) FAGCN 67.55 ± 0.81 (5) 42 .85 ± 0.83 (10) 60 .38 ± 1.21 (5) 63 .38 ± 0.91 (10) 60 .89 ± 1.12 (5) GATE 75.55 ± 0.30 (5) 45.73 ± 0.24(10) 62.95 ± 0.71 (5) 66.14 ± 1.57 (5) 66.63 ± 1.15(10) (a) roman-empire, GAT:28.96% test accuracy.  (b) roman-empire, GATE: 75.94% test accuracy. (c) Minesweeper, GAT:50.50% test AUROC.  (d) Minesweeper, GATE:67.57% test AUROC. Figure 6: Distribution of αvv against training epoch for a run of 5 layer networks on real-world heterophilic tasks. As ex- pected, GAT is unable to significantly vary neighborhood aggregation whereas GATE exhibits varying aggregation patterns across layers and tasks. These could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods for a given task. We defer the plots of the 10-layer models for tolokers dataset to Fig. 11 in Appendix C. tion takes place, the level of aggregation across all nodes, as indicated by the shape of αvv distribution, varies over network layers. This is expected as different nodes need different levels of aggregation depending on where they are situated in the graph topology. For example, peripheral nodes would require more aggregation than central nodes to obtain a similar amount of information. Therefore, as already observed with purposefully constructed synthetic data, GATE offers a more interpretable model than GAT in a real-world setting. While we focus our evaluation in Table 3 on comparison with the most relevant baselines such as attention-based ar- chitectures, we next present a more extensive comparison with 14 other baseline architectures in Table 4. For the re- sults reported in Table 3, we conduct experiments in a sim- ple setting without additional elements that may impact the performance such as skip connections, normalization, etc., to isolate the effect of the architecture and evaluate solely the impact of GATE’s ability to switch off neighborhood aggregation on real-world data. However, for the results in Table 4, we adopt the original codebase of (Platonov et al., 2023), which utilizes such elements to evaluate the performance of baseline GNNs and architectures specifi- cally designed for heterophilic datasets. We evaluate GATE in the same settings optimized for their experiments. For easy comparison, we replicate their results from Table 4 in (Platonov et al., 2023). We observe in Table 4 that while GATE outperforms GAT (and other baselines) significantly, GATE has comparable performance to GAT-sep, a variant of GAT, despite GATE being more parameter efficient by an order of magnitude. More specifically, GAT-sep and GATE introduce d2 and d additional parameters, respectively, in a layer. By corre- spondingly adapting GATE, we find GATE-sep to achieve the best performance in most cases. Therefore, additional techniques generally employed to boost performance are compatible and complementary to GATE. GATE’s ability to benefit from depth in terms of generaliza- tion is demonstrated on OGB datasets (see Table 5). In par- 8GATE: How to Keep Out Intrusive Neighbors Table 4: An extensive comparison of GATE with baseline GNNs using the experimental setup of (Platonov et al., 2023). Accuracy is reported for roman-empire and amazon-ratings, and ROC AUC is reported for the remaining three datasets. roman-empire amazon-ratings minesweeper tolokers questions GATE 89.51 ± 0.49 52.49 ± 0.46 92.82 ± 0.90 84.62 ± 0.69 78.46 ± 1.17 GAT 80.87 ± 0.30 49.09 ± 0.63 92.01 ± 0.68 83.70 ± 0.47 77.43 ± 1.20 GATE-sep 89.78 ± 0.54 54.51 ± 0.38 94.18 ± 0.43 84.48 ± 0.57 78.20 ± 1.00 GAT-sep 88.75 ± 0.41 52.70 ± 0.62 93.91 ± 0.35 83.78 ± 0.43 76.79 ± 0.71 GT 86.51 ± 0.73 51.17 ± 0.66 91.85 ± 0.76 83.23 ± 0.64 77.95 ± 0.68 GT-sep 87.32 ± 0.39 52.18 ± 0.80 92.29 ± 0.47 82.52 ± 0.92 78.05 ± 0.93 GCN 73.69 ± 0.74 48.70 ± 0.63 89.75 ± 0.52 83.64 ± 0.67 76.09 ± 1.27 SAGE 85.74 ± 0.67 53.63 ± 0.39 93.51 ± 0.57 82.43 ± 0.44 76.44 ± 0.62 H2GCN 60.11 ± 0.52 36.47 ± 0.23 89.71 ± 0.31 73.35 ± 1.01 63.59 ± 1.46 CPGNN 63.96 ± 0.62 39.79 ± 0.77 52.03 ± 5.46 73.36 ± 1.01 65.96 ± 1.95 GPR-GNN 64.85 ± 0.27 44.88 ± 0.34 86.24 ± 0.61 72.94 ± 0.97 55.48 ± 0.91 FSGNN 79.92 ± 0.56 52.74 ± 0.83 90.08 ± 0.70 82.76 ± 0.61 78.86 ± 0.92 GloGNN 59.63 ± 0.69 36.89 ± 0.14 51.08 ± 1.23 73.39 ± 1.17 65.74 ± 1.19 FAGCN 65.22 ± 0.56 44.12 ± 0.30 88.17 ± 0.73 77.75 ± 1.05 77.24 ± 1.26 GBK-GNN 74.57 ± 0.47 45.98 ± 0.71 90.85 ± 0.58 81.01 ± 0.67 74.47 ± 0.86 JacobiConv 71.14 ± 0.42 43.55 ± 0.48 89.66 ± 0.40 68.66 ± 0.65 73.88 ± 1.16 ResNet 65.88 ± 0.38 45.90 ± 0.52 50.89 ± 1.39 72.95 ± 1.06 70.34 ± 0.76 ResNet+SGC 73.90 ± 0.51 50.66 ± 0.48 70.88 ± 0.90 80.70 ± 0.97 75.81 ± 0.96 ResNet+adj 52.25 ± 0.40 51.83 ± 0.57 50.42 ± 0.83 78.78 ± 1.11 75.77 ± 1.24 Table 5: Mean test accuracy ±95% confidence interval (and number of network layers). We replicate the results for GAT reported by (Brody et al., 2022). GATE leverages deeper networks to substantially outperform GAT. OGB- GAT GATE arxiv 71.87 ± 0.16 (3) 79.57 ± 0.84 (12) products 80.63 ± 0.46 (3) 86.24 ± 1.01 (8) mag 32.61 ± 0.29 (2) 35.29 ± .36 (5) ticular, GATE improves the SOTA test accuracy (78.03%) on the arxiv dataset achieved by a model using embed- dings learned by a language model instead of raw node features(Duan et al., 2023), as reported on the OGB leader- board. While the better performance of deeper models with limited neighborhood aggregation in certain layers indi- cates reduced over-smoothing, we also verify this insight quantitatively (see Table 9 in Appendix C). Our experimental code is available at https:// github.com/RelationalML/GATE.git. 6. Conclusion We experimentally illustrate a structural limitation of GAT that disables the architecture, in practice, to switch off task- irrelevant neighborhood aggregation. This obstructs GAT from achieving its intended potential. Based on insights from an existing conservation law of gradient flow dynam- ics in GAT, we have explained the source of this problem. To verify that we have identified the correct issue, we re- solve it with a modification of GAT, which we call GATE, and derive the corresponding modified conservation law. GATE holds multiple advantages over GAT, as it can lever- age the benefits of depth as in MLPs, offer interpretable, learned self-attention coefficients, and adapt the model to the necessary degree of neighborhood aggregation for a given task. We verify this on multiple synthetic and real- world tasks, where GATE significantly outperforms GAT and also achieves a new SOTA test accuracy on the OGB- arxiv dataset. Therefore, we argue that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. 9GATE: How to Keep Out Intrusive Neighbors Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. Bian, T., Xiao, X., Xu, T., Zhao, P., Huang, W., Rong, Y ., and Huang, J. Rumor detection on social media with bi- directional graph convolutional networks. In AAAI Con- ference on Artificial Intelligence, 2020. Bo, D., Wang, X., Shi, C., and Shen, H. Beyond low- frequency information in graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. Burkholz, R. and Dubatovka, A. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. In Graph Representation Learning Workshop, International Conference on Machine Learn- ing, 2020. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y ., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Simple and deep graph convolutional networks. InInternational Conference on Machine Learning, 2020. Cong, W., Ramezani, M., and Mahdavi, M. On provable benefits of depth in training graph convolutional net- works. In Advances in Neural Information Processing Systems, 2021. Deac, A., Lackenby, M., and Veliˇckovi´c, P. Expander graph propagation. In Learning on Graphs Conference, 2022. Duan, K., Liu, Q., Chua, T.-S., Yan, S., Ooi, W. T., Xie, Q., and He, J. Simteg: A frustratingly simple approach improves textual graph learning, 2023. Eliasof, M., Ruthotto, L., and Treister, E. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general framework to mitigate oversmoothing, over- squashing, and underreaching, 2023. Finkelshtein, B., Huang, X., Bronstein, M., and ˙Ismail ˙Ilkan Ceylan. Cooperative graph neural networks, 2023. Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. In Journal of Machine Learning Research, 2023. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learn- ing discrete structures for graph neural networks. In In- ternational Conference on Machine Learning, 2020. Gasteiger, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2019. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 9, pp. 249–256, May 2010. Gomes, D., Ruelens, F., Efthymiadis, K., Nowe, A., and Vrancx, P. When are graph neural networks better than structure agnostic methods? In Neural Information Pro- cessing Systems Workshop ICBINB, 2022. Gori, M., Monfardini, G., and Scarselli, F. A new model for learnig in graph domains. InIEEE International Joint Conference on Neural Networks, 2005. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach. In International Joint Conference on Artificial Intelligence, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021. 10GATE: How to Keep Out Intrusive Neighbors Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. Kim, D. and Oh, A. How to find your friendly neighbor- hood: Graph attention design with self-supervision. In International Conference on Learning Representations , 2021. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. InInternational Conference on Learning Representations, 2017. Lee, S. Y ., Bu, F., Yoo, J., and Shin, K. Towards deep attention in graph neural networks: Problems and reme- dies. In International Conference on Machine Learning, 2023. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Liu, M., Gao, H., and Ji, S. Towards deeper graph neu- ral networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2022. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2023. Monken, A., Haberkorn, F., Gopinatha, M., Freeman, L., and Batarseh, F. A. Graph neural networks for modeling causality in international trade. In AAAI Conference on Artificial Intelligence, 2021. Mustafa, N. and Burkholz, R. Are GATS out of balance? In Advances in Neural Information Processing Systems, 2023. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. In Advances in Neural Infor- mation Processing Systems, 2021. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations , 2020. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evalua- tion of gnns under heterophily: are we really making progress? In International Conference on Learning Rep- resentations, 2023. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node clas- sification. In International Conference on Learning Rep- resentations, 2020. Shlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. 2021. Stretcu, O., Viswanathan, K., Movshovitz-Attias, D., Pla- tanios, E., Ravi, S., and Tomkins, A. Graph agreement models for semi-supervised learning. In Advances in Neural Information Processing Systems, 2019. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations , 2018. Wang, G., Ying, R., Huang, J., and Leskovec, J. Improving graph attention networks with large margin-based con- straints. In Graph Representation Learning Workshop, Neural Information Processing Systems, 2019. Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Yan, Y ., Hashemi, M., Swersky, K., Yang, Y ., and Koutra, D. Two sides of the same coin: Heterophily and over- smoothing in graph convolutional neural networks. In IEEE International Conference on Data Mining, 2022. Yang, H., Ma, K., and Cheng, J. Rethinking graph regular- ization for graph neural networks. InAdvances in Neural Information Processing Systems, 202. Yang, L., Kang, Z., Cao, X., Jin, D., Yang, B., and Guo, Y . Topology optimization based graph convolutional net- work. In International Joint Conference on Artificial In- telligence, 2019. Zhang, M. and Chen, Y . Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. Zhang, W., Yang, M., Sheng, Z., Li, Y ., Ouyang, W., Tao, Y ., Yang, Z., and Cui, B. Node dependent local smooth- ing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. 11GATE: How to Keep Out Intrusive Neighbors Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth- ing in gnns. In International Conference on Learning Representations, 2020. Zhou, K., Huang, X., Li, Y ., Zha, D., Chen, R., and Hu, X. Towards deeper graph neural networks with differen- tiable group normalization. In Advances in Neural Infor- mation Processing Systems, 2020. Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in graph convolutional networks. InConfer- ence on Information and Knowledge Management, 2021. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 12GATE: How to Keep Out Intrusive Neighbors A. Theoretical Derivations A.1. Derivation of Insight 4.2 Statement (Restated Insight 4.2) . GATs are challenged to switch off neighborhood aggregation during training, as this would require the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. We have to distinguish GATs with and without weight sharing in our analysis. GATs with weight sharing: To investigate the ability of a GAT to switch off neighborhood aggregation, let us focus on a link(i, j) that should neither contribute to the feature transformation of i nor j. This implies that we need to find attention parameters a (and potentially feature transformations W) so that αij/αii << 1 with αij/αii = exp ( eij − eii). This implies that we require eij − eii << 0 and thus aT ϕ (W (hi + hj)) − 2aT ϕ (W (hi)) << 0. Since we also require αij/αjj << 1, it follows from adding both inequalities that aT [ϕ (W (hi + hj)) − (ϕ (Whi) + ϕ (Whj))] << 0. This inequality can only be fulfilled if there exists at least one feature f for which ∆fij ; = a[f] [ϕ (W[f, :] (hi + hj)) − (ϕ (W[f, :]hi) + ϕ (W[f, :]hi))] fulfills ∆fij << 0. Yet, note that if both ϕ (W[f, :]hi) and ϕ (W[f, :]hj) are positive or both are negative, we just get ∆fij = 0 because of the definition of a LeakyReLU. Thus, there must exist at least one feature f so that without loss of generality ϕ (W[f, :]hi) < 0 and ϕ (W[f, :]hj) > 0. It follows that if a[f] > 0 that 0 > a[f]ϕ (W[f, :]hi) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hj)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hj)) also receives a negative contribution that makes αij/αjj smaller. Yet, what happens to αij/αii? By distinguishing two cases, namely W[f, :] (hi + hj) > 0 or W[f, :] (hi + hj) < 0 and computing a[f] [ϕ (W (hi + hj)) − 2ϕ (W[f, :]hj)] > 0 we find the feature contribution to be positive. If a[f] < 0, then 0 > a[f]ϕ (W[f, :]hj) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hi)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hi)) and αij/αjj is reduced. Similarly, we can derive that at the same time αij/αii is increased, however. This implies that any feature that contributes to reducing ∆fij automatically increases one feature while it increases another. We therefore need multiple featuresf to contribute to reducing either αij/αii or αij/αjj to compensate for other increases. This implies, in order to switch off neighborhood aggregation, we would need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Yet, all these norms are constrained by the derived conservation law and therefore prevent learning a representation that switches off full neighborhoods. GATs without weight sharing: The flow of argumentation without weight sharing is very similar to the one above with weight sharing. Yet, we have to distinguish more cases. 13GATE: How to Keep Out Intrusive Neighbors Similarly to before, we require αij/αjj << 1 and αji/αii << 1. It follows from adding both related inequalities that aT [ϕ (Wshi + Wthj) + ϕ (Wshj + Wthi) − ϕ ((Ws + Wt) hi) − ϕ ((Ws + Wt) hj)] << 0. This implies that for at least one feature f, we require a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ ((Ws[f, :] + Wt[f, :]) hi) − ϕ ((Ws[f, :] + Wt[f, :]) hj)] << 0. (9) Again, our goal is to show that this feature automatically decreases the contribution of one feature while it increases another. As argued above, switching off neighborhood aggregation would therefore need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Our derived norm constraints, however, prevent learning such a model representation. Concretely, without loss of generality, we therefore have to show that if a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 < 0, (10) at the same time, we receive a[f][ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 > 0, (11) (or vice versa). In principle, we have to show this for 16 different cases of pre-activation sign configurations for the four terms in Eq. (9). Yet, since the argument is symmetric with respect to exchanging i and j, only 8 different cases remain. Two trivial cases are identical signs for all four terms. These are excluded, as the left hand side (LHS) of Eq. (9) would become zero and thus not contribute to our goal to switch off neighborhood aggregation. In the following, we will discuss the remaining six cases. Please note that for the remainder of this derivation α >0 denotes the slope of the leakyReLU and not the attention weights αij. 1. Case (+ − ++): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj > 0. From this assumption and the fact that ϕ is a leakyReLU it follows that the LHS of Eq. (9) be- comes: a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 ] = a[f](α −1)[Ws[f, :]hj + Wt[f, :]hi]. Since α −1 < 0 and [Ws[f, :]hj + Wt[f, : ]hi] < 0 according to our assumption, Eq. (9) demands a[f] < 0. To switch off neighborhood aggregation, we would need to be able to make the LHS of Eq. (10) and Eq. (11) Eq. (11) negative. Yet, a negative a[f] leads to a positive LHS of Eq. (11). Thus, the assumed sign configuration cannot support switching off neighborhood aggregation. 2. Case (+ − −−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hi + Wt[f, :]hj], which demands a[f] < 0. Accordingly, the LHS of Eq. (10) is clearly negative, while the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that implyWs[f, :]hj < Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj < 0 and Ws[f, :]hi + Wt[f, :]hj > 0. Again, this result implies that the considered sign configuration does not support switching off neighborhood aggregation. 3. Case (+ + + −): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi > 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes positive, which hampers switching-off neighborhood aggregation as discussed. 14GATE: How to Keep Out Intrusive Neighbors 4. Case (− − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj < 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. The LHS of Eq. (9) becomes a[f](α −1)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes clearly negative. However, the LHS of Eq. (11) is positive, asa[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that imply Ws[f, :]hj > Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj > 0 and Ws[f, :]hi + Wt[f, :]hj < 0. Again, this analysis implies that the considered sign configuration does not support switching off neighborhood aggregation. 5. Case (+ − +−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. According to our assumptions the LHS of Eq. (10) can only be negative if a[f] < 0. Yet, the LHS of Eq. (11) can only be negative if a[f] > 0. Thus, this case clearly cannot contribute to switching off neighborhood aggregation. 6. Case (+ − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. Eq. (9) becomes a[f](1 − α)Ws[f, :] (hi − hj) < 0. At the same time, the LHS of Eq. (10) simplifies to a[f]Ws[f, : ](hi − hj) and the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. Hence, a negative Eq. (9) leads to a positive Eq. (11). Accordingly, the last possible sign configuration also does not support switching off neighborhood aggregation, which concludes our derivation. A.2. Proof of Theorem 4.3 Statement (Restated Theorem 4.3). The gradients and parameters of GATE for layer l ∈ [L − 1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al+1 s [i], ∇al+1 s [i]L⟩ + ⟨al+1 t [i], ∇al+1 t [i]L⟩. (12) and, if additional independent matrices Ul and Vl are trainable, it also holds ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩ = ⟨Ul[i, :], ∇Ul[i,:]L⟩ + ⟨V l[i, :], ∇V l[i,:]L⟩. (13) The proof is analogous to the derivation of Theorem 2.2 by (Mustafa & Burkholz, 2023) that is restated in this work as Theorem 4.1. For ease, we replicate their notation and definitions here. Statement (Rescale invariance: Def 5.1 by Mustafa & Burkholz (2023)). The loss L(θ) is rescale-invariant with respect to disjoint subsets of the parametersθ1 and θ2 if for everyλ >0 we haveL(θ) = L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). Statement (Gradient structure due to rescale invariance Lemma 5.2 in (Mustafa & Burkholz, 2023)). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1L⟩ − ⟨θ2, ∇θ2L⟩ = 0. (14) We first consider the simpler case of GATES, i.e. W = U = V Theorem A.1 (Structure of GATES gradients). The gradients and parameters of GATES for layer l ∈ [L−1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩. (15) Following a similar strategy to (Mustafa & Burkholz, 2023), we identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Given the following definition of disjoint subsets θ1 and θ2 of the parameter set θ, associated with neuron i in layer l, 15GATE: How to Keep Out Intrusive Neighbors θ1 = {x|x ∈ Wl[i, :]} θ2 = {w|w ∈ Wl+1[:, i]} ∪ {al s[i]} ∪ {al t[i]} We show that the loss of GATES remains invariant for any λ >0. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. The scaled network parameters are denoted with a tilde as ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i], and ˜Wl[i, j] = λWl[i, j], and the corresponding networks components scaled as a result are denoted by ˜hl u[i], ˜hl+1 v [k], and ˜αl uv. We show that the parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] = hl+1 v [k]. Also recall Eq. (4) for W = U = V as: el uv = ((1 − quv)al s + (quv)al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) where quv = 1 if u = v and quv = 0 if u ̸= v. For simplicity, we rewrite this as: el uv,u̸=v = (al s)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (16) el uv,u=v = (al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (17) We show that ˜αl uv = exp(˜el uv)P u′∈N(v) exp(˜eluv) = αl uv , because (18) ˜el uv,u̸=v = el uv,u̸=v , and ˜el uv,u=v = el uv,u=v (19) which follows from the positive homogeneity of ϕ that allows ˜el uv,u=v = λ−1al s[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (20) = λ−1λal s[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (21) = el uv,u̸=v. (22) and similarly, 16GATE: How to Keep Out Intrusive Neighbors ˜el uv,u=v = λ−1al t[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (23) = λ−1λal t[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (24) = el uv,u=v. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] = ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i]. In the next layer, we therefore have ˜hl+1 v [k] = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k]. Thus, the output node representations of the network remain unchanged, and the loss L is rescale-invariant. Next consider the case that Wl, Ul, and V l are independent matrices. Similarly to the previous reasoning, we see that if we scale ˜Wl[i, :] = Wl[i, :]λ, then also scaling ˜Wl+1[:, i] = Wl+1[:, i]λ−1 and ˜al+1 s [i] = al+1 s [i]λ−1 and ˜al+1 t [i] = al+1 t [i]λ−1 will keep the GATE layer unaltered. In this case, we obtain an additional rescaling relationship between al s, al t and Ul, V l. A rescaling of the form ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i] could be compensated by ˜Ul[i, :] = Ul[i, :]λ and ˜V l[i, :] = V l[i, :]λ. It follows immediately that ˜euv = euv. A.3. Derivation of Insight 4.4 Following the analysis in A.1, in contrast to GAT, αij/αii << 1 can be easily realized in GATE with as[f] < 0 and at[f] > 0 for all or only a subset of the features. Note that for the non-weight-sharing case, U and V in GATE would simply correspond to Ws and Wt, respectively, in GATE and the same line of reasoning holds. Large norms are usually not required to create a notable difference in size between eii and eij. 17GATE: How to Keep Out Intrusive Neighbors B. Experimental Settings Our complete experimental setup is described as follows. Non-linearity For GATS and GAT networks, we substitute ϕ in Eq. (3) with LeakyReLU as defined in the standard architecture. For GATE, we substitute ϕ in Eq. (4) with ReLU in order to be able to interpret the sign of as and at parameters as contributing positively or negatively to neighborhood aggregation. MLP, MLP +GAT , and FAGCN also all use ReLU after every hidden layer. Network Width We vary the depth of GAT and GATE networks in all our experiments as specified. For synthetic datasets, the network width is fixed to 64 in all cases. For OGB datasets, we use the hidden dimensions used by (Brody et al., 2022). For the remaining datasets, the network width is also fixed to 64. Initialization The feature transformation parameter matrices, i.e.,W, U, and V are initialized randomly with an orthog- onal looks-linear structure (Burkholz & Dubatovka, 2019) for MLP, MLP+GAT , GAT(S) and GATE(S). The parameters a in GAT(S) use Xavier initialization (Glorot & Bengio, 2010), as is the standard. In GATE(S), as and at are initialized to 0 to initially give equal weights to the features of a node itself and its neighboring nodes. Optimization Synthetic, OGB, and remaining real-world tasks are run for a maximum of 10000, 2000, 5000 epochs, respectively, using the Adam optimizer. To isolate the effect of the architecture and study the parameter dynamics during training as best as possible, we do not use any additional elements such as weight decay and dropout regularization. We also do not perform any hyperparameter optimization. However, the learning rate is adjusted for different real-world datasets to enable stable training of models as specified in Table 6. Nevertheless, for a fair comparison, the same learning rate is used for a given problem across all architectures. For all synthetic data, a learning rate of 0.005 is used. Real-world datasets use their standard train/test/validation splits, i.e. those provided by Pytorch Geometric for Planetoid datasets Cora and Citeseer, by OGB framework for OGB datasets, and by (Platonov et al., 2023) for all remaining real-world datasets. Code Our experimental code and synthetic data generators are available at https://github.com/ RelationalML/GATE.git. Table 6: Details of real-world datasets used in experiments. dataset # nodes # edges # features # classes learning rate used for L layer networks ogb-arxiv 169, 343 2 , 315, 598 128 40 L = [12] : 0.001 ogb-products 2, 449, 029 123 , 718, 152 100 47 L = [8] : 0.001 ogb-mag 736, 389 10 , 792, 672 128 349 L = [5] : 0.005 roman-empire 22, 662 32 , 927 300 18 L = [5] : 0.001, L= [10] : 0.0005 amazon-ratings 24, 492 93 , 050 300 5 L = [5] : 0.001, L= [10] : 0.0005 questions 48, 921 153 , 540 301 2 L = [5] : 0.001, L= [10] : 0.0005 minesweeper 10, 000 39 , 402 7 2 L = [5] : 0.001, L= [10] : 0.0005 tolokers 11, 758 519 , 000 10 2 L = [5] : 0.001, L= [10] : 0.0005 cora 2, 708 10 , 556 1 , 433 7 L = [2, 5] : 0.005, L= [10, 20] : 0.0005 citeseer 3, 327 9 , 104 3 , 703 6 L = [2, 5] : 0.001, L= [10, 20] : 0.0001 actor 7, 600 26 , 659 932 5 L = [2, 5, 10, 20] : 0.005 texas 183 279 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.0005 wisconsin 251 450 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.005 18GATE: How to Keep Out Intrusive Neighbors C. Additional Results Smaller Real-World Datasets We evaluate GAT and GATE on five small-scale real-world datasets with varying ho- mophily levels β as defined in (Pei et al., 2020) and report results in Table 7. Higher values ofβ indicate higher homophily, i.e. similar nodes (with the same label) tend to be connected. We note that a 2-layer network of a baseline method for het- erophilic datasets, Geom-GCN (Pei et al., 2020), attains test accuracy (%) of64.1, 67.6, and 31.6 for Wisconsin, Texas, and Actor datasets, respectively, which is in line with that achieved by GATE. Except for Citeseer, the best overall performance for each dataset is achieved on a shallow model. This is not surprising as these datasets are small-scale and potentially prone to over-fitting in large models, particularly since we do not use any skip connections or regularization to retain model performance. Furthermore, the three heterophilic datasets have been recently shown to be problematic (Platonov et al., 2023). Therefore, a better evaluation of GATE is on relatively large-scale OGB datasets (Hu et al., 2021) and more recent heterophilic datasets (Platonov et al., 2023) that can exploit the flexibility of GATE. Although GATE is more pa- rameterized than GAT, it usually requires fewer training epochs and generalizes better, in addition to other advantages over GAT as discussed in the paper. Table 7: Test accuracy (%) of GAT and GATE models for network depthL on small-scale real-world datasets with varying homophily levels β. Entries marked with * indicate models that achieve 100% training accuracy and stable test accuracy. Otherwise, test accuracy at max. validation accuracy is reported. Data β L = 2 L = 5 L = 10 L = 20 GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE Wisc. 0.21 62 .7* 80.4 70.5* 51.0 70.5 60.7* 45.1 62.7 58.8 47 .1 62.7 60.7 Texas 0.11 56 .7* 67.6* 67.6* 51.4 67.6* 67.6* 56.7* 62.2* 62.3* 59.4* 62.1* 64.9 Actor 0.24 27 .1 32.2 31.6 25 .4 27 .5 29.2 25.3 27 .4 27.9 24.5 24 .6 29.4 Cora 0.83 80 .0 81.0* 80.8 79 .8 80.8* 80.4 77 .6 80.0* 79.2 77 .7 77 .2* 79.0 Cite. 0.71 68 .0 67 .6* 68.3 67.2 68.7* 67.8 66 .9 67.6* 67.6 68.2 67 .1* 69.2 Initialization of attention parameters in GAT We show in Fig. 7 that setting the initial value of attention parameters as and at in GATE to zero is, in fact, not what enables neighborhood aggregation but rather the separation ofa into as and at as discussed in Insight 4.4. Figure 7: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels for GAT with attention parameters a initialized to zero. Left to right:1, 2 and 5 layer models that achieve test accuracy of100%, 52.7%, and 36.2%, respectively, which is similar to the results obtained by standard Xavier initialization of attention parameters in GAT. Further analysis of experiments We present the analysis of α coefficients learned for some experiments in the main paper that were deferred to the appendix due to space limitations. 19GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 8: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 5 layer models. (a) original labels (b) Random labels Figure 9: Distribution of αvv against training epoch for the self-sufficient learning problem using Cora graph structure with original (top) and random (bottom) node labels and input node features as a one-hot encoding of labels. Left to right: 1, 2, and 5 layer GATES models that all 100% test accuracy except in the case of 5 layer model using original labels. In this case, although a training accuracy if 100% is achieved at 32 epochs with test accuracy 97.3%, a maximum test accuracy of 98.4% is reached at 7257 epochs. Training the model to run to 15000 epochs only increases it to 98.4%. An increased learning rate did not improve this case. However, we also run the GAT model for 15000 epochs for this case, and it achieves 85.9% test accuracy at epoch 47 where the model achieves 100% accuracy and only achieves a maximum test accuracy of 89.3% briefly at epoch 8. 20GATE: How to Keep Out Intrusive Neighbors Figure 10: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 1. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 1, 2, and 3 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 1 layer of the 2 and 3 layer models perform neighborhood aggregation. Table 8: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Entries marked with * identify models where 100% train accuracy is not achieved. Underlined entries identify the model with the highest train accuracy at the epoch of max. test accuracy. This provides an insight into how similar the function represented by the trained model is to the function used to generate node labels, i.e. whether the model is simply overfitting to the train data or really learning the task. Higher training and test accuracy simultaneously indicate better learning. In this regard, the difference in train accuracy at max. test accuracy between GATE and GATS or GAT is only 0.4, 1.0 and 0.6 for the settings (k = 1, L= 3), (k = 2, L= 4) and (k = 3, L= 3), respectively. k L Test Acc. @ Epoch of Max. Train Acc. Max Test Acc. @ Epoch GATS GAT GATE GAT S GAT GATE 1 1 92.0@2082* 91.2@6830* 93.2@3712* 93.2@1421 92.0@9564 93.6@3511 2 89.6@8524* 88.0@8935 91.2@942 91.6@5188 92.8@4198 95.6@111 3 86.4@9180* 88.8@997 92.8@618 91.2@6994 92.8@437 97.2@82 2 2 88.8@6736* 89.6@3907 88.8@467 93.2@151 93.2@95 92.0@105 3 82.0@7612 89.2@1950 91.6@370 91.6@1108 93.2@856 95.2@189 4 84.8@4898 82.4@739 87.2@639 88.0@1744 88.4@423 90.4@447 3 3 80.8@8670 80.4@737 85.2@391 86.4@1578 88.8@285 92.0@47 4 78.0@3012 80.4@767 89.6@480 86.8@1762 85.6@469 91.6@139 5 80.0@6611 74.4@1701 86.0@447 85.6@921 83.6@1098 91.2@243 21GATE: How to Keep Out Intrusive Neighbors (a) Tolokers, GAT:61.6% test AUROC.  (b) Tolokers, GATE:69.2% test AUROC. Figure 11: Distribution of αvv against training epoch for one run of 10 layer networks on real-world heterophilic task. Figure 12: Distribution of αvv against training epoch of 2-layer (left) and 5-layer (right) GAT networks for heterophilic datasets Texas (top) and Actor (bottom) 2-layer modes. Despite having connections to unrelated neighbors, GAT is unable to switch off neighborhood aggregation. Figure 13: Distribution of αvv, against training epoch of 2-layer (left) and 5-layer (right) GATE networks for heterophilic datasets Texas (top) and Actor (bottom), across layers could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods. For instance, in the case of Texas, GATE carries out little to no neighborhood aggregation in the first layer over input node features. Instead, aggregation is mainly done over node features transformed in earlier layers that effectuate non-linear feature learning as in perceptrons. However, in the case of Actor, GATE prefers most of the neighborhood aggregation to occur over the input node features, indicating that they are more informative for the task at hand. 22GATE: How to Keep Out Intrusive Neighbors Over-smoothing analysis In the main paper, we have already established the superior performance of GATE, compared to GAT, on several tasks. Intuitively, this can partially be attributed to reduced over-smoothing as its root cause, unnecessary neighborhood aggregation, is alleviated. Here, we verify this insight quantitatively. A widely accepted measure of over-smoothing is the Dirichlet energy (DE) (Cai & Wang, 2020). However, Eliasof et al. (2023) propose a modification of DE to measure GAT energy EGAT , that we use to evaluate over-smoothing in our experiments (see Table 9). We note that the notion of ‘over-smoothing’ is itself task-dependent. It is difficult to determine the optimal degree of smoothing for a task and the threshold that determines ‘over’-smoothing. This merits an in-depth analysis and curation of task-dependent smoothness measures that are not our focus. To show that GATE reduced over- smoothing relative to GAT, it suffices that a decrease in smoothing and an increase in accuracy occur simultaneously. Table 9: The measures Einput, EGAT and EGATE denote the smoothness of input node features, node features at the last layer L of the trained GAT and GATE models, respectively. Two cases are considered: All node pairs and only adjacent node pairs to measure smoothing at the global graph and local node level. Higher values indicate less smoothing. Node representations learned by GATE achieve higher test accuracy on all these tasks, as reported in the main paper, and are simultaneously less smooth than GAT in most cases, indicating that GATE potentially alleviates over-smoothing in GATs. (a) Synthetic self-sufficient task: varying graph structure and label distribution, node features as one-hot encoding of labels, L = 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE Cora, Original Labels 6.016 e + 06 1 .281 e + 08 2.037 e + 09 2.006 e + 03 7 .903 e + 03 1.971 e + 05 Cora, Random Labels 6.283 e + 06 3 .472 e + 09 3.747 e + 09 9.080 e + 03 9 .306 e + 05 3.966 e + 06 ER(p = .01), 2 Classes 4.994 e + 05 2 .701 e + 06 4.478 e + 07 5.042 e + 03 2 .272 e + 04 3.229 e + 05 ER(p = .01), 8 Classes 8.745 e + 05 3 .350 e + 07 2.615 e + 08 8.694 e + 03 1 .762 e + 05 1.960 e + 06 (b) Synthetic neighbor-dependent task: graph structure, node features, generator parameter k, and label distribution as in Section 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE k = 1, L= 3 1 .953 e + 06 5.306 e + 07 5.095 e + 07 1 .957 e + 05 4.234 e + 05 3.610 e + 05 k = 2, L= 4 1 .975 e + 06 1 .193 e + 07 2.198 e + 07 2.012 e + 04 1 .016 e + 05 1.939 e + 05 k = 3, L= 5 1 .951 e + 06 1 .645 e + 07 1.053 e + 08 1.966 e + 04 1 .408 e + 05 9.096 e + 05 (c) Real-world tasks. Dataset L All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE roman-empire 5 1 .274 e + 09 1 .002 e + 11 7.491 e + 11 7.878 e + 04 2 .441 e + 06 4.009 e + 07 amazon-ratings 10 3 .844 e + 08 1 .187 e + 10 2.272 e + 10 4.933 e + 04 3 .848 e + 05 7.430 e + 05 minesweeper 5 6 .869 e + 07 1 .386 e + 09 2.531 e + 10 2.628 e + 04 1 .946 e + 05 7.017 e + 06 tolokers 10 1 .391 e + 08 1 .044 e + 11 1.042 e + 11 3.423 e + 05 1 .249 e + 08 1.397 e + 08 cora 10 5 .088 e + 05 1 .437 e + 07 2.783 e + 08 6.490 e + 02 5 .959 e + 02 1.226 e + 04 citeseer 10 3 .463 e + 05 2 .916 e + 05 1.126 e + 07 2.360 e + 02 2 .426 e + 00 1.030 e + 02 texas 10 1 .945 e + 06 3 .280 e + 04 3.877 e + 04 1.758 e + 04 8 .487 e + 01 9.695 e + 01 actor 10 2 .612 e + 08 1.800 e + 07 1.364 e + 07 1 .237 e + 05 2.810 e + 03 2.215 e + 03 wisconsin 10 4 .438 e + 06 1 .057 e + 07 1.008 e + 08 3.299 e + 04 3 .765 e + 04 7.363 e + 08 23GATE: How to Keep Out Intrusive Neighbors Comparison with other GNNs Other GNN architectures could potentially switch off neighborhood aggregation, as we show here. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatena- tion operation). We evaluate the performance of three such architectures that, in principle, employ different aggregation methods, which are likely to be capable of switching off neighborhood aggregation, on synthetic datasets empirically and discuss their ability or inability to switch off neighborhood aggregation qualitatively as follows. 1. ωGAT (Eliasof et al., 2023) introduces an additional feature-wise layer parameter ω that can, in principle, switch off neighborhood aggregation by setting ω parameters to 0, in addition to the attention mechanism based on GAT. How- ever, in practice, as we verify on our synthetic dataset in Figure 14, it is unable to effectively switch off neighborhood aggregation. Although it outperforms GAT, it is still substantially worse than GATE, especially for the deeper model due to unnecessary neighborhood aggregations. Another architecture based on graph attention, superGAT(Kim & Oh, 2021), falls under the paradigm of structural learning as it uses a self-supervised attention mechanism essentially for link prediction between nodes, and therefore its comparison with GATE is infeasible. 2. GraphSAGE (Hamilton et al., 2018) uses the concatenation operation to combine the node’s own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neigh- borhood aggregation for the synthetic datasets designed for the self-sufficient learning task (see Table 10). Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing (see Table 11). 3. FAGCN (Bo et al., 2021) requires a slightly more detailed analysis. Authors of FAGCN state in the paper that: ‘When αG ij ≈ 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.’ where αG ij defined in the paper can be considered analogous to αij in GAT, though they are defined differently. Thus, from an expressivity point of view, FAGCN should be able to assign parameters such that all αG ij = 0. However, we empirically observe on synthetic datasets designed for the self-sufficient learning task, values of αG ij do not, in fact, approach zero. Despite being unable to switch off neighborhood aggregation, FAGCN, in its default implementation, achieves 100% test accuracy on the task. We discover this is so because FAGCN introduces direct skip connections of non-linearly transformed raw node features to every hidden layer. Given the simplicity of the one-hot encoded features in the datasets and the complete dependence of the label on these features, FAGCN is able to represent the desired function. In order to better judge its ability to switch off neighborhood aggregation by setting αG ij = 0, we remove this skip connection. From an expressivity point of view, FAGCN should still be able to achieve 100% test accuracy by using only the (non-)linear transformations of raw features initially and performing no neighborhood aggregation in the hidden layers. However, we find that FAGCN was unable to emulate this behavior in practice. For a fair comparison of the differently designed attention mechanism in FAGCN with GATE, we introduce self-loops in the data so FAGCN may also receive a node’s own features in every hidden layer. Even then, FAGCN fails to achieve perfect test accuracy as shown in Table 10. Therefore, we suspect the attention mechanism in FAGCN may also be susceptible to the trainability issues we have identified for the attention mechanism in GAT. Nevertheless, the capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE and both could be combined. It would be interesting to derive conservation laws inherent to other architectures such as FAGCN and GraphSAGE and study how they govern the behaviour of parameters. Furthermore, by design, FAGCN does not perform any non-linear transformations of aggregated neighborhood features which may be necessary in some tasks, such as our synthetic dataset for the neighbor-dependent learning task. As Table 11 shows, GATE outperforms FAGCN on such a task. Lastly, we would like to emphasize that our aim is to provide insights into the attention mechanism of GAT and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. 24GATE: How to Keep Out Intrusive Neighbors Figure 14: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels, for theωGAT architecture for the1, 2 and 5 layer models that achieve test accuracy of 100%, 98.5%, and 49.3%, respectively. Table 10: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of network layers, respectively. Original (Orig.) and Randomized (Rand.) labels are used for the Cora structure. The FAGCN model is implemented without skip connections from the input layer to every other layer and without any self-loops in input data, whereas FAGCN* denotes the model also without skip connections but with self-loops introduced for all nodes in input data. Structure C L Max. Test Accuracy (%) GAT GATE FAGCN FAGCN* SAGE Cora O,7 1 100 100 90.1 97 .6 100 2 94 .6 100 94.2 94 .9 98 .8 5 88 .5 99.7 87.1 89 .1 92 .4 R,7 1 100 100 61.6 97 .8 100 2 57 .0 100 69.2 70 .5 100 5 36 .7 100 21.2 36 .7 99 .6 ER (p = 0.01) R,2 1 100 100 100 100 100 2 100 100 100 100 100 5 99 .6 100 96.4 99 .2 100 R,8 1 99 .2 100 86.4 98 .8 100 2 97 .6 100 86.0 91 .6 100 5 38 .4 100 31.6 40 .4 100 25GATE: How to Keep Out Intrusive Neighbors Table 11: Neighbor-dependent learning: k and L denote the number of hops aggregated in the neighborhood to generate labels, and the number of layers of the evaluated network, respectively. k L Max Test Accuracy (%) @ Epoch GAT GATE SAGE FAGCN 1 1 92@9564 93.6 @ 3511 93.2@2370 93 .2@1618 2 92 .8@4198 95.6 @ 111 95.6@723 94 .1@1455 3 92 .8@437 97.2 @ 82 96.8@100 81 .2@573 2 2 93.2 @ 95 92.0@105 90 .8@199 90 .4@170 3 93 .2@856 95.2 @ 189 94.4@113 88 .8@283 4 88 .4@423 90 .4@447 92.4 @ 139 87.6@549 3 3 88 .8@285 92.0 @ 47 87.6@45 89 .2@528 4 85 .6@469 91.6 @ 139 88@60 89 .2@3191 5 83 .6@1098 91.2 @ 243 86.0@35 88 .8@205 26",
      "meta_data": {
        "arxiv_id": "2406.00418v2",
        "authors": [
          "Nimrah Mustafa",
          "Rebekka Burkholz"
        ],
        "published_date": "2024-06-01T12:31:15Z",
        "pdf_url": "https://arxiv.org/pdf/2406.00418v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper identifies and experimentally demonstrates a structural limitation of Graph Attention Networks (GATs): their inability to switch off task-irrelevant neighborhood aggregation, which leads to over-smoothing and limits their performance. The authors propose GATE, an extension of GAT, that overcomes this limitation by enabling flexible weighting of node and neighborhood features. GATE alleviates over-smoothing, benefits from higher depth like perceptrons, often outperforms GATs on heterophilic datasets by down-weighting unrelated neighbors, and offers interpretable self-attention coefficients. The research also introduces a synthetic test bed to analyze a model's ability to utilize the appropriate amount of neighborhood aggregation and updates an existing conservation law related to GAT gradient dynamics for GATE.",
        "methodology": "The core methodology involves modifying the GAT architecture to create GATE. While a GAT layer is defined by equations (1) and (2), GATE modifies the attention scoring function 'euv' (Eq. 3) to Eq. (4): 'euv = (1u!=v * al_s + 1u=v * al_t)T * phi(Ul*hl-1_u + Vl*hl-1_v)'. This change introduces separate attention parameters 'al_s' for neighbors and 'al_t' for the node itself, allowing GATE to flexibly weight the importance of node features versus neighborhood features. Theoretically, this enables GATE to switch off neighborhood aggregation in a well-trainable parameter regime, in contrast to GATs where norm constraints on attention parameters ('a') make it difficult. The paper also derives an updated conservation law for GATE gradients (Theorem 4.3) to support these claims. ReLU is used as the non-linearity for GATE to interpret the sign of 'as' and 'at' parameters.",
        "experimental_setup": "The experimental validation of GATE is conducted on both synthetic and real-world graphs. The synthetic test bed includes two node classification problems: 'self-sufficient learning' (label-relevant information only in node's own features) and 'neighbor-dependent learning' (label-relevant information only in k-hop neighbors' features). These use Erdős–Rényi graphs (N=1000, p=0.01) and the Cora dataset structure with original/randomized labels, and one-hot encoded labels/multivariate normal features. Real-world evaluations are performed on five heterophilic benchmark datasets (roman-empire, amazon-ratings, questions, minesweeper, tolokers) and three OGB datasets (OGB-arxiv, OGB-products, OGB-mag), as well as smaller datasets (Cora, Citeseer, Actor, Texas, Wisconsin). Standard train/validation/test splits are used. Metrics include test accuracy and AUC-ROC. Models are trained using the Adam optimizer for up to 10000 epochs (synthetic), 2000 (OGB), or 5000 (others), with network widths of 64 (synthetic/small real-world) or hidden dimensions from Brody et al. (2022) for OGB. Feature transformation matrices (W, U, V) are initialized with orthogonal looks-linear structure, and 'as'/'at' in GATE are initialized to 0. Basic experiments avoid weight decay, dropout, or hyperparameter optimization to isolate architectural effects.",
        "limitations": "The paper highlights that GAT's structural limitation is its inability to switch off task-irrelevant neighborhood aggregation, leading to over-smoothing and hindering performance, particularly with increased model depth or randomized labels. This is attributed to norm constraints imposed by GAT's gradient flow dynamics, requiring large attention parameters to switch off aggregation, which leads to a less trainable regime. For the synthetic neighbor-dependent task, GATE cannot achieve perfect 100% test accuracy, possibly due to data points near the decision boundary which is not crisply defined. Smaller real-world datasets are noted to be prone to overfitting in deeper models without additional regularization or skip connections. The paper also notes that the notion of 'over-smoothing' is task-dependent, making it difficult to determine the optimal degree or a universal threshold for 'over'-smoothing, suggesting a need for task-dependent smoothness measures.",
        "future_research_directions": "The paper suggests that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. Possible extensions include combining GATE's capabilities with other GNN techniques, such as the capacity of FAGCN to learn negative associations with neighboring nodes. Further research could also focus on deriving conservation laws inherent to other GNN architectures like FAGCN and GraphSAGE to understand how they govern parameter behavior. Another direction could involve an in-depth analysis and curation of task-dependent smoothness measures to better understand and quantify 'over-smoothing'."
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the optimization and learning dynamics of Graph Attention Networks (GATs), a popular GNN architecture. It derives a novel conservation law of GAT gradient flow dynamics, which explains why a significant portion of parameters in GATs with standard initialization struggle to change during training, a problem amplified in deeper GATs. To address this, the study proposes a balanced initialization scheme that enables more effective gradient propagation, allowing the training of deeper networks and achieving considerable speedups in training and convergence time compared to standard initialization. This work serves as a foundational step for studying the learning dynamics of positive homogeneous models with attention mechanisms.",
        "methodology": "The methodology is centered on analyzing the gradient flow dynamics of GATs. The authors derive a conservation law by identifying multiplicative rescale invariance in GATs for every neuron at each layer, utilizing principles from traditional deep neural networks and Lemma 6.2 (gradient structure due to rescale invariance). This law relates the scalar products of parameters and their gradients. Based on this theoretical insight, a 'balancedness' condition (c=0) is defined. A practical initialization scheme is then devised, termed 'Balancing,' which involves setting attention parameters to zero and appropriately scaling feature weights to satisfy the balancedness condition. Additionally, a 'Balanced Orthogonal Initialization' is proposed, incorporating a Looks-Linear-Orthogonal (LLortho) structure for feature weights prior to balancing, which aims to enhance dynamical isometry, particularly effective with ReLU activation functions.",
        "experimental_setup": "Experiments were conducted on nine benchmark datasets for semi-supervised node classification: Cora, Citeseer, Pubmed, Actor, Chameleon, Cornell, Squirrel, Texas, and Wisconsin. The datasets used standard train/validation/test splits, with isolated nodes removed from Citeseer. All experiments were implemented using the Pytorch Geometric framework on Nvidia T4 Tensor Core or Nvidia GeForce RTX 3060 Laptop GPUs. Models were trained for up to 5000 epochs using SGD and Adam optimizers, with specific learning rates chosen for different depths and datasets without fine-tuning across initializations. Model selection was based on the highest validation accuracy. Evaluations involved comparisons across different GAT depths (2 to 80 layers) and initialization schemes (Xavier, Xavier with zero attention, Balanced Xavier, Balanced LL-Orthogonal). Performance metrics included test accuracy, epochs to convergence, analysis of relative parameter change, and relative gradient norms. Ablation studies also compared with Lipschitz Normalization and other orthogonal initialization variants.",
        "limitations": "The derived conservation law is specific to the self-attention mechanisms in the original GAT and GATv2 models, and their variations like ωGAT. It does not directly apply to other types of self-attention, such as dot-product self-attention, which would require modifications to the law. The theoretical derivation assumes positively homogeneous activation functions, meaning activation functions like ELU, which are not strictly homogeneous, may negatively impact the proposed orthogonal initializations (e.g., BalO). Furthermore, the balancing technique does not achieve perfect dynamical isometry in GATs due to the complexities of neighborhood aggregation, unlike its effect in perceptrons. While the theory assumes infinitesimal learning rates (gradient flow), it holds sufficiently well for practical, finite learning rates and extends to optimizers like Adam, though primarily derived for vanilla gradient descent. The study's focus was on highlighting trainability mechanisms rather than outperforming state-of-the-art specialized models on heterophilic datasets.",
        "future_research_directions": "Future work could explore how dynamical isometry can be achieved or approximated more effectively in general Graph Neural Networks. Another promising direction is to derive modifications to the conservation law for other attention-based models, particularly those utilizing the dot-product self-attention mechanism like SuperGAT, and for transformer architectures, including those adapted for graph learning and vision transformers that inherently demand greater depth. Additionally, further investigation into how overparameterization, specifically increased width, contributes to the trainability of deeper GNN models could be an area of independent interest."
      }
    },
    {
      "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method",
      "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning\nrepresentations of attributed graphs. To scale GCNs to large graphs,\nstate-of-the-art methods use various layer sampling techniques to alleviate the\n\"neighbor explosion\" problem during minibatch training. We propose GraphSAINT,\na graph sampling based inductive learning method that improves training\nefficiency and accuracy in a fundamentally different way. By changing\nperspective, GraphSAINT constructs minibatches by sampling the training graph,\nrather than the nodes or edges across GCN layers. Each iteration, a complete\nGCN is built from the properly sampled subgraph. Thus, we ensure fixed number\nof well-connected nodes in all layers. We further propose normalization\ntechnique to eliminate bias, and sampling algorithms for variance reduction.\nImportantly, we can decouple the sampling from the forward and backward\npropagation, and extend GraphSAINT with many architecture variants (e.g., graph\nattention, jumping connection). GraphSAINT demonstrates superior performance in\nboth accuracy and training time on five large graphs, and achieves new\nstate-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).",
      "full_text": "Published as a conference paper at ICLR 2020 GraphSAINT: G RAPH SAMPLING BASED INDUCTIVE LEARNING METHOD Hanqing Zeng∗ University of Southern California zengh@usc.edu Hongkuan Zhou∗ University of Southern California hongkuaz@usc.edu Ajitesh Srivastava University of Southern California ajiteshs@usc.edu Rajgopal Kannan US Army Research Lab rajgopal.kannan.civ@mail.mil Viktor Prasanna University of Southern California prasanna@usc.edu ABSTRACT Graph Convolutional Networks (GCNs) are powerful models for learning repre- sentations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use variouslayer samplingtechniques to alleviate the “neighbor explosion” problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efﬁciency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure ﬁxed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the for- ward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on ﬁve large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). 1 I NTRODUCTION Recently, representation learning on graphs has attracted much attention, since it greatly facilitates tasks such as classiﬁcation and clustering (Wu et al., 2019; Cai et al., 2017). Current works on Graph Convolutional Networks (GCNs) (Hamilton et al., 2017; Chen et al., 2018b; Gao et al., 2018; Huang et al., 2018; Chen et al., 2018a) mostly focus on shallow models (2 layers) on relatively small graphs. Scaling GCNs to larger datasets and deeper layers still requires fast alternate training methods. In a GCN, data to be gathered for one output node comes from its neighbors in the previous layer. Each of these neighbors in turn, gathers its output from the previous layer, and so on. Clearly, the deeper we back track, the more multi-hop neighbors to support the computation of the root. The number of support nodes (and thus the training time) potentially grows exponentially with the GCN depth. To mitigate such “neighbor explosion”, state-of-the-art methods use variouslayer sampling techniques. The works by Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a) ensure that only a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. Chen et al. (2018b) and Huang et al. (2018) further propose samplers to restrict the neighbor expansion factor to 1, by ensuring a ﬁxed sample size in all layers. While these methods signiﬁcantly speed up training, they face challenges in scalability, accuracy or computation complexity. ∗Equal contribution 1 arXiv:1907.04931v4  [cs.LG]  16 Feb 2020Published as a conference paper at ICLR 2020 Present work We present GraphSAINT (Graph SAmpling based INductive learning meThod) to efﬁciently train deep GCNs. GraphSAINT is developed from a fundamentally different way of minibatch construction. Instead of building a GCN on the full training graph and then sampling across the layers, we sample the training graph ﬁrst and then build a full GCN on the subgraph. Our method is thus graph samplingbased. Naturally, GraphSAINT resolves “neighbor explosion”, since every GCN of the minibatches is a small yet complete one. On the other hand, graph sampling based method also brings new challenges in training. Intuitively, nodes of higher inﬂuence on each other should have higher probability to form a subgraph. This enables the sampled nodes to “support” each other without going outside the minibatch. Unfortunately, such strategy results in non-identical node sampling probability, and introduces bias in the minibatch estimator. To address this issue, we develop normalization techniques so that the feature learning does not give preference to nodes more frequently sampled. To further improve training quality, we perform variance reduction analysis, and design light-weight sampling algorithms by quantifying “inﬂuence” of neighbors. Experiments on GraphSAINT using ﬁve large datasets show signiﬁcant performance gain in both training accuracy and time. We also demonstrate the ﬂexibility of GraphSAINT by integrating our minibatch method with popular GCN architectures such as JK-net (Xu et al., 2018) and GAT (Veliˇckovi´c et al., 2017). The resulting deep models achieve new state-of-the-art F1 scores on PPI (0.995) and Reddit (0.970). 2 R ELATED WORK A neural network model that extends convolution operation to the graph domain is ﬁrst proposed by Bruna et al. (2013). Further, Kipf & Welling (2016); Defferrard et al. (2016) speed up graph convolution computation with localized ﬁlters based on Chebyshev expansion. They target relatively small datasets and thus the training proceeds in full batch. In order to scale GCNs to large graphs, layer sampling techniques (Hamilton et al., 2017; Chen et al., 2018b; Ying et al., 2018a; Chen et al., 2018a; Gao et al., 2018; Huang et al., 2018) have been proposed for efﬁcient minibatch training. All of them follow the three meta steps: 1. Construct a complete GCN on the full training graph. 2. Sample nodes or edges of each layer to form minibatches. 3. Propagate forward and backward among the sampled GCN. Steps (2) and (3) proceed iteratively to update the weights via stochastic gradient descent. The layer sampling algorithm of GraphSAGE (Hamilton et al., 2017) performs uniform node sampling on the previous layer neighbors. It enforces a pre-deﬁned budget on the sample size, so as to bound the minibatch computation complexity. Ying et al. (2018a) enhances the layer sampler of Hamilton et al. (2017) by introducing an importance score to each neighbor. The algorithm presumably leads to less information loss due to weighted aggregation. S-GCN (Chen et al., 2018a) further restricts neighborhood size by requiring only two support nodes in the previous layer. The idea is to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN (Chen et al., 2018b) performs sampling from another perspective. Instead of tracking down the inter-layer connections, node sampling is performed independently for each layer. It applies importance sampling to reduce variance, and results in constant sample size in all layers. However, the minibatches potentially become too sparse to achieve high accuracy. Huang et al. (2018) improves FastGCN by an additional sampling neural network. It ensures high accuracy, since sampling is conditioned on the selected nodes in the next layer. Signiﬁcant overhead may be incurred due to the expensive sampling algorithm and the extra sampler parameters to be learned. Instead of sampling layers, the works of Zeng et al. (2018) and Chiang et al. (2019) build mini- batches from subgraphs. Zeng et al. (2018) proposes a speciﬁc graph sampling algorithm to ensure connectivity among minibatch nodes. They further present techniques to scale such training on shared-memory multi-core platforms. More recently, ClusterGCN (Chiang et al., 2019) proposes graph clustering based minibatch training. During pre-processing, the training graph is partitioned into densely connected clusters. During training, clusters are randomly selected to form minibatches, and intra-cluster edge connections remain unchanged. Similar to GraphSAINT, the works of Zeng et al. (2018) and Chiang et al. (2019) do not sample the layers and thus “neighbor explosion” is avoided. Unlike GraphSAINT, both works are heuristic based, and do not account for bias due to the unequal probability of each node / edge appearing in a minibatch. Another line of research focuses on improving model capacity. Applying attention on graphs, the architectures of Veliˇckovi´c et al. (2017); Zhang et al. (2018); Lu et al. (2019) better capture neighbor features by dynamically adjusting edge weights. Klicpera et al. (2018) combines PageRank with GCNs to enable efﬁcient information propagation from many hops away. To develop deeper models, 2Published as a conference paper at ICLR 2020 “skip-connection” is borrowed from CNNs (He et al., 2015; Huang et al., 2017) into the GCN context. In particular, JK-net Xu et al. (2018) demonstrates signiﬁcant accuracy improvement on GCNs with more than two layers. Note, however, that JK-net (Xu et al., 2018) follows the same sampling strategy as GraphSAGE (Hamilton et al., 2017). Thus, its training cost is high due to neighbor explosion. In addition, high order graph convolutional layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) also help propagate long-distance features. With the numerous architectural variants developed, the question of how to train them efﬁciently via minibatches still remains to be answered. 3 P ROPOSED METHOD : GraphSAINT Graph sampling based method is motivated by the challenges in scalability (in terms of model depth and graph size). We analyze the bias (Section 3.2) and variance (Section 3.3) introduced by graph sampling, and thus, propose feasible sampling algorithms (Section 3.4). We show the applicability of GraphSAINT to other architectures, both conceptually (Section 4) and experimentally (Section 5.2). In the following, we deﬁne the problem of interest and the corresponding notations. A GCN learns representation of an un-directed, attributed graph G(V,E), where each node v ∈V has a length-f attribute xv. Let A be the adjacency matrix and ˜A be the normalized one (i.e., ˜A = D−1A, and D is the diagonal degree matrix). Let the dimension of layer-ℓinput activation be f(ℓ). The activation of node v is x(ℓ) v ∈Rf(ℓ) , and the weight matrix is W(ℓ) ∈Rf(ℓ)×f(ℓ+1) . Note that xv = x(1) v . Propagation rule of a layer is deﬁned as follows: x(ℓ+1) v = σ (∑ u∈V ˜Av,u ( W(ℓ) )T x(ℓ) u ) (1) where ˜Av,u is a scalar, taking an element of ˜A. And σ(·) is the activation function (e.g., ReLU). We use subscript “s” to denote parameterd of the sampled graph (e.g.,Gs, Vs, Es). GCNs can be applied under inductive and transductive settings. While GraphSAINT is applicable to both, in this paper, we focus on inductive learning. It has been shown that inductive learning is especially challenging (Hamilton et al., 2017) — during training, neither attributes nor connections of the test nodes are present. Thus, an inductive model has to generalize to completely unseen graphs. 3.1 M INIBATCH BY GRAPH SAMPLING 0 1 2 3 4 5 6 8 9 7 0 1 2 3 5 7 0 1 2 3 5 7 0 1 2 3 5 7 Gs = SAMPLE(G) Full GCN on Gs Figure 1: GraphSAINT training algorithm GraphSAINT follows the design philosophy of directly sampling the training graph G, rather than the corresponding GCN. Our goals are to 1. extract appropriately connected subgraphs so that little information is lost when propagating within the subgraphs, and 2. combine information of many subgraphs together so that the training process overall learns good representation of the full graph. Figure 1 and Algorithm 1 illustrate the training algorithm. Before training starts, we perform light-weight pre-processing on Gwith the given sampler SAMPLE. The pre-processing estimates the probability of a node v∈V and an edge e∈E being sampled by SAMPLE. Such probability is later used to normalize the subgraph neighbor aggregation and the minibatch loss (Section 3.2). Afterwards, 3Published as a conference paper at ICLR 2020 Algorithm 1GraphSAINT training algorithm Input: Training graph G(V,E,X); Labels Y ; Sampler SAMPLE; Output: GCN model with trained weights 1: Pre-processing: Setup SAMPLE parameters; Compute normalization coefﬁcients α, λ. 2: for each minibatch do 3: Gs (Vs,Es) ←Sampled sub-graph of Gaccording to SAMPLE 4: GCN construction on Gs. 5: {yv |v∈Vs}← Forward propagation of {xv |v∈Vs}, normalized by α 6: Backward propagation from λ-normalized loss L(yv,yv). Update weights. 7: end for training proceeds by iterative weight updates via SGD. Each iteration starts with an independently sampled Gs (where |Vs| ≪|V|). We then build a full GCN on Gs to generate embedding and calculate loss for every v∈Vs. In Algorithm 1, node representation is learned by performing node classiﬁcation in the supervised setting, and each training node vcomes with a ground truth label yv. Intuitively, there are two requirements for SAMPLE: 1. Nodes having high inﬂuence on each other should be sampled in the same subgraph. 2. Each edge should have non-negligible probability to be sampled. For requirement (1), an ideal SAMPLE would consider the joint information from node connections as well as attributes. However, the resulting algorithm may have high complexity as it would need to infer the relationships between features. For simplicity, we deﬁne “inﬂuence” from the graph connectivity perspective and design topology based samplers. Requirement (2) leads to better generalization since it enables the neural net to explore the full feature and label space. 3.2 N ORMALIZATION A sampler that preserves connectivity characteristic of Gwill almost inevitably introduce bias into minibatch estimation. In the following, we present normalization techniques to eliminate biases. Analysis of the complete multi-layer GCN is difﬁcult due to non-linear activations. Thus, we analyze the embedding of each layer independently. This is similar to the treatment of layers independently by prior work (Chen et al., 2018b; Huang et al., 2018). Consider a layer-(ℓ+ 1)node vand a layer-ℓ node u. If vis sampled (i.e., v∈Vs), we can compute the aggregated feature of vas: ζ(ℓ+1) v = ∑ u∈V ˜Av,u αu,v ( W(ℓ) )T x(ℓ) u 1 u|v = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u 1 u|v, (2) where ˜x(ℓ) u = ( W(ℓ))T x(ℓ) u , and 1 u|v ∈{0,1}is the indicator function given vis in the subgraph (i.e., 1 u|v = 0if v∈Vs ∧(u,v) ̸∈Es; 1 u|v = 1if (u,v) ∈Es; 1 u|v not deﬁned if v̸∈Vs). We refer to the constant αu,v as aggregator normalization. Deﬁne pu,v = pv,u as the probability of an edge (u,v) ∈E being sampled in a subgraph, and pv as the probability of a node v∈V being sampled. Proposition 3.1. ζ(ℓ+1) v is an unbiased estimator of the aggregation ofvin the full(ℓ+ 1)th GCN layer, ifαu,v = pu,v pv . i.e.,E ( ζ(ℓ+1) v ) = ∑ u∈V ˜Av,u ˜x(ℓ) u . Assuming that each layer independently learns an embedding, we use Proposition 3.1 to normalize feature propagation of each layer of the GCN built byGraphSAINT. Further, let Lv be the loss on vin the output layer. The minibatch loss is calculated as Lbatch = ∑ v∈Gs Lv/λv, where λv is a constant that we term loss normalization. We set λv = |V|· pv so that: E(Lbatch) = 1 |G| ∑ Gs∈G ∑ v∈Vs Lv λv = 1 |V| ∑ v∈V Lv. (3) Feature propagation within subgraphs thus requires normalization factors αand λ, which are com- puted based on the edge and node probability pu,v, pv. In the case of random node or random edge samplers, pu,v and pv can be derived analytically. For other samplers in general, closed form expression is hard to obtain. Thus, we perform pre-processing for estimation. Before training starts, 4Published as a conference paper at ICLR 2020 we run the sampler repeatedly to obtain a set of N subgraphs G. We setup a counter Cv and Cu,v for each v∈V and (u,v) ∈E, to count the number of times the node or edge appears in the subgraphs of G. Then we set αu,v = Cu,v Cv = Cv,u Cv and λv = Cv N . The subgraphs Gs ∈G can all be reused as minibatches during training. Thus, the overhead of pre-processing is small (see Appendix D.2). 3.3 V ARIANCE We derive samplers for variance reduction. Letebe the edge connectingu, v, and b(ℓ) e = ˜Av,u ˜x(ℓ−1) u + ˜Au,v ˜x(ℓ−1) v . It is desirable that variance of all estimators ζ(ℓ) v is small. With this objective, we deﬁne: ζ = ∑ ℓ ∑ v∈Gs ζ(ℓ) v pv = ∑ ℓ ∑ v,u ˜Av,u pvαu,v ˜x(ℓ) u 1 v1 u|v = ∑ ℓ ∑ e b(ℓ) e pe 1 (ℓ) e . (4) where 1 e = 1if e∈Es; 1 e = 0if e̸∈Es. And 1 v = 1if v∈Vs; 1 v = 0if v̸∈Vs. The factor pu in the ﬁrst equality is present so that ζis an unbiased estimator of the sum of all node aggregations at all layers: E(ζ) =∑ ℓ ∑ v∈VE ( ζ(ℓ) v ) . Note that 1 (ℓ) e = 1 e,∀ℓ, since once an edge is present in the sampled graph, it is present in all layers of our GCN. We deﬁne the optimal edge sampler to minimize variance for every dimension of ζ. We restrict ourselves to independent edge sampling. For each e∈E, we make independent decision on whether it should be in Gs or not. The probability of including eis pe. We further constrain ∑pe = m, so that the expected number of sampled edges equals to m. The budget mis a given sampling parameter. Theorem 3.2. Under independent edge sampling with budgetm, the optimal edge probabilities to minimize the sum of variance of eachζ’s dimension is given by:pe = m∑ e′ ∑ ℓ b(ℓ) e′  ∑ ℓ b(ℓ) e . To prove Theorem 3.2, we make use of the independence among graph edges, and the dependence among layer edges to obtain the covariance of 1 (ℓ) e . Then using the fact that sum of pe is a constant, we use the Cauchy-Schwarz inequality to derive the optimal pe. Details are in Appendix A. Note that calculating b(ℓ) e requires computing ˜x(ℓ−1) v , which increases the complexity of sampling. As a reasonable simpliﬁcation, we ignore ˜x(ℓ) v to make the edge probability dependent on the graph topology only. Therefore, we choose pe ∝ ˜Av,u + ˜Au,v = 1 deg(u) + 1 deg(v) . The derived optimal edge sampler agrees with the intuition in Section 3.1. If two nodes u, v are connected and they have few neighbors, then uand vare likely to be inﬂuential to each other. In this case, the edge probability pu,v = pv,u should be high. The above analysis on edge samplers also inspires us to design other samplers, which are presented in Section 3.4. Remark We can also apply the above edge sampler to perform layer sampling. Under the indepen- dent layer sampling assumption of Chen et al. (2018b), one would sample a connection ( u(ℓ),v(ℓ+1)) with probability p(ℓ) u,v ∝ 1 deg(u) + 1 deg(v) . For simplicity, assume a uniform degree graph (of degree d). Then p(ℓ) e = p. For an already sampled u(ℓ) to connect to layer ℓ+ 1, at least one of its edges has to be selected by the layer ℓ+ 1sampler. Clearly, the probability of an input layer node to “survive” the Lnumber of independent sampling process is ( 1 −(1 −p)d )L−1 . Such layer sampler potentially returns an overly sparse minibatch for L> 1. On the other hand, connectivity within a minibatch of GraphSAINT never drops with GCN depth. If an edge is present in layer ℓ, it is present in all layers. 3.4 S AMPLERS Based on the above variance analysis, we present several light-weight and efﬁcient samplers that GraphSAINT has integrated. Detailed sampling algorithms are listed in Appendix B. Random node sampler We sample |Vs|nodes from Vrandomly, according to a node probability distribution P(u) ∝ ˜A:,u  2 . This sampler is inspired by the layer sampler of Chen et al. (2018b). 5Published as a conference paper at ICLR 2020 Random edge sampler We perform edge sampling as described in Section 3.3. Random walk based samplersAnother way to analyze graph sampling based multi-layer GCN is to ignore activations. In such case, Llayers can be represented as a single layer with edge weights given by B = ˜AL. Following a similar approach as Section 3.3, if it were possible to pick pairs of nodes (whether or not they are directly connected in the original ˜A) independently, then we would set pu,v ∝Bu,v + Bv,u, where Bu,v can be interpreted as the probability of a random walk to start at uand end at v in Lhops (and Bv,u vice-versa). Even though it is not possible to sample a subgraph where such pairs of nodes are independently selected, we still consider a random walk sampler with walk length Las a good candidate for L-layer GCNs. There are numerous random walk based samplers proposed in the literature (Ribeiro & Towsley, 2010; Leskovec & Faloutsos, 2006; Hu & Lau, 2013; Li et al., 2015). In the experiments, we implement a regular random walk sampler (with rroot nodes selected uniformly at random and each walker goes hhops), and also a multi-dimensional random walk sampler deﬁned in Ribeiro & Towsley (2010). For all the above samplers, we return the subgraph induced from the sampled nodes. The induction step adds more connections into the subgraph, and empirically helps improve convergence. 4 D ISCUSSION Extensions GraphSAINT admits two orthogonal extensions. First, GraphSAINT can seamlessly integrate other graph samplers. Second, the idea of training by graph sampling is applicable to many GCN architecture variants: 1. Jumping knowledge(Xu et al., 2018): since our GCNs constructed during training are complete, applying skip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods (Chen et al., 2018b; Huang et al., 2018), extra modiﬁcation to their samplers is required, since the jumping knowledge architecture requires layer-ℓ samples to be a subset of layer-(ℓ−1) samples∗. 2. Attention (Veliˇckovi´c et al., 2017; Fey, 2019; Zhang et al., 2018): while explicit variance reduction is hard due to the dynamically updated attention values, it is reasonable to apply attention within the subgraphs which are considered as representatives of the full graph. Our loss and aggregator normalizations are also applicable†. 3. Others: To support high order layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) or even more complicated networks for the task of graph classiﬁcation (Ying et al., 2018b), we replace the full adjacency matrix A with the (normalized) one for the subgraph As to perform layer propagation. Comparison GraphSAINT enjoys: 1. high scalability and efﬁciency, 2. high accuracy, and 3. low training complexity. Point (1) is due to the signiﬁcantly reduced neighborhood size compared with Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a). Point (2) is due to the better inter- layer connectivity compared with Chen et al. (2018b), and unbiased minibatch estimator compared with Chiang et al. (2019). Point (3) is due to the simple and trivially parallelizable pre-processing compared with the sampling of Huang et al. (2018) and clustering of Chiang et al. (2019). 5 E XPERIMENTS Setup Experiments are under the inductive, supervised learning setting. We evaluate GraphSAINT on the following tasks: 1. classifying protein functions based on the interactions of human tissue proteins (PPI), 2. categorizing types of images based on the descriptions and common properties of online images (Flickr), 3. predicting communities of online posts based on user comments (Reddit), 4. categorizing types of businesses based on customer reviewers and friendship (Yelp), and 5. classifying product categories based on buyer reviewers and interactions (Amazon). For PPI, we use the small version for the two layer convergence comparison (Table 2 and Figure 2), since Hamilton et al. (2017) and Chen et al. (2018a) report accuracy for this version in their original papers. We use the large version for additional comparison with Chiang et al. (2019) to be consistent with its reported accuracy. All datasets follow “ﬁxed-partition” splits. Appendix C.2 includes further details. ∗The skip-connection design proposed by Huang et al. (2018) does not have such “subset” requirement, and thus is compatible with both graph sampling and layer sampling based methods. †When applying GraphSAINT to GAT (Veliˇckovi´c et al., 2017), we remove the softmax step which normalizes attention values within the same neighborhood, as suggested by Huang et al. (2018). See Appendix C.3. 6Published as a conference paper at ICLR 2020 Table 1: Dataset statistics (“m” stands formulti-class classiﬁcation, and “s” for single-class.) Dataset Nodes Edges Degree Feature Classes Train / Val / Test PPI 14,755 225,270 15 50 121 (m) 0.66 / 0.12 / 0.22 Flickr 89,250 899,756 10 500 7 (s) 0.50 / 0.25 / 0.25 Reddit 232,965 11,606,919 50 602 41 (s) 0.66 / 0.10 / 0.24 Yelp 716,847 6,977,410 10 300 100 (m) 0.75 / 0.10 / 0.15 Amazon 1,598,960 132,169,734 83 200 107 (m) 0.85 / 0.05 / 0.10 PPI (large version) 56,944 818,716 14 50 121 (m) 0.79 / 0.11 / 0.10 We open source GraphSAINT‡. We compare with six baselines: 1. vanilla GCN (Kipf & Welling, 2016), 2. GraphSAGE (Hamilton et al., 2017), 3. FastGCN (Chen et al., 2018b), 4. S-GCN (Chen et al., 2018a), 5. AS-GCN (Huang et al., 2018), and 6. ClusterGCN (Chiang et al., 2019). All baselines are executed with their ofﬁcially released code (see Appendix C.3 for downloadable URLs and commit numbers). Baselines and GraphSAINT are all implemented in Tensorﬂow with Python3. We run experiments on a NVIDIA Tesla P100 GPU (see Appendix C.1 for hardware speciﬁcation). 5.1 C OMPARISON WITH STATE-OF-THE -ART Table 2 and Figure 2 show the accuracy and convergence comparison of various methods. All results correspond to two-layer GCN models (for GraphSAGE, we use its mean aggregator). For a given dataset, we keep hidden dimension the same across all methods. We describe the detailed architecture and hyperparameter search procedure in Appendix C.3. The mean and conﬁdence interval of the accuracy values in Table 2 are measured by three runs under the same hyperparameters. The training time of Figure 2 excludes the time for data loading, pre-processing, validation set evaluation and model saving. Our pre-processing incurs little overhead in training time. See Appendix D.2 for cost of graph sampling. For GraphSAINT, we implement the graph samplers described in Section 3.4. In Table 2, “Node” stands for random node sampler; “Edge” stands for random edge sampler; “RW” stands for random walk sampler; “MRW” stands for multi-dimensional random walk sampler. Table 2: Comparison of test set F1-micro score with state-of-the-art methods Method PPI Flickr Reddit Yelp Amazon GCN 0.515 ±0.006 0.492 ±0.003 0.933 ±0.000 0.378 ±0.001 0.281 ±0.005 GraphSAGE 0.637 ±0.006 0.501 ±0.013 0.953 ±0.001 0.634 ±0.006 0.758 ±0.002 FastGCN 0.513 ±0.032 0.504 ±0.001 0.924 ±0.001 0.265 ±0.053 0.174 ±0.021 S-GCN 0.963 ±0.010 0.482 ±0.003 0.964 ±0.001 0.640 ±0.002 — ‡ AS-GCN 0.687 ±0.012 0.504 ±0.002 0.958 ±0.001 — ‡ — ‡ ClusterGCN 0.875 ±0.004 0.481 ±0.005 0.954 ±0.001 0.609 ±0.005 0.759 ±0.008 GraphSAINT-Node 0.960±0.001 0.507 ±0.001 0.962 ±0.001 0.641 ±0.000 0.782 ±0.004 GraphSAINT-Edge 0.981±0.007 0.510 ±0.002 0.966±0.001 0.653±0.003 0.807 ±0.001 GraphSAINT-RW 0.981±0.004 0.511±0.001 0.966±0.001 0.653±0.003 0.815±0.001 GraphSAINT-MRW 0.980±0.006 0.510 ±0.001 0.964 ±0.000 0.652 ±0.001 0.809 ±0.001 Table 3: Additional comparison with ClusterGCN (test set F1-micro score) PPI (large version) Reddit 2 ×512 5 ×2048 2 ×128 4 ×128 ClusterGCN 0.903 ±0.002 0.994 ±0.000 0.954 ±0.001 0.966 ±0.001 GraphSAINT 0.941±0.003 0.995±0.000 0.966±0.001 0.970±0.001 ‡Open sourced code: https://github.com/GraphSAINT/GraphSAINT ‡The codes throw runtime error on the large datasets (Yelp or Amazon). 7Published as a conference paper at ICLR 2020 0 20 40 600.4 0.6 0.8 1 Validation F1-micro PPI 0 10 20 30 40 0.44 0.46 0.48 0.5 0.52 Flickr 0 50 100 1500.9 0.92 0.94 0.96 0.98 Reddit 0 200 400 600 800 0.25 0.45 0.65 Yelp 0 200 400 0.2 0.4 0.6 0.8 Training time (second) Amazon GCN GraphSAGE FastGCN* S-GCN AS-GCN ClusterGCN GraphSAINT *: CPU execution time -RW Figure 2: Convergence curves of 2-layer models on GraphSAINT and baselines Clearly, with appropriate graph samplers, GraphSAINT achieves signiﬁcantly higher accuracy on all datasets. For GraphSAINT-Node, we use the same node probability as FastGCN. Thus, the accuracy improvement is mainly due to the switching from layer sampling to graph sampling (see “Remark” in Section 3.3). Compared with AS-GCN, GraphSAINT is signiﬁcantly faster. The sampler of AS-GCN is expensive to execute, making its overall training time even longer than vanilla GCN. We provide detailed computation complexity analysis on the sampler in Appendix D.2. For S-GCN on Reddit, it achieves similar accuracy as GraphSAINT, at the cost of over 9×longer training time. The released code of FastGCN only supports CPU execution, so its convergence curve is dashed. Table 3 presents additional comparison with ClusterGCN. We useL×f to specify the architecture, where Land f denote GCN depth and hidden dimension, respectively. The four architectures are the ones used in the original paper (Chiang et al., 2019). Again, GraphSAINT achieves signiﬁcant accuracy improvement. To train models with L> 2 often requires additional architectural tweaks. ClusterGCN uses its diagonal enhancement technique for the 5-layer PPI and 4-layer Reddit models. GraphSAINT uses jumping knowledge connection (Xu et al., 2018) for 4-layer Reddit. Evaluation on graph samplers From Table 2, random edge and random walk based samplers achieve higher accuracy than the random node sampler. Figure 3 presents sensitivity analysis on parameters of “RW”. We use the same hyperparameters (except the sampling parameters) and network architecture as those of the “RW” entries in Table 2. We ﬁx the length of each walker to2 (i.e., GCN depth), and vary the number of roots rfrom 250 to 2250. For PPI, increasing rfrom 250 to 750 signiﬁcantly improves accuracy. Overall, for all datasets, accuracy stabilizes beyond r= 750. 5.2 GraphSAINT ON ARCHITECTURE VARIANTS AND DEEP MODELS In Figure 4, we train a 2-layer and a 4-layer model of GAT (Veliˇckovi´c et al., 2017) and JK-net (Xu et al., 2018), by using minibatches of GraphSAGE and GraphSAINT respectively. On the two 4-layer architectures, GraphSAINT achieves two orders of magnitude speedup than GraphSAGE, indicating much better scalability on deep models. From accuracy perspective, 4-layer GAT-SAGE and JK- SAGE do not outperform the corresponding 2-layer versions, potentially due to the smoothening effect caused by the massive neighborhood size. On the other hand, with minibatches returned by our edge sampler, increasing model depth of JK-SAINT leads to noticeable accuracy improvement (from 0.966 of 2-layer to 0.970 of 4-layer). Appendix D.1 contains additional scalability results. 6 C ONCLUSION We have presented GraphSAINT, a graph sampling based training method for deep GCNs on large graphs. We have analyzed bias and variance of the minibatches deﬁned on subgraphs, and proposed 8Published as a conference paper at ICLR 2020 0 1,000 2,0000.4 0.6 0.8 1 Number of walkers Test F1-micro PPI Flickr Reddit Yelp Amazon Figure 3: Sensitivity analysis 100 102 104 0.93 0.94 0.95 0.96 0.97 Training time (second) Validation F1-micro GAT 100 102 104 JK-net GraphSAINT 2-layer GraphSAINT 4-layer GraphSAGE 2-layer GraphSAGE 4-layer Figure 4: GraphSAINT with JK-net and GAT (Reddit) normalization techniques and sampling algorithms to improve training quality. We have conducted extensive experiments to demonstrate the advantage of GraphSAINT in accuracy and training time. An interesting future direction is to develop distributed training algorithms using graph sampling based minibatches. After partitioning the training graph in distributed memory, sampling can be performed independently on each processor. Afterwards, training on the self-supportive subgraphs can signiﬁcantly reduce the system-level communication cost. To ensure the overall convergence quality, data shufﬂing strategy for the graph nodes and edges can be developed together with each speciﬁc graph sampler. Another direction is to perform algorithm-system co-optimization to accelerate the training of GraphSAINT on heterogeneous computing platforms (Zeng et al., 2018; Zeng & Prasanna, 2019). The resolution of “neighbor explosion” by GraphSAINT not only reduces the training computation complexity, but also improves hardware utilization by signiﬁcantly less data trafﬁc to the slow memory. In addition, task-level parallelization is easy since the light-weight graph sampling is completely decoupled from the GCN layer propagation. ACKNOWLEDGEMENT This material is based on work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract Number FA8750-17-C-0086 and National Science Foundation (NSF) under Contract Numbers CCF-1919289 and OAC-1911229. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of DARPA or NSF. REFERENCES Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard, Kristina Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolution architec- tures via sparsiﬁed neighborhood mixing. arXiv preprint arXiv:1905.00067, 2019. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. CoRR, abs/1312.6203, 2013. URL http://arxiv.org/abs/ 1312.6203. HongYun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph embedding: Problems, techniques and applications. CoRR, abs/1709.07604, 2017. URL http://arxiv.org/abs/1709.07604. Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with variance reduction. In ICML, pp. 941–949, 2018a. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations (ICLR), 2018b. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An ef- ﬁcient algorithm for training deep and large graph convolutional networks. CoRR, abs/1905.07953, 2019. URL http://arxiv.org/abs/1905.07953. 9Published as a conference paper at ICLR 2020 Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing Systems, pp. 3844–3852, 2016. Matthias Fey. Just jump: Dynamic neighborhood aggregation in graph neural networks. CoRR, abs/1904.04849, 2019. URL http://arxiv.org/abs/1904.04849. Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, pp. 1416–1424, New York, NY , USA, 2018. ACM. ISBN 978-1-4503-5552-0. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30, pp. 1024–1034. 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. Pili Hu and Wing Cheong Lau. A survey and taxonomy of graph sampling. arXiv preprint arXiv:1308.5865, 2013. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pp. 4558–4567, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907. Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Personalized embedding propa- gation: Combining neural networks on graphs with personalized pagerank. CoRR, abs/1810.05997, 2018. URL http://arxiv.org/abs/1810.05997. John Boaz Lee, Ryan A. Rossi, Xiangnan Kong, Sungchul Kim, Eunyee Koh, and Anup Rao. Higher- order graph convolutional networks. CoRR, abs/1809.07697, 2018. URL http://arxiv.org/ abs/1809.07697. Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 631–636. ACM, 2006. R. Li, J. X. Yu, L. Qin, R. Mao, and T. Jin. On random walk based graph sampling. In 2015 IEEE 31st International Conference on Data Engineering, pp. 927–938, April 2015. doi: 10.1109/ICDE. 2015.7113345. Haonan Lu, Seth H. Huang, Tian Ye, and Xiuyan Guo. Graph star net for generalized multi-task learning. CoRR, abs/1906.12330, 2019. URL http://arxiv.org/abs/1906.12330. Bruno Ribeiro and Don Towsley. Estimating and sampling graphs with multidimensional random walks. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pp. 390–403. ACM, 2010. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. 10Published as a conference paper at ICLR 2020 Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. CoRR, abs/1901.00596, 2019. URL http: //arxiv.org/abs/1901.00596. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, 2018a. ISBN 978-1-4503-5552-0. Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS’18, pp. 4805–4815, USA, 2018b. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id= 3327345.3327389. Hanqing Zeng and Viktor Prasanna. GraphACT: Accelerating GCN training on CPU-FPGA hetero- geneous platforms. arXiv preprint arXiv:2001.02498, 2019. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Accurate, efﬁcient and scalable graph embedding. CoRR, abs/1810.11899, 2018. URL http: //arxiv.org/abs/1810.11899. Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated atten- tion networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018. Zhenpeng Zhou. Graph convolutional networks for molecules. CoRR, abs/1706.09916, 2017. URL http://arxiv.org/abs/1706.09916. A P ROOFS Proof of Proposition 3.1.Under the condition that vis sampled in a subgraph: E ( ζ(ℓ+1) v ) =E (∑ u∈V ˜Av,u αu,v ˜x(ℓ) u 1 u|v ) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u E ( 1 u|v ) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u P((u,v) sampled|vsampled) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u P((u,v) sampled) P(vsampled) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u pu,v pv (5) where the second equality is due to linearity of expectation, and the third equality (conditional edge probability) is due to the initial condition that vis sampled in a subgraph. It directly follows that, when αu,v = pu,v pv , E ( ζ(ℓ+1) v ) = ∑ u∈V ˜Av,u ˜x(ℓ) u 11Published as a conference paper at ICLR 2020 Proof of Theorem 3.2.Below, we use Cov (·) to denote covariance and Var(·) to denote variance. For independent edge sampling as deﬁned in Section 3.3, Cov ( 1 (ℓ1) e1 ,1 (ℓ2) e2 ) = 0,∀e1 ̸= e2. And for a full GCN on the subgraph, Cov ( 1 (ℓ1) e ,1 (ℓ2) e ) = pe −p2 e. To start the proof, we ﬁrst assume that the b(ℓ) e is one dimensional (i.e., a scalar) and denote it by b(ℓ) e . Now, Var(ζ) = ∑ e,ℓ ( b(ℓ) e pe )2 Var ( 1 (ℓ) e ) + 2 ∑ e,ℓ1<ℓ2 b(ℓ1) e b(ℓ2) e p2e Cov ( 1 (ℓ1) e ,1 (ℓ2) e ) = ∑ e,ℓ ( b(ℓ) e )2 pe − ∑ e,ℓ ( b(ℓ) e )2 + 2 ∑ e,ℓ1<ℓ2 b(ℓ1) e b(ℓ2) e p2e ( pe −p2 e ) = ∑ e (∑ ℓ b(ℓ) e )2 pe − ∑ e (∑ ℓ b(ℓ) e )2 (6) Let a given constant m= ∑ e pe be the expected number of sampled edges. By Cauchy-Schwarz in- equality: ∑ e ( ∑ ℓ b(ℓ) e ) 2 pe m= ∑ e (∑ ℓ b(ℓ) e√pe )2 ∑ e (√pe )2 ≥ (∑ e,ℓ b(ℓ) e )2 . The equality is achieved when ⏐⏐⏐ ∑ ℓ b(ℓ) e√pe ⏐⏐⏐∝√pe. i.e., variance is minimized when pe ∝ ⏐⏐⏐∑ ℓ b(ℓ) e ⏐⏐⏐. It directly follows that: pe = m ∑ e′ ⏐⏐⏐∑ ℓ b(ℓ) e′ ⏐⏐⏐ ⏐⏐⏐⏐⏐ ∑ ℓ b(ℓ) e ⏐⏐⏐⏐⏐ For the multi-dimensional case of b(ℓ) e , following similar steps as above, it is easy to show that the optimal edge probability to minimize ∑ i Var(ζi) (where iis the index for ζ’s dimensions) is: pe = m ∑ e′ ∑ ℓ b(ℓ) e′   ∑ ℓ b(ℓ) e  B S AMPLING ALGORITHM Algorithm 2 lists the four graph samplers we have integrated into GraphSAINT. The naming of the samplers follows that of Table 2. Note that the sampling parameters nand mspecify a budget rather than the actual number of nodes and edges in the subgraph Gs. Since certain nodes or edges in the training graph Gmay be repeatedly sampled under a single invocation of the sampler, we often have |Vs|<n for node and MRW samplers, |Vs|<2mfor edge sampler, and |Vs|<r ·hfor RW sampler. Also note that the edge sampler presented in Algorithm 2 is an approximate version of the independent edge sampler deﬁned in Section 3.4. Complexity (excluding the subgraph induction step) of the original version in Section 3.4 is O(|E|), while complexity of the approximate one is O(m). When m≪|E|, the approximate version leads to identical accuracy as the original one, for a given m. C D ETAILED EXPERIMENTAL SETUP C.1 H ARDWARE SPECIFICATION AND ENVIRONMENT We run our experiments on a single machine with Dual Intel Xeon CPUs (E5-2698 v4 @ 2.2Ghz), one NVIDIA Tesla P100 GPU (16GB of HBM2 memory) and 512GB DDR4 memory. The code is written in Python 3.6.8 (where the sampling part is written with Cython 0.29.2). We use Tensorﬂow 1.12.0 on CUDA 9.2 with CUDNN 7.2.1 to train the model on GPU. Since the subgraphs are sampled independently, we run the sampler in parallel on 40 CPU cores. 12Published as a conference paper at ICLR 2020 Algorithm 2Graph sampling algorithms by GraphSAINT Input: Training graph G(V,E); Sampling parameters: node budget n; edge budget m; number of roots r; random walk length h Output: Sampled graph Gs (Vs,Es) 1: function NODE (G,n) ⊿Node sampler 2: P(v) := ˜A:,v  2 /∑ v′∈V ˜A:,v′  2 3: Vs ←nnodes randomly sampled (with replacement) from Vaccording to P 4: Gs ←Node induced subgraph of Gfrom Vs 5: end function 6: function EDGE (G,m) ⊿Edge sampler (approximate version) 7: P((u,v)) := ( 1 deg(u) + 1 deg(v) ) /∑ (u′,v′)∈E ( 1 deg(u′) + 1 deg(v′) ) 8: Es ←medges randomly sampled (with replacement) from Eaccording to P 9: Vs ←Set of nodes that are end-points of edges in Es 10: Gs ←Node induced subgraph of Gfrom Vs 11: end function 12: function RW(G,r,h) ⊿Random walk sampler 13: Vroot ←rroot nodes sampled uniformly at random (with replacement) from V 14: Vs ←Vroot 15: for v∈Vroot do 16: u←v 17: for d= 1to hdo 18: u←Node sampled uniformly at random from u’s neighbor 19: Vs ←Vs ∪{u} 20: end for 21: end for 22: Gs ←Node induced subgraph of Gfrom Vs 23: end function 24: function MRW(G,n,r) ⊿Multi-dimensional random walk sampler 25: VFS ←rroot nodes sampled uniformly at random (with replacement) from V 26: Vs ←VFS 27: for i= r+ 1to ndo 28: Select u∈VFS with probability deg(u)/∑ v∈VFS deg(v) 29: u′←Node randomly sampled from u’s neighbor 30: VFS ←(VFS \\{u}) ∪{u′} 31: Vs ←Vs ∪{u} 32: end for 33: Gs ←Node induced subgraph of Gfrom Vs 34: end function 13Published as a conference paper at ICLR 2020 100 101 102 103 104 105 10−6 10−4 10−2 100 Degree P(degree ≥k) PPI Flickr Reddit Yelp Amazon Figure 5: Degree Distribution C.2 A DDITIONAL DATASET DETAILS Here we present the detailed procedures to prepare the Flickr, Yelp and Amazon datasets. The Flickr dataset originates from NUS-wide §. The SNAP website ¶collected Flickr data from four different sources including NUS-wide, and generated an un-directed graph. One node in the graph represents one image uploaded to Flickr. If two images share some common properties (e.g., same geographic location, same gallery, comments by the same user, etc.), there is an edge between the nodes of these two images. We use as the node features the 500-dimensional bag-of-word representation of the images provided by NUS-wide. For labels, we scan over the 81 tags of each image and manually merged them to 7 classes. Each image belongs to one of the 7 classes. The Yelp dataset is prepared from the rawjson data of businesses, users and reviews provided in the open challenge website∥. For nodes and edges, we scan the friend list of each user in the raw json ﬁle of users. If two users are friends, we create an edge between them. We then ﬁlter out all the reviews by each user and separate the reviews into words. Each review word is converted to a 300-dimensional vector using the Word2Vec model pre-trained on GoogleNews∗∗. The word vectors of each node are added and normalized to serve as the node feature (i.e., xv). As for the node labels, we scan the raw json ﬁle of businesses, and use the categories of the businesses reviewed by a user vas the multi-class label of v. For the Amazon dataset, a node is a product on the Amazon website and an edge (u,v) is created if products uand vare bought by the same customer. Each product contains text reviews (converted to 4-gram) from the buyer. We use SVD to reduce the dimensionality of the 4-gram representation to 200, and use the obtained vectors as the node feature. The labels represent the product categories (e.g., books, movies, shoes). Figure 5 shows the degree distribution of the ﬁve graphs. A point (k,p) in the plot means the probability of a node having degree at least kis p. C.3 A DDITIONAL DETAILS IN EXPERIMENTAL CONFIGURATION Table 4 summarizes the URLs to download the baseline codes. The optimizer for GraphSAINT and all baselines is Adam (Kingma & Ba, 2014). For all baselines and datasets, we perform grid search on the hyperparameter space deﬁned by: •Hidden dimension: {128,256,512} §http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm ¶https://snap.stanford.edu/data/web-flickr.html ∥https://www.yelp.com/dataset ∗∗https://code.google.com/archive/p/word2vec/ 14Published as a conference paper at ICLR 2020 Table 4: URLs and commit number to run baseline codes Baseline URL Commit Vanilla GCN github.com/williamleif/GraphSAGE a0fdef GraphSAGE github.com/williamleif/GraphSAGE a0fdef FastGCN github.com/matenure/FastGCN b8e6e6 S-GCN github.com/thu-ml/stochastic_gcn da7b78 AS-GCN github.com/huangwb/AS-GCN 5436ec ClusterGCNgithub.com/google-research/google-research/tree/master/cluster_gcn99021e Table 5: Training conﬁguration of GraphSAINT for Table 2 Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length Node PPI 0.01 0.0 6000 — — — Flickr 0.01 0.2 8000 — — — Reddit 0.01 0.1 8000 — — — Yelp 0.01 0.1 5000 — — — Amazon 0.01 0.1 4500 — — — Edge PPI 0.01 0.1 — 4000 — — Flickr 0.01 0.2 — 6000 — — Reddit 0.01 0.1 — 6000 — — Yelp 0.01 0.1 — 2500 — — Amazon 0.01 0.1 — 2000 — — RW PPI 0.01 0.1 — — 3000 2 Flickr 0.01 0.2 — — 6000 2 Reddit 0.01 0.1 — — 2000 4 Yelp 0.01 0.1 — — 1250 2 Amazon 0.01 0.1 — — 1500 2 MRW PPI 0.01 0.1 8000 — 2500 — Flickr 0.01 0.2 12000 — 3000 — Reddit 0.01 0.1 8000 — 1000 — Yelp 0.01 0.1 2500 — 1000 — Amazon 0.01 0.1 4500 — 1500 — •Dropout: {0.0,0.1,0.2,0.3} • Learning rate: {0.1,0.01,0.001,0.0001} The hidden dimensions used for Table 2, Figure 2, Figure 3 and Figure 4 are: 512 for PPI, 256 for Flickr, 128 for Reddit, 512 for Yelp and 512 for Amazon. All methods terminate after a ﬁxed number of epochs based on convergence. We save the model producing the highest validation set F1-micro score, and reload it to evaluate the test set accuracy. For vanilla GCN and AS-GCN, we set the batch size to their default value 512. For GraphSAGE, we use the mean aggregator with the default batch size 512. For S-GCN, we set the ﬂag -cv -cvd (which stand for “control variate” and “control variate dropout”) with pre-computation of the ﬁrst layer aggregation. According to the paper (Chen et al., 2018a), such pre-computation signiﬁcantly reduces training time without affecting accuracy. For S-GCN, we use the default batch size 1000, and for FastGCN, we use the default value 400. For ClusterGCN, its batch size is determined by two parameters: the cluster size and the number of clusters per batch. We sweep the cluster size from 500 to 10000 with step 500, and the number of clusters per batch from {1,10,20,40}to determine the optimal conﬁguration for each dataset / architecture. Considering that for ClusterGCN, the cluster structure may be sensitive to the cluster size, and for FastGCN, the minibatch connectivity may increase with the sample size, we present additional experimental results to reveal the relation between accuracy and batch size in Appendix D.3. 15Published as a conference paper at ICLR 2020 Table 6: Training conﬁguration of GraphSAINT for Table 3 Arch. Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length 2×512 MRW PPI (large) 0.01 0.1 1500 — 300 — 5×2048 RW PPI (large) 0.01 0.1 — — 3000 2 2×128 Edge Reddit 0.01 0.1 — 6000 — — 4×128 Edge Reddit 0.01 0.2 — 11000 — — Table 7: Training conﬁguration of GraphSAINT for Figure 4 (Reddit) 2-layer GAT-SAINT 4-layer GAT-SAINT 2-layer JK-SAINT 4-layer JK-SAINT Hidden dimension 128 128 128 128 AttentionK 8 8 — — Aggregation⨁ — — Concat. Concat. Sampler RW RW Edge Edge (root: 3000; length: 2) (root: 2000; length: 4) (budget: 6000) (budget: 11000) Learning rate 0.01 0.01 0.01 0.01 Dropout 0.2 0.2 0.1 0.2 Conﬁguration of GraphSAINT to reproduce Table 2 results is shown in Table 5. Conﬁguration of GraphSAINT to reproduce Table 3 results is shown in Table 6. Below we describe the conﬁguration for Figure 4. The major difference between a normal GCN and a JK-net (Xu et al., 2018) is that JK-net has an additional ﬁnal layer that aggregates all the output hidden features of graph convolutional layers 1 to L. Mathematically, the additional aggregation layer outputs the ﬁnal embedding xJK as follows: xJK = σ ( WT JK · L⨁ ℓ=1 x(ℓ) v ) (7) where based on Xu et al. (2018), ⨁is the vector aggregation operator: max-pooling, concatenation or LSTM (Hochreiter & Schmidhuber, 1997) based aggregation. The graph attention of GAT (Veli ˇckovi´c et al., 2017) calculates the edge weights for neighbor aggregation by an additional neural network. With multi-head ( K) attention, the layer- (ℓ−1) features propagate to layer-(ℓ) as follows: x(ℓ) v =  K k=1 σ   ∑ u∈neighbor(v) αk u,vWkx(ℓ−1) v   (8) where ∥is the vector concatenation operation, and the coefﬁcient αis calculated with the attention weights ak by: αk u,v = LeakyReLU (( ak)T [ Wkxu∥Wkxv ]) (9) Note that the αcalculation is slightly different from the original equation in Veliˇckovi´c et al. (2017). Namely, GAT-SAINT does not normalize αby softmax across all neighbors of v. We make such modiﬁcation since under the minibatch setting, node vdoes not see all its neighbors in the training graph. The removal of softmax is also seen in the attention design of Huang et al. (2018). Note that during the minibatch training, GAT-SAINT further applies another edge coefﬁcient on top of attention for aggregator normalization. Table 7 shows the conﬁguration of the GAT-SAINT and JK-SAINT curves in Figure 4. 16Published as a conference paper at ICLR 2020 2 3 4 5 60 2 4 6 8 GCN depth Normalized training time GraphSAINT: Reddit S-GCN: Reddit GraphSAINT: Yelp S-GCN: Yelp Figure 6: Comparison of training efﬁciency PPI Flickr Reddit Yelp Amazon 0 0.5 1 1.5 Fraction of training time Node Edge RW MRW Figure 7: Fraction of training time on sampling D A DDITIONAL EXPERIMENTS D.1 T RAINING EFFICIENCY ON DEEP MODELS We evaluate the training efﬁciency for deeper GCNs. We only compare with S-GCN, since implemen- tations for other layer sampling based methods have not yet supported arbitrary model depth. The batch size and hidden dimension are the same as Table 2. On the two large graphs (Reddit and Yelp), we increase the number of layers and measure the average time per minibatch execution. In Figure 6, training cost of GraphSAINT is approximately linear with GCN depth. Training cost of S-GCN grows dramatically when increasing the depth. This reﬂects the “neighbor explosion” phenomenon (even though the expansion factor of S-GCN is just 2). On Yelp, S-GCN gives “out-of-memory” error for models beyond 5 layers. D.2 C OST OF SAMPLING AND PRE-PROCESSING Cost of graph samplers ofGraphSAINT Graph sampling introduces little training overhead. Let ts be the average time to sample one subgraph on a multi-core machine. Let tt be the average time to perform the forward and backward propagation on one minibatch on GPU. Figure 7 shows the ratio ts/tt for various datasets. The parameters of the samplers are the same as Table 2. For Node, Edge and RW samplers, we observe that time to sample one subgraph is in most cases less than 25% of the training time. The MRW sampler is more expensive to execute. Regarding the complete pre-processing procedure, we repeatedly run the sampler for N = 50·|V| /|Vs|times before training, to estimate the node and edge probability as discussed in Section 3.2 (where |Vs|is the average subgraph size). These sampled subgraphs are reused as training minibatches. Thus, if training runs for more than N iterations, the pre-processing is nearly zero-cost. Under the setting of Table 2, pre-processing on PPI and Yelp and Amazon does not incur any overhead in training time. Pre-processing on Flickr and Reddit (with RW sampler) takes less than 40% and 15% of their corresponding total training time. Cost of layers sampler of AS-GCNAS-GCN uses an additional neural network to estimate the conditional sampling probability for the previous layer. For a node v already sampled in layer ℓ, features of layer-(ℓ−1) corresponding to all v’s neighbors need to be fed to the sampling neural network to obtain the node probability. For sake of analysis, assume the sampling network is a single layer MLP, whose weight WMLP has the same shape as the GCN weights W(ℓ). Then we can show, for a L-layer GCN on a degree-dgraph, per epoch training complexity of AS-GCN is approximately γ = (d·L) /∑L−1 ℓ=0 dℓ times that of vanilla GCN. For L = 2, we have γ ≈2. This explains the observation that AS-GCN is slower than vanilla GCN in Figure 2. Additional, Table 8 shows the training time breakdown for AS-GCN. Clearly, its sampler is much more expensive than the graph sampler of GraphSAINT. 17Published as a conference paper at ICLR 2020 Table 8: Per epoch training time breakdown for AS-GCN Dataset Sampling time (sec) Forward / Backward propagation time (sec) PPI 1.1 0.2 Flickr 5.3 1.1 Reddit 20.7 3.5 Cost of clustering of ClusterGCNClusterGCN uses the highly optimized METIS software††to perform clustering. Table 9 summarizes the time to obtain the clusters for the ﬁve graphs. On the large and dense Amazon graph, the cost of clustering increase dramatically. The pre-processing time of ClusterGCN on Amazon is more than 4×of the total training time. On the other hand, the sampling cost of GraphSAINT does not increase signiﬁcantly for large graphs (see Figure 7). Table 9: Clustering time of ClusterGCN PPI Flickr Reddit Yelp Amazon Time (sec) 2.2 11.6 40.0 106.7 2254.2 Taking into account the pre-processing time, sampling time and training time altogether, we sum- marize the total convergence time of GraphSAINT and ClusterGCN in Table 10 (corresponding to Table 2 conﬁguration). On graphs that are large and dense (e.g., Amazon), GraphSAINT achieves signiﬁcantly faster convergence. Note that both the sampling of GraphSAINT and clustering of ClusterGCN can be performed ofﬂine. Table 10: Comparison of total convergence time (pre-processing + sampling + training, unit: second) PPI Flickr Reddit Yelp Amazon GraphSAINT-Edge 91.0 7.0 16.6 273.9 401.0 GraphSAINT-RW 103.6 7.5 17.2 310.1 425.6 ClusterGCN 163.2 12.9 55.3 256.0 2804.8 D.3 E FFECT OF BATCH SIZE Table 11 shows the change of test set accuracy with batch sizes. For each row of Table 11, we ﬁx the batch size, tune the other hyperparameters according to Appendix C.3, and report the highest test set accuracy achieved. For GraphSAGE, S-GCN and AS-GCN, their default batch sizes (512,1000 and 512, respectively) lead to the highest accuracy on all datasets. For FastGCN, increasing the default batch size (from 400 to 4000) leads to noticeable accuracy improvement. For ClusterGCN, different datasets correspond to different optimal batch sizes. Note that the accuracy in Section 5.1 is already tuned by identifying the optimal batch size on a per graph basis. For FastGCN, intuitively, increasing batch size may help with accuracy improvement since the minibatches may become better connected. Such intuition is veriﬁed by the rows of 400 and 2000. However, increasing the batch size from 2000 to 4000 does not further improve accuracy signiﬁcantly. For ClusterGCN, the optimal batch size depends on the cluster structure of the training graph. For PPI, small batches are better, while for Amazon, batch size does not have signiﬁcant impact on accuracy. For GraphSAGE, overly large batches may have negative impact on accuracy due to neighbor explosion. Approximately, GraphSAGE expand 10×more neighbors per layer. For a 2-layer GCN, a size 2 ×103 minibatch would then require the support of 2 ×105 nodes from the ††http://glaros.dtc.umn.edu/gkhome/metis/metis/download ∗Default batch size ¶The training does not converge. ‡The codes throw runtime error on the large datasets (Yelp or Amazon). 18Published as a conference paper at ICLR 2020 Table 11: Test set F1-micro for the baselines under various batch sizes Method Batch size PPI Flickr Reddit Yelp Amazon GraphSAGE 256 0.600 0.474 0.934 0.563 0.428 512∗ 0.637 0.501 0.953 0.634 0.758 1024 0.610 0.482 0.935 0.632 0.705 2048 0.625 0.374 0.936 0.563 0.447 FastGCN 400∗ 0.513 0.504 0.924 0.265 0.174 2000 0.561 0.506 0.934 0.255 0.196 4000 0.564 0.507 0.934 0.260 0.195 S-GCN 500 0.519 0.462 — ¶ — ¶ — ‡ 1000∗ 0.963 0.482 0.964 0.640 — ‡ 2000 0.646 0.482 0.949 0.614 — ‡ 4000 0.804 0.482 0.949 0.594 — ‡ 8000 0.694 0.481 0.950 0.613 — ‡ AS-GCN 256 0.682 0.504 0.950 — ‡ — ‡ 512∗ 0.687 0.504 0.958 — ‡ — ‡ 1024 0.687 0.502 0.951 — ‡ — ‡ 2048 0.670 0.502 0.952 — ‡ — ‡ ClusterGCN 500 0.875 0.481 0.942 0.604 0.752 1000 0.831 0.478 0.947 0.602 0.756 1500 0.865 0.480 0.954 0.602 0.752 2000 0.828 0.469 0.954 0.609 0.759 2500 0.849 0.476 0.954 0.598 0.745 3000 0.840 0.473 0.954 0.607 0.754 3500 0.846 0.473 0.952 0.602 0.754 4000 0.853 0.472 0.949 0.605 0.756 input layer. Note that the full training graph size of Reddit is just around 1.5 ×105. Thus, no matter which nodes are sampled in the output layer, GraphSAGE would almost always propagate features within the full training graph for initial layers. We suspect this would lead to difﬁculties in learning. For S-GCN, with batch size of 500, it fails to learn properly on Reddit and Yelp. The accuracy ﬂuctuates in a region of very low value, even after appropriate hyperparameter tuning. For AS-GCN, its accuracy is not sensitive to the batch size, since AS-GCN addresses neighbor explosion and also ensures good inter-layer connectivity within the minibatch. 19",
      "meta_data": {
        "arxiv_id": "1907.04931v4",
        "authors": [
          "Hanqing Zeng",
          "Hongkuan Zhou",
          "Ajitesh Srivastava",
          "Rajgopal Kannan",
          "Viktor Prasanna"
        ],
        "published_date": "2019-07-10T21:11:13Z",
        "pdf_url": "https://arxiv.org/pdf/1907.04931v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "GraphSAINT addresses the scalability and accuracy challenges of training Graph Convolutional Networks (GCNs) on large graphs with deep layers, particularly the \"neighbor explosion\" problem during minibatch training. Unlike prior layer-sampling methods, GraphSAINT proposes a graph sampling-based inductive learning approach where minibatches are constructed by first sampling a subgraph from the training graph and then building a complete GCN on that subgraph. Key contributions include resolving neighbor explosion, proposing normalization techniques to eliminate bias from non-uniform sampling probabilities, designing light-weight sampling algorithms for variance reduction, and demonstrating flexibility by integrating with various GCN architectures (e.g., GAT, JK-net). GraphSAINT achieves superior performance in both accuracy and training time on five large graphs, setting new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).",
        "methodology": "GraphSAINT constructs minibatches by directly sampling the training graph G to create subgraphs Gs, on which a full GCN is built and trained. This fundamentally differs from layer-sampling methods that construct a GCN on the full graph and then sample nodes/edges across layers. The training algorithm involves pre-processing to estimate node and edge sampling probabilities (pv, pu,v), which are then used to compute normalization coefficients: αu,v = pu,v/pv for aggregator normalization (feature propagation) and λv = |V|·pv for loss normalization, ensuring unbiased minibatch estimators. For variance reduction, optimal edge probabilities (pe) are derived (pe ∝ |∑ℓ b(ℓ)e|), simplified to pe ∝ 1/deg(u) + 1/deg(v) for topology-based samplers. GraphSAINT integrates several light-weight samplers: Random Node Sampler (sampling based on node degrees), Random Edge Sampler (sampling based on edge degrees), Random Walk Sampler (RW, starting with root nodes and expanding via random walks), and Multi-dimensional Random Walk Sampler (MRW). The process includes an induction step to add more connections into the subgraph, empirically improving convergence. The method is designed to be decoupled from forward/backward propagation, allowing seamless integration with GCN architecture variants like Jumping Knowledge Networks (JK-net) and Graph Attention Networks (GAT).",
        "experimental_setup": "GraphSAINT was evaluated in an inductive, supervised learning setting on five large graph datasets: PPI (protein-protein interaction, 121 multi-class), Flickr (image categorization, 7 single-class), Reddit (online posts community prediction, 41 single-class), Yelp (business categorization, 100 multi-class), and Amazon (product category classification, 107 multi-class). All datasets used fixed-partition splits for training, validation, and testing. The primary evaluation metric was F1-micro score. GraphSAINT's performance was compared against six baselines: Vanilla GCN, GraphSAGE, FastGCN, S-GCN, AS-GCN, and ClusterGCN, using their official code implementations. Experiments were conducted on a single machine with Dual Intel Xeon CPUs and an NVIDIA Tesla P100 GPU, using Python 3.6.8 and TensorFlow 1.12.0. Hyperparameters (hidden dimension, dropout, learning rate) were optimized via grid search based on validation set F1-micro score. Training time measurements excluded data loading, pre-processing, validation, and model saving. The cost of sampling and pre-processing for GraphSAINT and baselines was analyzed separately.",
        "limitations": "The ideal variance reduction sampler requires computing ˜x(ℓ-1)v, which can increase sampling complexity; a practical simplification based on graph topology is used instead. The Multi-dimensional Random Walk (MRW) sampler is observed to be more computationally expensive than other samplers integrated into GraphSAINT. Although GraphSAINT's pre-processing overhead is often minimal (especially if sampled subgraphs are reused for many training iterations), it can incur a notable percentage of total training time for certain datasets. While GraphSAINT resolves neighbor explosion, some baselines like S-GCN still suffer from dramatically increasing training costs with deeper GCNs, leading to out-of-memory errors on large graphs. Similarly, ClusterGCN's pre-processing (clustering) can be very expensive for large and dense graphs, sometimes exceeding total training time.",
        "future_research_directions": "Future work could focus on developing distributed training algorithms that leverage graph sampling-based minibatches. This involves partitioning the training graph across multiple processors, performing independent sampling, and devising data shuffling strategies to ensure overall convergence quality while minimizing system-level communication costs. Another direction is to explore algorithm-system co-optimization to accelerate GraphSAINT's training on heterogeneous computing platforms (e.g., CPU-FPGA). The inherent reduction in training computation complexity and improved hardware utilization due to less data traffic could be further enhanced, and task-level parallelization could be leveraged due to the decoupled nature of graph sampling and GCN layer propagation."
      }
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of accelerating Graph Neural Network (GNN) training by optimizing sampling variance. Existing variance reduction sampling algorithms are suboptimal for GCNs and inapplicable to more general GNNs like Graph Attention Networks (GAT) due to the intractable computation of optimal sampling distributions involving changing node embeddings and learned weights. The authors formulate the sampling variance optimization as an adversary bandit problem, where rewards are related to embeddings and learned weights, varying constantly. They propose novel bandit samplers that maintain nonparametric estimates and update the sampler iteratively. Theoretically, their algorithm asymptotically approaches the optimal variance within a factor of 3. Empirically, their approach demonstrates superior convergence, faster rates, and lower sample variances on multiple datasets compared to state-of-the-art methods, working for both GCNs and attentive GNNs.",
        "methodology": "The optimization of sampling variance is formulated as an adversary bandit problem. The core idea is to maintain nonparametric estimates of the sampler and update it towards optimal variance after acquiring partial knowledge about sampled neighbors. The reward for choosing a set of neighbors is defined as the negative derivative of the effective variance. Two bandit algorithms are proposed: 1. GNN-BS, based on the adversary Multi-Armed Bandit (MAB) setting, where a single neighbor is sampled k times, and the policy is updated by EXP3. 2. GNN-BS.M, based on MAB with multiple plays, which uses an efficient k-combination sampler (DepRound) to sample a k-element subset of neighbors once, and updates the policy by EXP3.M. For attentive GNNs where attention values (alpha_ij) are learned and cannot be evaluated with only sampled neighborhoods, an adjusted feedback attention value (alpha'_ij) is defined using a surrogate sum of sampled unnormalized attentions.",
        "experimental_setup": "Experiments were conducted on five benchmark datasets: Cora, Pubmed, PPI, Reddit, and Flickr, along with the OGB protein dataset. The GNN architectures used for comparison include GCN, GAT, and GeniePath. The number of layers was fixed at 2, and hidden embedding dimensions were set to 16 for Cora/Pubmed and 256 for PPI/Reddit/Flickr. For attentive GNNs, a single multi-head was used. Performance was evaluated using Micro F1 scores on test data and convergence rates on validation data (epochs and time). Comparison algorithms included GraphSAGE, FastGCN, LADIES, AS-GCN, S-GCN, ClusterGCN, and GraphSAINT, as well as their attentive GNN counterparts (AS-GAT, GraphSAINT-GAT). Hyperparameters such as learning rate, L2-norm regularizers, and dropout rate were optimized via grid search. Sample sizes (k) were set to 1 for Cora/Pubmed, 5 for Flickr, and 10 for PPI/Reddit. Batch sizes were 256 for layer sampling methods and S-GCN. OGB protein dataset parameters were: learning rate 1e-3, batch size 256, hidden dimension 64, sample size 10, epochs 200.",
        "limitations": "The optimal sampling distribution is computationally intractable because it requires full knowledge of neighbors' hidden embeddings or learned weights, which are not known a priori and change during training. Existing importance sampling methods ignore embedding norms and assume fixed weights, making them suboptimal and only applicable to GCNs, not attentive GNNs. The rigorous MAB setting requires exactly one action and policy update, making the GNN-BS approach (repeating 1-arm selection k times) not strictly rigorous, though GNN-BS.M (multiple plays) is. For GNN-BS.M, the effective variance is approximated using Jensen's inequality, which might introduce some looseness. The choice of adversary bandit setting leaves other bandit types for future study. Graph sampling approaches, while potentially faster under specific conditions, are limited as they assume all vertices have labels and their performance can be sensitive to graph partitioning.",
        "future_research_directions": "Future work includes extending the bandit sampler derivations, which currently follow node-wise samplers, to layer-wise sampling approaches. Another direction is to study other bandit problem settings beyond the adversary bandit setting explored in this paper."
      }
    },
    {
      "title": "Accelerating Gossip SGD with Periodic Global Averaging",
      "abstract": "Communication overhead hinders the scalability of large-scale distributed\ntraining. Gossip SGD, where each node averages only with its neighbors, is more\ncommunication-efficient than the prevalent parallel SGD. However, its\nconvergence rate is reversely proportional to quantity $1-\\beta$ which measures\nthe network connectivity. On large and sparse networks where $1-\\beta \\to 0$,\nGossip SGD requires more iterations to converge, which offsets against its\ncommunication benefit. This paper introduces Gossip-PGA, which adds Periodic\nGlobal Averaging into Gossip SGD. Its transient stage, i.e., the iterations\nrequired to reach asymptotic linear speedup stage, improves from\n$\\Omega(\\beta^4 n^3/(1-\\beta)^4)$ to $\\Omega(\\beta^4 n^3 H^4)$ for non-convex\nproblems. The influence of network topology in Gossip-PGA can be controlled by\nthe averaging period $H$. Its transient-stage complexity is also superior to\nLocal SGD which has order $\\Omega(n^3 H^4)$. Empirical results of large-scale\ntraining on image classification (ResNet50) and language modeling (BERT)\nvalidate our theoretical findings.",
      "full_text": "Accelerating Gossip SGD with Periodic Global Averaging Yiming Chen * 1 Kun Yuan* 1 Yingya Zhang 1 Pan Pan 1 Yinghui Xu 1 Wotao Yin1 Abstract Communication overhead hinders the scalability of large-scale distributed training. Gossip SGD, where each node averages only with its neigh- bors, is more communication-efﬁcient than the prevalent parallel SGD. However, its convergence rate is reversely proportional to quantity 1 −β which measures the network connectivity. On large and sparse networks where 1 −β → 0, Gossip SGD requires more iterations to converge, which offsets against its communication beneﬁt. This paper introduces Gossip-PGA, which adds Periodic Global Averaging into Gossip SGD. Its transient stage, i.e., the iterations required to reach asymptotic linear speedup stage, improves from Ω(β4n3/(1−β)4) to Ω(β4n3H4) for non-convex problems. The inﬂuence of network topology in Gossip-PGA can be controlled by the averaging period H. Its transient-stage complexity is also superior to Local SGD which has orderΩ(n3H4). Empirical results of large-scale training on image classiﬁcation (ResNet50) and language modeling (BERT) validate our theoretical ﬁndings. 1. Introduction The scale of deep learning nowadays calls for efﬁcient large- scale distributed training across multiple computing nodes in the data-center clusters. In distributed optimization, a network of nnodes cooperate to solve the problem min x∈Rd 1 n n∑ i=1 [fi(x) := Eξi∼DiFi(x; ξi)] (1) where each component fi is local and private to node iand the random variable ξi denotes the local data that follows distribution Di. We assume each node ican locally evaluate stochastic gradients ∇Fi(x; ξi) where ξi ∼Di, but must communicate to access information from other nodes. *Equal contribution 1Alibaba Group, Hangzhou, China. Corre- spondence to: Kun Yuan <kun.yuan@alibaba-inc.com>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). METHOD EPOCH ACC.% T IME (HRS .) PARALLEL SGD 120 76.26 2.22 GOSSIP SGD ( RING ) 120 74.86 1.56 GOSSIP SGD ( EXPO ) 120 75.34 1.55 GOSSIP SGD ( RING ) 240 75.62 3.02 GOSSIP SGD ( EXPO ) 240 76.18 3.03 Table 1.Top-1 validation accuracy for ImageNet with 256 GPUs connected with the ring or one-peer exponential network. Gossip SGD takes more time to reach the same accuracy as Parallel SGD. Parallel SGD methods are leading algorithms to solve (1), in which every node processes local training samples inde- pendently, and synchronize gradients every iteration either using a central Parameter Server (PS)(Li et al., 2014) or the All-Reduce communication primitive (Patarasuk & Yuan, 2009). The global synchronization in Parallel SGD either incurs signiﬁcant bandwidth cost or high latency, which hampers the training scalability. Many alternative methods have been proposed to reduce communication overhead in distributed training. Gossip SGD, also known as decentralized SGD (Nedic & Ozdaglar, 2009; Chen & Sayed, 2012; Lian et al., 2017; 2018; Ass- ran et al., 2019), recently received lots of attention. This line of work lets each node communicate with (some of) their direct neighbors. In a sparse topology such as one-peer exponential graph (Assran et al., 2019), each node only com- municates with one neighbor each time. This gossip-style communication is much faster than PS and All-Reduce but the computed average can be highly inaccurate. Local SGD (Stich, 2019; Yu et al., 2019; Lin et al., 2018) is another line of work that increases the computation-to-communication ratio. Local SGD lets each node to run local gradient de- scent for multiple rounds and only average their parameters globally once in a while. By communicating less frequently, Local SGD reduces the communication overhead. The reduced communication in Gossip and Local SGDs comes at a cost: slower convergence rate. While both al- gorithms are proved to have convergence linear speedup asymptotically, they are sensitive to network topology and synchronization period, respectively. For Gossip SGD, the convergence rate is inversely proportional to 1 −β (β is deﬁned in Remark 1). Since β →1 on the large and sparse network topology which is most valuable for deep training, Gossip SGD will converge very slow and require more iter- ations than Parallel SGD to achieve a desired solution. This arXiv:2105.09080v1  [cs.LG]  19 May 2021Accelerating Gossip SGD with Periodic Global Averaging GOSSIP SGD G OSSIP -PGA IID NON -IID IID NON -IID (PROPOSED ) SMALL OR DENSE NETWORK (WHEN 1 1−β <H ) Ω( n3β4 (1−β)2 ) Ω( n3β4 (1−β)4 ) Ω( n3β4C2 β) Ω( n3β4C2 β (1−β)2 ) LARGE OR SPARSE NETWORK (WHEN 1 1−β ≥H) Ω( n3β4 (1−β)2 ) Ω( n3β4 (1−β)4 ) Ω( n3β4C2 β) Ω( n3β4C2 βH2) Table 2.The lengths of the transient stages of Gossip SGD and Gossip-PGA. Since Cβ = ∑H−1 k=0 βk = (1 −βH)/(1 −β) < min{1/(1 −β),H}, Gossip-PGA always has shorter transient stage, more evident on large and sparse networks where β →1. LOCAL SGD G OSSIP -PGA IID SCENARIO Ω(n3H2) Ω( n3β4C2 β) NON -IID SCENARIO Ω(n3H4) Ω( n3β4C2 βH2) Table 3.The lengths of the transient stages of Local SGD and Gossip-PGA. Gossip-PGA always has shorter transient stages than Local SGD since β <1 and Cβ <H . Such superiority becomes more signiﬁcant on well-connected networks where β →0. may nullify its communication efﬁciency and result in even more training time (see Table 1). Local SGD with a large averaging period meets the same issue. This paper proposes Gossip-PGA, which adds periodic All- Reduce global averaging into Gossip to accelerate its conver- gence especially on large and sparse networks. Gossip-PGA also extends Local SGD with fast gossip-style communica- tion after local updates. When the same averaging period H is used, the additional gossip communication in Gossip- PGA endows it with faster convergence than Local SGD. Challenges. Gossip-PGA can be regarded as a special form of the topology-changing Gossip SGD (Koloskova et al., 2020) and SlowMo (Wang et al., 2019) (in which the base optimizer is set as Gossip SGD, and the momentum coefﬁ- cient β = 0). However, its theory and practical performance were not carefully investigated in literature. Unanswered im- portant questions include how much acceleration can PGA bring to Gossip and Local SGDs, in what scenario can PGA beneﬁts most, how to adjust the averaging period effectively, and how Gossip-PGA performs in large-scale deep learning systems. Providing quantitative answers to these questions requires new understanding on the interplay between gossip communication and global averaging period. Simply follow- ing existing analysis in (Koloskova et al., 2020) will result in incomplete conclusions, see Remark 5. Also, the analysis in SlowMo (Wang et al., 2019) does not consider heteroge- neous data distributions and cannot cover our results. 1.1. Main Results This paper proves that Gossip-PGA converges at O ( σ√ nT SGD rate + C 1 3 ββ 2 3 (σ 2 3 + D 1 3 βb 2 3 ) T 2 3 + βDβ T )    Extra overhead (2) for both smooth convex and non-convex functions fi (the metrics used for both scenarios can be referred to Theorems 1 and 2), where nis the network size, T is the total number of iterations, σ2 denotes gradient noise, b2 gauges data heterogeneity, β ∈(0,1) measures how well the network is connected, H is the global averaging period, and we deﬁne Cβ = ∑H−1 k=0 βk and Dβ = min{H,1/(1 −β)}. Linear speedup. When T is sufﬁciently large, the ﬁrst term 1/ √ nT dominates (2). This also applies to Parallel, Local, and Gossip SGDs. Gossip-PGA and these algorithms all require T = Ω(1 /(nϵ2)) iterations to reach a desired accuracy ϵ, which is inversely proportional to n. We say an algorithm is in its linear-speedup stage at Tth iteration if, for this T, the term involving nT is dominating the rate. Transient stage. Transient stage is referred to those itera- tions before an algorithm reaches its linear-speedup stage, that is iterations 1,...,T where T is relatively small so non- nT terms (i.e., the extra overhead terms in (2)) still domi- nate the rate. We take Gossip-PGA in the non-iid scenario (b2/3 ≥σ) as example. To reach linear speedup, T has to satisfy T 2 3 /(C 1 3 ββ 2 3 D1/3 β ) ≥n 1 2 T 1 2 , i.e., T ≥n3β4C2 βD2 β. So, the transient stage has Ω(n3β4C2 βD2 β) iterations. Tran- sient stage is an important metric to measure the scala- bility of distributed algorithms. Shorter transient stage than Gossip SGD . The transient stage comparison between Gossip SGD and Gossip-PGA is shown in Table 2. Since Cβ = (1 −βH)/(1 −β) < min{H,1/(1 −β)}, we conclude Gossip-PGA always has a shorter transient stage than Gossip SGD for any βand H. Moreover, the superiority of Gossip-PGA becomes evident when the network is large and sparse, i.e., 1 −β → 0. In this case, the transient stage of Gossip SGD can grow dramatically (see the second line in Table 2) while Gossip- PGA is controlled by the global period Hbecause Cβ <H . As a result, Gossip-PGA improves the transient stage of Gossip-SGD from O(n3/(1 −β)4) (or O(n3/(1 −β)2 in the iid scenario) to O(n3) when β →1. Shorter transient stage than Local SGD . The transient stage comparison between Local SGD and Gossip-PGA is shown in Table 3. Using Cβ <H , we ﬁnd Gossip-PGA is always endowed with a shorter transient stage than Local SGD. Moreover, when the network is well-connected suchAccelerating Gossip SGD with Periodic Global Averaging that β →0, it holds that Cβ →1. Gossip-PGA will have a signiﬁcantly shorter transient stage than Local SGD. 1.2. Contributions • We establish the convergence rate of Gossip-PGA for both smooth convex and non-convex problems. Our results clarify how gossip communication and periodic global averaging collaborate to improve the transient stage of Gossip and Local SGDs. We also established shorter wall-clock training times of Gossip-PGA. • We propose Gossip-AGA, which has adaptive global averaging periods. Gossip-AGA automatically adjusts H and has convergence guarantees. • We conduct various experiments (convex logistic re- gression and large-scale deep learning tasks) to vali- date all established theoretical results. In particular, the proposed Gossip-PGA/AGA achieves a similar conver- gence speed to parallel SGD in iterations, but provides 1.3 ∼1.9×runtime speed-up. The introduced global averaging steps in Gossip-PGA/AGA remedy the accu- racy degradation in Gossip SGD and Local SGD. 2. Related Work Decentralized optimization algorithms can be tracked back to (Tsitsiklis et al., 1986). After that, decentralized optimiza- tion has been intensively studied in signal processing and control community. Decentralized gradient descent (DGD) (Nedic & Ozdaglar, 2009), diffusion (Chen & Sayed, 2012) and dual averaging (Duchi et al., 2011) are among the ﬁrst decentralized algorithms that target on general optimization problems. However, these algorithms suffer from a bias caused by data heterogeneity (Yuan et al., 2016). Various primal-dual algorithms are proposed to overcome this issue, and they are based on alternating direction method of multi- pliers (ADMM) (Shi et al., 2014), explicit bias-correction (Shi et al., 2015; Yuan et al., 2019; Li et al., 2019c), gradient tracking (Xu et al., 2015; Di Lorenzo & Scutari, 2016; Nedic et al., 2017; Qu & Li, 2018), coordinate-descent methods (He et al., 2018), and dual acceleration (Scaman et al., 2017; 2018; Uribe et al., 2020). In the context of machine learning, decentralized SGD, also known as Gossip SGD, have gained a lot of attention re- cently. (Lian et al., 2017) ﬁrst proves Gossip SGD can reach the same linear speedup as vanilla parallel SGD. Af- ter that, (Assran et al., 2019) comes out to extend Gossip SGD to directed topology. A recent work (Koloskova et al., 2020) proposes a uniﬁed framework to analyze algorithms with changing topology and local updates. While it covers Gossip-PGA as a special form, the theoretical and practi- cal beneﬁts of periodic global averaging were not studied therein. The data heterogeneity issue suffered in Gossip SGD is discussed and addressed in (Tang et al., 2018; Yuan et al., 2020; Lu et al., 2019; Xin et al., 2020). Gossip SGD is also extended to asynchronous scenarios in (Lian et al., 2018; Luo et al., 2020). Local SGD can be traced back to (Zinkevich et al., 2010) which proposed a one-shot averaging. More frequent aver- aging strategy is proposed in (Zhang et al., 2016), and the convergence property of Local SGD is established in (Yu et al., 2019; Stich, 2019; Bayoumi et al., 2020). Local SGD is also widely-used in federated learning (McMahan et al., 2017; Li et al., 2019a). Another closely related work (Wang et al., 2019) proposes a slow momentum (SlowMo) framework, where each node, similar to the Gossip-PGA algorithm proposed in this pa- per, periodically synchronizes across the network and per- forms a momentum update. The analysis in SlowMo cannot cover the convergence results in this paper due to its data- homogeneous setting. In addition, we will clarify some new questions such as how much acceleration can PGA bring to Gossip and Local SGDs, and how to adjust the averaging period effectively. Various techniques can be integrated to Gossip SGD to improve its communication efﬁciency. This paper does not consider quantization (Alistarh et al., 2017; Bernstein et al., 2018), gradient compression (Tang et al., 2019; Koloskova et al., 2019b;a) and lazy communication (Chen et al., 2018; Liu et al., 2019), but these orthogonal techniques can be added to our methods. 3. Gossip SGD with Periodic Global Average Assume all computing nodes are connected over a graph G = {V,E}where V = {1,2,··· ,n}denote the node index and Edenote the communication links between all nodes. Similar to existing decentralized algorithms (Nedic & Ozdaglar, 2009; Chen & Sayed, 2012; Lian et al., 2017; Assran et al., 2019), information exchange in the gossip step is only allowed to occur between connected neighbors. To characterize the decentralized communication, we let W ∈Rn×n be a doubly stochastic matrix, i.e., W ≥0, W1n = 1n and 1T nW = 1T n. The (i,j)-th element wij is the weight to scale information ﬂowing from node j to node i. If nodes iand jare not neighbors then wij = 0, and if they are neighbors or identical then the weight wij >0. Furthermore, we deﬁne Ni as the set of neighbors of node i which also includes node iitself. The Gossip-PGA algorithm is listed in Algorithm 1. In the gossip step, every node icollects information from all its connected neighbors. For global average step, nodes synchronize their model parameters using the efﬁcient All- Reduce primitives. When H →∞, Gossip-PGA will re- duce to standard Gossip SGD; when W = 1 n11n, Gossip-Accelerating Gossip SGD with Periodic Global Averaging Algorithm 1 Gossip-PGA Require: Initialize learning rate γ >0, weight matrix W, global averaging period H, and let each x(0) i to be equivalent to each other. for k= 0,1,2,...,T −1, every node ido Sample ξ(k+1) i , update g(k) i =∇Fi(x(k) i ; ξ(k+1) i ) x (k+ 1 2 ) i = x(k) i −γg(k) i ⊿Local SGD update if mod(k+ 1,H) = 0 then x(k+1) i = 1 n ∑n j=1 x (k+ 1 2 ) j ⊿global average else x(k+1) i = ∑ j∈Ni wijx (k+ 1 2 ) j ⊿one gossip step PGA will reduce to vanilla parallel SGD; when W = I, Gossip-PGA will reduce to Local SGD. All-Reduce v.s. multiple Gossips. In a computing cluster with nnodes, global averaging is typically conducted in an efﬁcient Ring All-Reduce manner, rather than via multiple gossip steps as in (Berahas et al., 2018). The communi- cation time comparison between a single gossip and Ring All-Reduce step is listed in Appendix H. In the one-peer ex- ponential network, the exact global average can be achieved via ln(n) gossip communications, which generally takes more wall-clock time than a single Ring All-Reduce opera- tion. Therefore, we recommend exploiting All-Reduce to conduct global averaging in Gossip-PGA. Data-center v.s. wireless network. This paper considers deep training within high-performance data-center clusters, in which all GPUs are connected with high-bandwidth chan- nels and the network topology can be fully controlled. Under such setting, the periodic global averaging conducted with Ring All-Reduce has tolerable communication cost, see Ap- pendix H. For scenarios where global averaging is extremely expensive to conduct such as in wireless sensor network, the global averaging can be approximated via multiple gossip steps, or may not be recommended. 3.1. Assumptions and analysis highlights We now establish convergence rates for Gossip-PGA on smooth convex and non-convex problems. For all our theo- retical results we make the following standard assumptions. Assumption 1 (L-SMOOTHNESS ). Each local cost function fi(x) is differentiable, and there exists a constant Lsuch that for each x,y∈Rd: ∥∇fi(x) −∇fi(y)∥≤ L∥x−y∥. (3) Assumption 2 (GRADIENT NOISE ). Recall g(k) i is the stochastic gradient noise deﬁned in line 2 of Algorithm 1. It is assumed that for any kand ithat E[g(k) i −∇fi(x(k) i )|F(k−1)] = 0, (4) E[∥g(k) i −∇fi(x(k) i )∥2|F(k−1)] ≤σ2 (5) for some constant σ2 >0. Moreover, we assume ξ(k) i is in- dependent of each other for anykand i. Filtration is deﬁned as F(k)= { {x(k) i }n i=1,{ξ(k) i }n i=1,··· ,{x(0) i }n i=1,{ξ(0) i }n i=1 } Assumption 3 (WEIGHTING MATRIX ). The network is strongly connected and the weight matrix W satisﬁes W1n = 1n, 1T nW = 1T n, null(I −W) = span( 1n). We also assume ∥W −1 n11T∥2 ≤βfor some β ∈(0,1). Remark 1. Quantity β ∈ (0,1) indicates how well the topology is connected. Smaller βindicates better-connected network while larger βimplies worse-connected topology. Analysis highlights. To derive the inﬂuence of periodic global averaging, we have to exploit all useful algorithm structures to achieve its superiority. These structures are: • x(k) i = ¯x(k) when mod (k,H) = 0. This structure relieves the inﬂuence of network topology; • Gossip communications within each period also con- tribute to consensus among nodes. This structure is crucial to establish superiority to Local SGD; • When network is large and sparse, i.e., H < 1 1−β, the global averaging is more critical to drive consen- sus. This structure is crucial to establish superiority to Gossip SGD when H < 1 1−β. • When network is small or dense, i.e., H > 1 1−β, gos- sip communication is more critical to drive consensus. This structure is crucial to establish superiority to Gos- sip SGD when H > 1 1−β. Ignoring any of the above structures in the analysis will result in incomplete conclusions on comparison among Gossip-PGA, Gossip SGD and Local SGD. 3.2. Convergence analysis: convex scenario Assumption 4 (CONVEXITY ). Each fi(x) is convex. Deﬁnition 1 (DATA HETEROGENEITY ). When each fi(x) is convex, we let b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2 denote the data heterogeneity. When each local data follows the same distribution, it holds that fi(x) = f(x) ∀iand hence ∇fi(x⋆) = ∇f(x⋆) = 0 which also implies b2 = 0. With Assumption 4, we let x⋆ be one of the global solutions to problem (1).Accelerating Gossip SGD with Periodic Global Averaging GOSSIP SGD (K OLOSKOVA ET AL ., 2020) G OSSIP -PGA RATES (GENERAL FORM ) O ( σ√ nT+ β 2 3 σ 2 3 T 2 3 (1−β) 1 3 + β 2 3 b 2 3 T 2 3 (1−β) 2 3 + β (1−β)T ) O ( σ√ nT+ C 1 3 β β 2 3 σ 2 3 T 2 3 + C 1 3 β D 1 3 β β 2 3 b 2 3 T 2 3 +βDβ T ) RATES (WHEN 1 1−β <H ) O ( σ√ nT+ β 2 3 σ 2 3 T 2 3 (1−β) 1 3 + β 2 3 b 2 3 T 2 3 (1−β) 2 3 + β (1−β)T ) O ( σ√ nT+ C 1 3 β β 2 3 σ 2 3 T 2 3 + C 1 3 β β 2 3 b 2 3 (1−β) 1 3 T 2 3 + β (1−β)T ) RATES (WHEN 1 1−β ≥H) O ( σ√ nT+ β 2 3 σ 2 3 T 2 3 (1−β) 1 3 + β 2 3 b 2 3 T 2 3 (1−β) 2 3 + β (1−β)T ) O ( σ√ nT+ C 1 3 β β 2 3 σ 2 3 T 2 3 + C 1 3 β H 1 3 β 2 3 b 2 3 T 2 3 +βH T ) Table 4.Convergence rate comparison between Gossip SGD and Gossip-PGA for smooth convex/non-convex problems. We use notation b2 to indicate the data heterogeneity for both convex and non-convex scenarios. Theorem 1. Under Assumptions 1–4, if γis chosen as γ= min { 1 12βLDβ , ( r0 r1(T + 1) )1 2 , ( r0 r2(T + 1) )1 3} (6) with constants r0 = 2 E∥¯x(0) −x⋆∥2,r1 = 2 σ2/n, and r2 = 6Lβ2Cβσ2 + 18Lβ2CβDβ, it holds for any T that Ef(ˆx(T)) −f(x⋆) = O ( σ√ nT + C 1 3 ββ 2 3 (σ 2 3 + D 1 3 βb 2 3 ) T 2 3 + βDβ T ) (7) where ¯x(k) = 1 n ∑n i=1 x(k) i , ˆx(T) = 1 T+1 ∑T k=0 ¯x(k), Cβ = ∑H−1 k=0 βk and Dβ = min{H,1/(1 −β)}. (Proof is in Appendix B.) Remark 2. When β →0, i.e., the network tends to be fully connected, Gossip-PGA will converge at rateO(σ/ √ nT), which recovers the rate of parallel SGD. Remark 3. When β →1, i.e., the information exchange via gossip communication is inefﬁcient, it holds that Cβ →H and Dβ = min{H,1/(1 −β)}= H. Substituting them to (7) will recover the rate of Local SGD, see Table 6. Remark 4. When H → ∞, i.e., the networked agents tend not to conduct global synchronization, it holds that Cβ →1/(1 −β) and Dβ = 1 1−β. Substituting these values to (7) will recover the rate of Gossip SGD, see Table 4. 3.3. Convergence analysis: non-convex scenario We ﬁrst introduce an assumption about data heterogeneity speciﬁcally for non-convex problems: Assumption 5 (DATA HETEROGENEITY ). There exists con- stant ˆb >0 such that 1 n ∑n i=1 ∥∇fi(x) −∇f(x)∥2 ≤ˆb2 for any x∈Rd. If local data follows the same distribution, it holds that ˆb= 0. Theorem 2. Under Assumptions 1–3 and 5, ifγsatisﬁes the condition (6) (replace b2 with ˆb2 and use r0 = 4Ef(¯x(0))), it holds for any T >0 that 1 T + 1 T∑ k=0 E∥∇f(¯x(k))∥2 GOSSIP SGD G OSSIP -PGA TRANSIENT ITER . O(n7) O(n5) SINGLE COMM . O(θd+ α) O(θd+ √nα) TRANSIENT TIME O(n7θd+ n7α) O(n5θd+ n5.5α) Table 5.Transient time comparison between non-iid Gossip SGD and Gossip-PGA over the speciﬁc grid (1−β = O(1/n)) topology. We choose H = √nas the period in Gossip-PGA. = O ( σ√ nT + C 1 3 ββ 2 3 (σ 2 3 + D 1 3 βb 2 3 ) T 2 3 + βDβ T ) (8) where ¯x(k) = 1 n ∑n i=1 x(k) i . (Proof is in Appendix C.) 3.4. Comparison with Gossip SGD To better illustrate how periodic global averaging helps re- lieve the affects of network topology in Gossip SGD, we list convergence rates of Gossip SGD and Gossip-PGA for smooth convex or non-convex problems in Table 4. The ﬁrst line is the general rate expression for both algorithms. In the second line we let Dβ = min{H,1/(1 −β)}= 1/(1 −β) for Gossip-PGA, and in the third line we let Dβ = H. Ac- cording to this table, we derive the transient stages of Gossip SGD and Gossip-PGA for each scenarios (i.e., large/small network, iid/non-iid data distributions) in Table 2 (see the derivation detail in Appendix D). As we have explained in Main Results subsection in the introduction, it is observed from Tables 2 and 4 that: (i) Gossip-PGA always converges faster (or has shorter transient stages) than Gossip SGD for any β and H value. (ii) Such superiority gets evident for large and sparse networks where β →1. Remark 5. The convergence analysis in topology-changing Gossip SGD (Koloskova et al., 2020) covers Gossip-PGA. By letting p = 1 and τ = H in Theorem 2 of (Koloskova et al., 2020), it is derived that Gossip-PGA has a transient stage on the order of Ω(n3H4) for non-convex non-iid sce- nario. Such transient stage cannot quantify the superiority to Gossip and Local SGDs. In fact, it may even show PGA can do harm to Gossip SGD when H > 1 1−β, which is counter-intuitive. This is because (Koloskova et al., 2020) is for the general time-varying topology. It does not utilizeAccelerating Gossip SGD with Periodic Global Averaging RATES L-SGD O ( σ√ nT+H 1 3 σ 2 3 T 2 3 +H 2 3 b 2 3 T 2 3 +H T ) G-PGA O ( σ√ nT+ C 1 3 β β 2 3 σ 2 3 T 2 3 + C 1 3 β H 1 3 β 2 3 b 2 3 T 2 3 +βH T ) Table 6.Convergence rate comparison between Local SGD (L- SGD) and Gossip-PGA (G-PGA) over smooth convex/non-convex problems. The rate for Local SGD is from (Koloskova et al., 2020; Yu et al., 2019; Li et al., 2019b). the structures listed in Sec. 3.1. Transient stage in runtime . Table 2 compares transient stages between Gossip-PGA and Gossip SGD in iterations. But what people really care about in practice is runtime. Since both Gossip SGD and Gossip-PGA have the same computational overhead per iteration, we will focus on com- munication time spent in the transient stage. Given the bandwidth in a computing cluster with size n, we let αdenote the point-to-point latency in the network, and θ denote the communication time cost to transmit a scalar variable. Since variable x in problem (1) has di- mension d, it will take θdtime to transmit xbetween two nodes. Under this setting, the All-Reduce global averag- ing step will take 2θd + nα = O(θd + nα) time (see section 2.5 in (Ben-Nun & Hoeﬂer, 2019)). The gossip- style communication time varies with different network topologies. For the commonly-used ring or grid topology, it takes |Ni|θd+ α= O(θd+ α) for one gossip communi- cation, where |Ni|is the neighborhood size of node i, and |Ni|= 3 for the ring and 5 for the grid. As to Gossip-PGA, if we amortize the periodic All-Reduce cost into each com- munication, it will have |Ni|θd+ α+ (2θd+ nα)/H = O(θd+ √nα) when we set H = √n. With the formula Total time = transient stage (in iteration) ×comm. per iter. We calculate and compare the transient time between non- iid Gossip-PGA and Gossip-SGD (over the grid topology) in Table 5. Other comparisons for iid scenario or the ring topology can be found in Appendix D. It is observed in all tables that Gossip-PGA has shorter transient time. 3.5. Comparison with Local SGD The convergence rates of Gossip-PGA and Local SGD are listed in Table 6, from which we derive the transient stages of them in Table 3 (details are in Appendix D). As we have explained in the introduction, it is observed from Tables 3 and 6 that (i) Gossip-PGA always converges faster (or has shorter transient stages) than Local SGD for any βand H value, and (ii) Such superiority gets more evident for well-connected network where β →0. As to the wall-clock transient time of Local SGD, if we amortize the periodic All-Reduce cost into each local up- date, it will take (2θd+nα)/H = O(θd/H+nα/H) com- munication time per iteration. Using the transient iteration derived in Table 3, the total transient time for Local SGD (non-iid scenario) will be O(n3H3(θd+ nα)). Comparing it with the total transient time O(n3HC2 ββ4(Hθd + nα)) for Gossip-PGA, we ﬁnd Gossip-PGA always has shorter transient runtime for a large H >β4C2 β. Remark 6. While we discuss in detail that the transient time of Gossip-PGA is shorter than Gossip and Local SGDs, it is worth noting that the communication time during the linear speedup stage (i.e., after the transient stage) also contributes to the total training time. In this stage, Gossip- PGA is less efﬁcient due to its periodic global averaging. However, we illustrate that Gossip-PGA is always endowed with shorter total training time than Gossip and Local SGDs with extensive deep learning experiments in Sec. 5. 4. Gossip SGD with Adaptive Global Average Gossip-PGA suffers from the burden of tuning H by hand. A small Hwill incur more communication overhead while a large value can slow down the convergence. We further pro- pose Gossip-AGA, an adaptive extension of Gossip-PGA. Intuition. A small consensus variance ∑n i=1 E∥xi −¯x∥2 would accelerate Gossip-PGA. To see that, if∑n i=1 E∥xi− ¯x∥2 = 0 for each iteration, then Gossip-PGA reduces to parallel SGD and can reach its fastest convergence. Recall from Lemma 8 in the appendix that the aver- aged consensus 1 T+1 ∑T k=0 E∥x(k) −¯x(k)∥2 is bounded by d1γ2 T+1 ∑T k=0 E∥∇f(¯x(k))∥2+d2γ2 where d1 and d2 are constants. It is observed that the initial consensus vari- ance (when T is small) can be signiﬁcant due to large γ and E∥∇f(¯x(k))∥2. In the later stage when T is sufﬁ- ciently large, both the diminishing step-size γand gradient E∥∇f(¯x(k))∥2 go to 0 and hence leading to a small consen- sus variance naturally. With these observations, it is intuitive to take global synchronizations more frequently in initial stages to reduce the overall consensus variance. Convergence. We denote H(ℓ) as the duration of the ℓ-th period. The following corollary establishes convergence for Gossip-PGA with any time-varying but ﬁnite global averaging period sequence {H(ℓ)}: Corollary 1. Suppose Assumptions 1–3 and 5 hold and the time-varying period H(ℓ) is upper bounded by Hmax = maxℓ≥0{H(ℓ)}. If γ satisﬁes the condition in Theorem 1 with H = Hmax, then Gossip-AGA converges at rate(8) in which H is replaced by Hmax. (Proof is in Appendix E.) Adaptive Strategy. This subsection will propose an adap- tive strategy that is inspired by (Wang & Joshi, 2019). If we recover the inﬂuence of the initial value F0 = Ef(¯x(0)) on convergence rate (8), Gossip-PGA for non-convex problemsAccelerating Gossip SGD with Periodic Global Averaging will converge at O (σF 1 2 0√ nT + H 1 3 β 2 3 σ 2 3 F 2 3 0 T 2 3 + H 2 3 β 2 3 ˆb 2 3 F 2 3 0 T 2 3 + βDβF0 T ) . For a ﬁxed T, a period H = σ 3 2 T 1 4 /(βˆbF 1 4 0 n 3 4 ) will guar- antee the linear speedup. Therefore, the initial period H(0) can be chosen asH(0) = d1/[Ef(¯x(0))] 1 4 for some constant d1. Similarly, for the ℓ-th period, workers can be viewed as restarting training at a new initial point ¯x(Tℓ−1) where Tℓ−1 = H(0) + ··· + H(ℓ−1). As a result, the ℓ-th period H(ℓ) can be chosen as H(ℓ) = d1/[Ef(¯x(Tℓ−1))] 1 4 . With such choice of H(0) and H(ℓ), it is not difﬁcult to have H(ℓ) = ( Ef(¯x(0)) Ef(¯x(Tℓ−1)) )1 4 H(0). (9) Since Ef(¯x(k)) will decrease as kincreases, (9) will gener- ate an increasing sequence of period H(ℓ). We list Gossip- AGA as Algorithm 2 in Appendix G and elaborate on im- plementation details there. 5. Experimental Results In this section, we ﬁrst examine how the transient stage dif- fers for Gossip-PGA, Gossip and Local SGDs on networks with different topology and size on convex logistic regres- sion. Next, we systematically evaluate the aforementioned methods on two typical large-scale deep learning tasks: im- age classiﬁcation (over 256 GPUs) and language modeling (over 64 GPUs). See Appendix F for implementation details. 5.1. Logistic Regression We consider a distributed logistic regression problem with fi(x) = 1 M ∑M m=1 ln[1 + exp( −yi,mhi,m)Tx], where {hi,m,yi,m}M m=1 are local data samples at agent i with hi,m ∈Rd being the feature vector and yi,m ∈{+1,−1} being the corresponding label. Each hi,m is generated from the normal distribution N(0; 10Id). To generate yi,m, we ﬁrst generate an auxiliary random vector x⋆ i ∈Rd with each entry following N(0,1). Next, we generate yi,m from a uni- form distribution U(0,1). If yi,m ≤1/[1 + exp(−hT i,mx⋆ i)] then yi,m is set as +1; otherwise yi,m is set as −1. We let x⋆ i = x⋆ ∀i to generate data for iid scenario and x⋆ i ̸= x⋆ j ∀i,j for non-iid scenario. Each x⋆ i is normalized. Figure 1 compares how Gossip-PGA performs against paral- lel and Gossip SGD over the ring topology and non-iid data distribution. The network sizes are set as n= 20,50,100 which results in β = 0.967,0.995,0.998. We set d = 10 and M = 8000. H is set as 16 in Gossip-PGA. The step- size γ is initialized as 0.2 and gets decreased by half for every 1000 iterations. We repeat all simulations 50 times and illustrate the mean of all trials with solid curve and METHOD ACC.% H RS EPOCHS /HRS TO 76%. PARALLEL SGD 76.26 2.22 94 / 1.74 LOCAL SGD 74.20 1.05 N.A. LOCAL SGD ×3 75.41 3 N.A. GOSSIP SGD 75.34 1.55 N.A. GOSSIP SGD ×2 76.18 3 198/2.55 OSGP 75.04 1.32 N.A. OSGP ×2 76.07 2.59 212/2.28 GOSSIP -PGA 76.28 1.66 109/1.50 GOSSIP -AGA 76.25 1.57 91/1.20 Table 7.Comparison of Top-1 validation accuracy (Column 2) and wall-clock training time (Column 3) on different methods after ﬁnishing all epochs. We also report the epochs and training time required to reach 76% accuracy (Column 4). “N.A.” implies that the target accuracy is not reached when all epochs are completed. standard deviation with shaded area. It is observed that both Gossip SGD and Gossip-PGA will asymptotically converge at the same rate as parallel SGD (i.e., the linear speedup stage), albeit with different transient stages. Gossip-PGA always has shorter transient stages than Gossip SGD, and such superiority gets more evident when network size in- creases (recall that 1 −β = O(1/n2)). For experiments on different topologies such as grid and exponential graph, on iid data distribution, and comparison with Local SGD, see Appendix F. All experiments are consistent with the theoretical transient stage comparisons in Tables 2 and 3. 5.2. Image Classiﬁcation The ImageNet-1k (Deng et al., 2009) 1 dataset consists of 1,281,167 training images and 50,000 validation images in 1000 classes. We train ResNet-50 (He et al., 2016) model (∼25.5M parameters) following the training protocol of (Goyal et al., 2017). We train total 120 epochs. The learning rate is warmed up in the ﬁrst 5 epochs and is decayed by a factor of 10 at 30, 60 and 90 epochs. We set the period to 6 for both Local SGD and Gossip-PGA. In Gossip-AGA, the period is set to 4 initially and changed adaptively afterwards, roughly 9% iterations conduct global averaging. Table 7 shows the top-1 validation accuracy and wall-clock training time of aforementioned methods. It is observed both Gossip-PGA and Gossip-AGA can reach comparable accuracy with parallel SGD after all 120 epochs but with roughly 1.3x ∼1.4x training time speed-up. On the other hand, while local and Gossip SGD completes all120 epochs faster than Gossip-PGA/AGA and parallel SGD, they suffer from a 2.06% and 0.92% accuracy degradation separately. Moreover, both algorithms cannot reach the 76% top-1 accu- racy within 120 epochs. We also compare with OSGP (Ass- ran et al., 2019), which adding overlapping on the Gossip 1The usage of ImageNet dataset in this paper is for non- commercial research purposes only.Accelerating Gossip SGD with Periodic Global Averaging Figure 1.Convergence comparison between Gossip-PGA, Gossip and parallel SGDs on the logistic regression problem over ring topology. The transient stage is determined by counting iterations before an algorithm exactly matches the convergence curve of Parallel SGD. Note that the transient stage for Gossip SGD in the middle and right sub-ﬁgures is beyond the plotting canvas. 0 2500 5000 7500 10000 12500 15000 17500 Iterations 2 4 6Training Loss 16000 18000 0.8 1.0 Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA 0 1000 2000 3000 4000 5000 6000 7000 8000 Training Time 2 4 6Training Loss Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA Figure 2.Convergence results on the ImageNet in terms of iteration and runtime. More results are in Appendix F.3. SGD. We ﬁnd OSGP ×2, while faster than Gossip SGD×2, still needs more time than Gossip-PGA to achieve 76% ac- curacy. To further illustrate how much time it will take local and Gossip SGD to reach the target accuracy, we run another Local SGD and Gossip SGD experiments with ex- tended epochs (i.e., Gossip SGD ×2 trains total 240 epochs and the learning rate is decayed at 60, 120, and 180 epoch. Local SGD ×3 trains total 360 epochs and the learning rate is decayed at 90, 180, and 270 epochs). It is observed that Gossip-SGD ×2 can reach the target with notably more time expense than Gossip-PGA/AGA and parallel SGD, and Local SGD ×3 still cannot reach the 76% accuracy. All these observations validate that periodic global averaging can accelerate Gossip SGD signiﬁcantly. Figure 2 shows the iteration-wise and runtime-wise con- vergence in terms of training loss. In the left ﬁgure, it is observed Gossip-PGA/AGA converges faster (in iteration) and more accurate than local and Gossip SGD, which is consistent with our theory. In the right ﬁgure, it is observed that Gossip-PGA/AGA is the fastest method (in time) that can reach the same training loss as parallel SGD. Compare with SlowMo. Gossip-PGA is an instance of SlowMo, in which the base optimizer is set as Gossip SGD, slow momentum β = 0, and slow learning rate α= 1. We made experiments to compare Gossip-PGA with SlowMo. It is observed the additional slow momentum update helps SlowMo with large H but degrades it when H is small. This observation is consistent with Fig. 3(a) in (Wang et al., 2019). This observation implies that the slow momentum update may not always be beneﬁcial in SlowMo. Period Gossip-PGA SlowMo H = 6 76.28 75.23 H = 48 75 .66 75.81 Table 8.Comparison of Top-1 validation accuracy with SlowMo with different periods. Ring Topology. While the convergence property of Gossip- PGA is established over the static network topology, we utilize the dynamic one-peer exponential topology in the above deep experiments because it usually achieves better accuracy. To illustrate the derived theoretical results, we make an additional experiment, over the static ring topology, to compare Gossip-PGA with Gossip SGD in Table 9. It is observed that Gossip-PGA can achieve better accuracy than Gossip SGD after running the same epochs, which coincides with our analysis that Gossip-PGA has faster convergence. Scalability. We establish in Theorem 2 that Gossip-PGA can achieve linear speedup in the non-convex setting. ToAccelerating Gossip SGD with Periodic Global Averaging 0 20000 40000 60000 80000 100000 Iterations 2 4 6 8 10Training Loss Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA 0 50000 100000 150000 200000 Training time 2 4 6 8 10Training Loss Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA Figure 3.Convergence results of BERT on the language modeling task in terms of iteration and runtime. Method Epoch Acc % Time(Hrs.) Gossip SGD 120 74.86 1.56 Gossip PGA 120 75.94 1.68 Table 9.Comparison of Top-1 validation accuracy on Gossip-PGA and Gossip SGD with ring topology. validate it, we conduct a scaling experiment and list the result in Table 10. Figures represent the ﬁnal accuracy and hours to ﬁnish training. It is observed that Gossip-PGA can achieve a roughly linear speedup in training time without notably performance degradation. Method 4 nodes 8 nodes 16 nodes 32 nodes Parallel SGD 76.3/11.6 76 .4/6.3 76 .3/3.7 76 .2/2.2 Gossip SGD 76.3/11.1 76 .4/5.7 75 .9/2.8 75 .0/1.5 Gossip PGA 76.4/11.2 76 .7/5.9 76 .3/3.0 76 .2/1.6 Table 10.Scaling effects on different methods with different num- bers of nodes. Figures represent the ﬁnal accuracy and hours to complete training. 5.3. Language Modeling BERT (Devlin et al., 2018) is a widely used pre-training language representation model for NLP tasks. We train a BERT-Large model (∼330M parameters) on the Wikipedia METHOD FINAL LOSS RUNTIME (HRS ) PARALLEL SGD 1.75 59.02 LOCAL SGD 2.85 20.93 LOCAL SGD ×3 1.88 60 GOSSIP SGD 2.17 29.7 GOSSIP SGD ×2 1.81 59.7 GOSSIP -PGA 1.82 35.4 GOSSIP -AGA 1.77 30.4 Table 11.Comparison of training loss and training time of BERT training on different algorithms after completing all training steps. and BookCorpus datasets. We set the period to 6 for both Local SGD and Gossip-PGA. In Gossip-AGA, the period is set to 4 initially and changed adaptively afterwards, roughly 9.6% iterations conduct global averaging. Table 11 shows the ﬁnal training loss and training runtime of the aforementioned methods. Gossip-AGA can reach com- parable training loss with parallel SGD, but with roughly 1.94 x training time speed-up. Gossip SGD and Local SGD cannot reach training loss that below 1.8 even if they are trained over 60 hours (see Local SGD ×3 and Gossip SGD ×2.) Figure 3 shows the iteration-wise and runtime-wise convergence w.r.t training loss of the aforementioned meth- ods. The left plot shows Gossip-PGA/AGA has almost the same convergence as Gossip SGD in iterations; the right plot shows that Gossip-AGA is the fastest method in training time that can reach the same accuracy as parallel SGD. 6. Conclusion We introduce Gossip-PGA/AGA to mitigate the slow con- vergence rate of Gossip SGD in distributed training. Theo- retically, we prove the convergence improvement in smooth convex and non-convex problem. Empirically, experimental results of large-scale training validate our theories. References Alistarh, D., Grubic, D., Li, J., Tomioka, R., and V ojnovic, M. Qsgd: Communication-efﬁcient sgd via gradient quan- tization and encoding. In Advances in Neural Information Processing Systems, pp. 1709–1720, 2017. Assran, M., Loizou, N., Ballas, N., and Rabbat, M. Stochas- tic gradient push for distributed deep learning. In Inter- national Conference on Machine Learning (ICML), pp. 344–353, 2019. Bayoumi, A. K. R., Mishchenko, K., and Richtarik, P. Tighter theory for local sgd on identical and heteroge-Accelerating Gossip SGD with Periodic Global Averaging neous data. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 4519–4529, 2020. Ben-Nun, T. and Hoeﬂer, T. Demystifying parallel and dis- tributed deep learning: An in-depth concurrency analysis. ACM Computing Surveys (CSUR), 52(4):1–43, 2019. Berahas, A. S., Bollapragada, R., Keskar, N. S., and Wei, E. Balancing communication and computation in distributed optimization. IEEE Transactions on Automatic Control, 64(8):3141–3155, 2018. Bernstein, J., Zhao, J., Azizzadenesheli, K., and Anandku- mar, A. signsgd with majority vote is communication efﬁ- cient and fault tolerant. arXiv preprint arXiv:1810.05291, 2018. Chen, J. and Sayed, A. H. Diffusion adaptation strategies for distributed optimization and learning over networks. IEEE Transactions on Signal Processing , 60(8):4289– 4305, 2012. Chen, T., Giannakis, G., Sun, T., and Yin, W. LAG: Lazily aggregated gradient for communication-efﬁcient distributed learning. In Advances in Neural Information Processing Systems, pp. 5050–5060, 2018. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248–255. Ieee, 2009. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Di Lorenzo, P. and Scutari, G. Next: In-network nonconvex optimization. IEEE Transactions on Signal and Informa- tion Processing over Networks, 2(2):120–136, 2016. Duchi, J. C., Agarwal, A., and Wainwright, M. J. Dual aver- aging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3):592–606, 2011. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016. He, L., Bian, A., and Jaggi, M. Cola: Decentralized linear learning. In Advances in Neural Information Processing Systems, pp. 4536–4546, 2018. Koloskova, A., Lin, T., Stich, S. U., and Jaggi, M. De- centralized deep learning with arbitrary communication compression. In International Conference on Learning Representations, 2019a. Koloskova, A., Stich, S., and Jaggi, M. Decentralized stochastic optimization and gossip algorithms with com- pressed communication. In International Conference on Machine Learning, pp. 3478–3487, 2019b. Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and Stich, S. U. A uniﬁed theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning (ICML), pp. 1–12, 2020. Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V ., Long, J., Shekita, E. J., and Su, B.-Y . Scaling distributed machine learning with the parameter server. In 11th {USENIX}Symposium on Operating Systems Design and Implementation ( {OSDI}14), pp. 583–598, 2014. Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the convergence of fedavg on non-iid data. In International Conference on Learning Representations, 2019a. Li, X., Yang, W., Wang, S., and Zhang, Z. Communica- tion efﬁcient decentralized training with multiple local updates. arXiv preprint arXiv:1910.09126, 2019b. Li, Z., Shi, W., and Yan, M. A decentralized proximal- gradient method with network independent step-sizes and separated convergence rates. IEEE Transactions on Signal Processing, July 2019c. early acces. Also available on arXiv:1704.07807. Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J. Can decentralized algorithms outperform central- ized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Infor- mation Processing Systems, pp. 5330–5340, 2017. Lian, X., Zhang, W., Zhang, C., and Liu, J. Asynchronous decentralized parallel stochastic gradient descent. In In- ternational Conference on Machine Learning, pp. 3043– 3052, 2018. Lin, T., Stich, S. U., Patel, K. K., and Jaggi, M. Don’t use large mini-batches, use local sgd. arXiv preprint arXiv:1808.07217, 2018. Liu, Y ., Xu, W., Wu, G., Tian, Z., and Ling, Q. Communication-censored admm for decentralized con- sensus optimization. IEEE Transactions on Signal Pro- cessing, 67(10):2565–2579, 2019.Accelerating Gossip SGD with Periodic Global Averaging Lu, S., Zhang, X., Sun, H., and Hong, M. Gnsd: A gradient- tracking based nonconvex stochastic algorithm for de- centralized optimization. In 2019 IEEE Data Science Workshop (DSW), pp. 315–321. IEEE, 2019. Luo, Q., He, J., Zhuo, Y ., and Qian, X. Prague: High- performance heterogeneity-aware asynchronous decen- tralized training. In Proceedings of the Twenty-Fifth In- ternational Conference on Architectural Support for Pro- gramming Languages and Operating Systems, pp. 401– 416, 2020. McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelli- gence and Statistics, pp. 1273–1282. PMLR, 2017. Nedic, A. and Ozdaglar, A. Distributed subgradient meth- ods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48–61, 2009. Nedic, A., Olshevsky, A., and Shi, W. Achieving geomet- ric convergence for distributed optimization over time- varying graphs. SIAM Journal on Optimization, 27(4): 2597–2633, 2017. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), pp. 8024–8035, 2019. Patarasuk, P. and Yuan, X. Bandwidth optimal all-reduce al- gorithms for clusters of workstations. Journal of Parallel and Distributed Computing, 69(2):117–124, 2009. Qu, G. and Li, N. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control of Network Systems, 5(3):1245–1260, 2018. Scaman, K., Bach, F., Bubeck, S., Lee, Y . T., and Massouli´e, L. Optimal algorithms for smooth and strongly convex distributed optimization in networks. In International Conference on Machine Learning, pp. 3027–3036, 2017. Scaman, K., Bach, F., Bubeck, S., Massouli´e, L., and Lee, Y . T. Optimal algorithms for non-smooth distributed opti- mization in networks. In Advances in Neural Information Processing Systems, pp. 2740–2749, 2018. Shi, W., Ling, Q., Yuan, K., Wu, G., and Yin, W. On the lin- ear convergence of the admm in decentralized consensus optimization. IEEE Transactions on Signal Processing, 62(7):1750–1761, 2014. Shi, W., Ling, Q., Wu, G., and Yin, W. EXTRA: An exact ﬁrst-order algorithm for decentralized consensus opti- mization. SIAM Journal on Optimization, 25(2):944–966, 2015. Stich, S. U. Local sgd converges fast and communicates little. In International Conference on Learning Represen- tations (ICLR), 2019. Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J. d2: De- centralized training over decentralized data. In Interna- tional Conference on Machine Learning, pp. 4848–4856, 2018. Tang, H., Yu, C., Lian, X., Zhang, T., and Liu, J. Dou- blesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression. In Interna- tional Conference on Machine Learning, pp. 6155–6165. PMLR, 2019. Tsitsiklis, J., Bertsekas, D., and Athans, M. Distributed asynchronous deterministic and stochastic gradient op- timization algorithms. IEEE transactions on automatic control, 31(9):803–812, 1986. Uribe, C. A., Lee, S., Gasnikov, A., and Nedi´c, A. A dual approach for optimal algorithms in distributed optimiza- tion over networks. Optimization Methods and Software, pp. 1–40, 2020. Wang, J. and Joshi, G. Adaptive communication strategies to achieve the best error-runtime trade-off in local-update sgd. In Systems and Machine Learning (SysML) Confer- ence, 2019. Wang, J., Tantia, V ., Ballas, N., and Rabbat, M. SlowMo: Improving communication-efﬁcient distributed sgd with slow momentum. arXiv preprint arXiv:1910.00643 , 2019. Xin, R., Khan, U. A., and Kar, S. An improved convergence analysis for decentralized online stochastic non-convex optimization. arXiv preprint arXiv:2008.04195, 2020. Xu, J., Zhu, S., Soh, Y . C., and Xie, L. Augmented dis- tributed gradient methods for multi-agent optimization under uncoordinated constant stepsizes. In IEEE Con- ference on Decision and Control (CDC), pp. 2055–2060, Osaka, Japan, 2015. You, Y ., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2019. Yu, H., Yang, S., and Zhu, S. Parallel restarted sgd with faster convergence and less communication: Demystify- ing why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelli- gence, volume 33, pp. 5693–5700, 2019.Accelerating Gossip SGD with Periodic Global Averaging Yuan, K., Ling, Q., and Yin, W. On the convergence of decentralized gradient descent. SIAM Journal on Opti- mization, 26(3):1835–1854, 2016. Yuan, K., Ying, B., Zhao, X., and Sayed, A. H. Exact dffusion for distributed optimization and learning – Part I: Algorithm development. IEEE Transactions on Signal Processing, 67(3):708 – 723, 2019. Yuan, K., Alghunaim, S. A., Ying, B., and Sayed, A. H. On the inﬂuence of bias-correction on distributed stochastic optimization. IEEE Transactions on Signal Processing, 2020. Zhang, J., De Sa, C., Mitliagkas, I., and R ´e, C. Paral- lel sgd: When does averaging help? arXiv preprint arXiv:1606.07365, 2016. Zinkevich, M., Weimer, M., Li, L., and Smola, A. J. Paral- lelized stochastic gradient descent. In Advances in neural information processing systems, pp. 2595–2603, 2010.Accelerating Gossip SGD with Periodic Global Averaging A. Preliminary Notation. We ﬁrst introduce necessary notations as follows. • x(k) = [(x(k) 1 )T; (x(k) 2 )T; ··· ; (x(k) n )T] ∈Rn×d • ∇F(x(k); ξ(k+1)) = [∇F1(x(k) 1 ; ξ(k+1) 1 )T; ··· ; ∇Fn(x(k) n ; ξ(k+1) n )T] ∈Rn×d • ∇f(x(k)) = [∇f1(x(k) 1 )T; ∇f2(x(k) 2 )T; ··· ; ∇fn(x(k) n )T] ∈Rn×d • ¯x(k) = [(¯x(k))T; (¯x(k))T; ··· ; (¯x(k))T] ∈Rn×d where ¯x(k) = 1 n ∑n i=1 x(k) i • x⋆ = [(x⋆)T; (x⋆)T; ··· ; (x⋆)T] ∈Rn×d where x⋆ is the global solution to problem (1). • W = [wij] ∈Rn×n. • 1n = col{1,1,··· ,1}∈ Rn. • Given two matrices x,y ∈Rn×d, we deﬁne inner product ⟨x,y⟩= tr(xTy) and the Frobenius norm ∥x∥2 F = ⟨x,x⟩. • Given W ∈Rn×n, we let ∥W∥2 = σmax(W) where σmax(·) denote the maximum sigular value. Gossip-PGA in matrix notation. For ease of analysis, we rewrite the main recursion of Gossip-PGA in matrix notation: x(k+1) = { W(x(k) −γ∇F(x(k); ξ(k+1))) If mod(k+ 1,H) ̸= 0 1 n1n1T n(x(k) −γ∇F(x(k); ξ(k+1))) otherwise (10) Gradient noise. We repeat the deﬁnition of ﬁltration in Assumption 2 here for convenience. F(k)= { {x(k) i }n i=1,{ξ(k) i }n i=1,··· ,{x(0) i }n i=1,{ξ(0) i }n i=1 } (11) • With Assumption 2, we can evaluate the magnitude of the averaged gradient noise: E[∥1 n n∑ i=1 ∇Fi(x(k−1) i ; ξ(k) i ) −1 n n∑ i=1 ∇fi(x(k−1) i )∥2|F(k−1)] (a) = 1 n2 n∑ i=1 E[∥∇Fi(x(k−1) i ; ξ(k) i ) −∇fi(x(k−1) i )∥2|F(k−1)] (5) ≤σ2 n (12) where (a) holds because ξ(k) i is independent for any iand E[∇Fi(x(k−1) i ; ξ(k) i ) −∇fi(x(k−1) i )|F(k−1)] = 0. • We deﬁne gradient noise as s(k) i = ∇Fi(x(k−1) i ; ξ(k) i ) −∇fi(x(k−1) i ). For any 0 ≤j ≤k<ℓ , it holds that E[ ( s(k) i )T s(ℓ) i |F(j)] (a) = EF(ℓ−1)/F(j) [ E[ ( s(k) i )T s(ℓ) i |F(ℓ−1)] ] = EF(ℓ−1)/F(j) [( s(k) i )T E[s(ℓ) i |F(ℓ−1)] ](4) = 0 (13) where F(ℓ−1)/F(j) := { {x(j+1) i }n i=1,{ξ(j+1) i }n i=1,··· ,{x(ℓ−1) i }n i=1,{ξ(ℓ−1) i }n i=1 } and (a) holds due to the law of total expectation. • For any 0 ≤k<ℓ , it holds that E[∥s(ℓ) i ∥2|F(k)] = EF(ℓ−1)/F(k) [ E[∥s(ℓ) i ∥2|Fℓ−1] ](5) ≤EF(ℓ−1)/F(k) [ σ2] = σ2 (14)Accelerating Gossip SGD with Periodic Global Averaging Smoothness. Since each fi(x) is assumed to be L-smooth in Assumption 1, it holds that f(x) = 1 n ∑n i=1 fi(x) is also L-smooth. As a result, the following inequality holds for any x,y∈Rd: fi(x) −fi(y) −L 2 ∥x−y∥2 ≤⟨∇fi(y),x−y⟩ (15) Smoothness and convexity. If each fi(x) is further assumed to be convex (see Assumption 4), it holds that f(x) = 1 n ∑n i=1 fi(x) is also convex. For this scenario, it holds for any x,y∈Rd that: ∥∇f(x) −∇f(x⋆)∥2 ≤2L ( f(x) −f(x⋆) ) (16) fi(x) −fi(y) ≤⟨∇fi(x),x−y⟩ (17) Network weighting matrix. Suppose a weighting matrix W ∈Rn×n satisﬁes Assumption 3, it holds that ∥W −1 n11T∥2 ≤β, ∥(W −1 n11T)k∥2 ≤βk, ∀k. (18) Submultiplicativity of the Frobenius norm. Given matrices W ∈Rn×n and y ∈Rn×d, it holds that ∥Wy∥F ≤∥W∥2∥y∥F. (19) To verify it, by lettingyj be the j-th column of y, we have ∥Wy∥2 F = ∑d j=1 ∥Wyj∥2 2 ≤∑d j=1 ∥W∥2 2∥yj∥2 2 = ∥W∥2 2∥y∥2 F. B. Convergence analysis for convex scenario B.1. Proof Outline for Theorem 1 The following lemma established in (Koloskova et al., 2020, Lemma 8) shows howE∥¯x(k) −x⋆∥2 evolves with iterations. Lemma 1 (DESCENT LEMMA (Koloskova et al., 2020)). When Assumptions 1–4 hold and step-size γ < 1 4L, it holds for k= 1,2,··· that E∥¯x(k) −x⋆∥2 ≤E∥¯x(k−1) −x⋆∥2 −γ ( Ef(¯x(k−1)) −f(x⋆) ) + 3Lγ 2n E∥x(k−1) −¯x(k−1)∥2 F + γ2σ2 n , (20) where ¯x(k) = [(¯x(k))T; ··· ; (¯x(k))T] ∈Rn×d. Remark 7. It is worth noting that Lemma 1 also holds for the standard Gossip SGD algorithm. The periodic global averaging step does not affect this descent lemma. Next we establish the consensus lemmas in which Gossip-PGA is fundamentally different from Gossip SGD. Note that Gossip-PGA takes global average every H iterations. For any k= 0,1,···, we deﬁne τ(k) = max{ℓ: ℓ≤kand mod(ℓ,H) = 0} (21) as the most recent iteration when global average is conducted. In Gossip-PGA, it holds that ¯xτ(k) = xτ(k) i for any i∈[n]. This is different from Gossip SGD in which ¯x(k) = x(k) i can only happen when k= 0. For Gossip-PGA, the real challenge is to investigate how the periodic global averaging helps reduce consensus error and hence accelerate the convergence rate. In fact, there are two forces in Gossip-PGA that drive local model parameters to reach consensus: the gossip communication and the periodic global averaging. Each of these two forces is possible to dominate the consensus controlling in different scenarios: Scenario I. Global averaging is more critical to guarantee consensus on large or sparse network, or when global averaging is conducted frequently. Scenario II. Gossip communication is more critical to guarantee consensus on small or dense network, or when global averaging is conducted infrequently. Ignoring either of the above scenario will lead to incomplete or even incorrect conclusions, as shown in Remark 5. In the following, we will establish a speciﬁc consensus lemma for each scenario and then unify them into one that precisely characterize how the consensus distance evolves with iterations in Gossip-PGA.Accelerating Gossip SGD with Periodic Global Averaging Lemma 2 (CONSENSUS LEMMA : G LOBAL AVERAGING DOMINATING ). Under Assumptions 1–4, it holds for k = τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E∥x(k+1) −¯x(k+1)∥2 F ≤6Hγ2β2L2 k∑ ℓ=τ(k) βk−ℓE∥x(ℓ)−¯x(ℓ)∥2 F + 12nHγ2β2L k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆))+2nγ2β2Cβ(3b2+σ2) (22) where b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2 implies data heterogeneity and Cβ = ∑H−1 k=0 βk = (1 −βH)/(1 −β). Lemma 3 (CONSENSUS LEMMA : G OSSIP DOMINATING ). Under Assumptions 1–4, it holds for k = τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E∥x(k+1) −¯x(k+1)∥2 F ≤6γ2β2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ)−¯x(ℓ)∥2 F + 12nγ2β2L 1 −β k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆))+2nγ2β2Cβ( 3b2 1 −β+σ2) (23) Observing Lemmas 2 and 3, it is found that bounds (22) and (23) are in the same shape except for some critical coefﬁcients. With the following relation: { y≤a1x+ b y≤a2x+ b =⇒ y≤min{a1,a2}x+ b, (24) we can unify Lemmas 2 and 3 into: Lemma 4 (UNIFIED CONSENSUS LEMMA ). Under Assumptions 1–4, it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E∥x(k+1) −¯x(k+1)∥2 F ≤6Dβγ2β2L2 k∑ ℓ=τ(k) βk−ℓE∥x(ℓ)−¯x(ℓ)∥2 F + 12nDβγ2β2L k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆))+2nγ2β2Cβ(3Dβb2+σ2) (25) where b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2 implies data heterogeneity, Cβ = ∑H−1 k=0 βk = (1 −βH)/(1 −β), and Dβ = min{ 1 1−β,H}. Remark 8. This lemma reﬂects how the network topology and the global averaging period contribute to the consensus controlling. For scenario I where the network is large or sparse such that 1/(1 −β) > H, Lemma 4 indicates that the consensus error is mainly controlled by the global averaging period (i.e., Dβ = H). On the other hand, for scenario II where the network is small or dense such that 1/(1 −β) < H, Lemma 4 indicates that the consensus error is mainly controlled by gossip communication (i.e., Dβ = 1/(1 −β)). Using Lemma 4, we derive the upper bound of the weighted running average of E∥xk −¯xk∥2 F: Lemma 5 (RUNNING CONSENSUS LEMMA ). Suppose Assumptions 1–4 hold and step-size γ <1/(4LβDβ), it holds for T >0 that 1 T + 1 T∑ k=0 E∥x(k) −¯x(k)∥2 F ≤2c2Dβ T + 1 T∑ k=0 ( Ef(¯x(k)) −f(x⋆) ) + 2c3 (26) where c2 and c3 are constants deﬁned as c2 = 12nβ2Dβγ2L, (27) c3 = 2nβ2γ2Cβ(3Dβb2 + σ2) (28)Accelerating Gossip SGD with Periodic Global Averaging With Lemmas 1 and 5, we can establish the ﬁnal convergence Theorem 1, see the proof in Sec. B.5. B.2. Proof of Lemma 1. This lemma was ﬁrst established in (Koloskova et al., 2020, Lemma 8). We made slight improvement to tight constants appeared in step-size ranges and upper bound (20). For readers’ convenience, we repeat arguments here. Recall the algorithm in (10). By taking the average on both sides, we reach that ¯x(k) −x⋆ = ¯x(k−1) −x⋆ −γ n n∑ i=1 ∇Fi(x(k−1) i ; ξ(k) i ), ∀k= 1,2,··· (29) By taking expectation over the square of both sides of the above recursion conditioned on F(k−1), we have E[∥¯x(k) −x⋆∥2|F(k−1)] (4) = ∥¯x(k−1) −x⋆ −γ n n∑ i=1 ∇fi(x(k−1) i )∥2 + γ2E[∥1 n n∑ i=1 ∇Fi(x(k−1) i ; ξ(k) i ) −1 n n∑ i=1 ∇fi(x(k−1) i )∥2|F(k−1)] (12) ≤∥ ¯x(k−1) −x⋆ −γ n n∑ i=1 ∇fi(x(k−1) i )∥2 + γ2σ2 n (30) Note that the ﬁrst term can be expanded as follows. ∥¯x(k−1) −x⋆ −γ n n∑ i=1 ∇fi(x(k−1) i )∥2 = ∥¯x(k−1) −x⋆ −γ n n∑ i=1 [∇fi(x(k−1) i ) −∇fi(x⋆)]∥2 = ∥¯x(k−1) −x⋆∥2 −2γ n n∑ i=1 ⟨¯x(k−1) −x⋆,∇fi(x(k−1) i ) −∇fi(x⋆)⟩    (A) + γ2∥1 n n∑ i=1 [∇fi(x(k−1) i ) −∇fi(x⋆)] ∥2    (B) (31) We now bound the term (A): 2γ n n∑ i=1 ⟨¯x(k−1) −x⋆,∇fi(x(k−1) i ) −∇fi(x⋆)⟩ = 2γ n n∑ i=1 ⟨¯x(k−1) −x⋆,∇fi(x(k−1) i )⟩ = 2γ n n∑ i=1 ⟨¯x(k−1) −x(k−1) i ,∇fi(x(k−1) i )⟩+ 2γ n n∑ i=1 ⟨x(k−1) i −x⋆,∇fi(x(k−1) i )⟩ (a) ≥ 2γ n n∑ i=1 ( fi(¯x(k−1)) −fi(x(k−1) i ) −L 2 ∥¯x(k−1) −x(k−1) i ∥2 ) + 2γ n n∑ i=1 ( fi(x(k−1) i ) −fi(x⋆) ) = 2γ n n∑ i=1 ( fi(¯x(k−1)) −fi(x⋆) ) −γL n ∥¯x(k−1) −x(k−1)∥2 F = 2γ ( f(¯x(k−1)) −f(x⋆) ) −γL n ∥¯x(k−1) −x(k−1)∥2 F (32) where (a) holds because of the inequality (15) and (17). We next bound term (B) in (31): γ2∥1 n n∑ i=1 [∇fi(x(k−1) i ) −∇fi(x⋆)] ∥2Accelerating Gossip SGD with Periodic Global Averaging = γ2∥1 n n∑ i=1 [∇fi(x(k−1) i ) −∇fi(¯x(k−1)) + ∇fi(¯x(k−1)) −∇fi(x⋆)] ∥2 (3) ≤2γ2L2 n ∥x(k−1) −¯x(k−1)∥2 F + 2γ2∥∇f(¯x(k−1)) −∇f(x⋆)∥2 (16) ≤ 2γ2L2 n ∥x(k−1) −¯x(k−1)∥2 F + 4Lγ2( f(¯x(k−1)) −f(x⋆) ) . (33) Substituting (33) and (32) into (31), we have ∥¯x(k−1) −x⋆ −γ n n∑ i=1 ∇fi(x(k−1) i )∥2 ≤∥¯x(k−1) −x⋆∥2 −2γ(1 −2Lγ) ( f(¯x(k−1)) −f(x⋆) ) + (γL n + 2γ2L2 n ) ∥¯x(k−1) −x(k−1)∥2 F ≤∥¯x(k−1) −x⋆∥2 −γ ( f(¯x(k−1)) −f(x⋆) ) + 3γL 2n ∥¯x(k−1) −x(k−1)∥2 F (34) where the last inequality holds when γ < 1 4L. Substituting the above inequality into (30) and taking expectation over the ﬁltration, we reach the result in (20). B.3. Proofs of Lemma 2 and 3. Note the gossip averaging is conducted when k= τ(k),τ(k) + 1,··· ,τ(k) + H−1, i.e., x(k+1) = W(x(k) −γ∇F(x(k); ξ(k+1))). (35) Since ¯x(k+1) = 1 n11Tx(k+1), it holds that ¯x(k+1) = 1 n11T(x(k) −γ∇F(x(k); ξ(k+1))). (36) With the above two recursions, we have x(k+1) −¯x(k+1) = (W −1 n11T)(x(k) −γ∇F(x(k); ξ(k+1))) (37) In the following we will derive two upper bounds for E∥x(k+1) −¯x(k+1)∥2. Bound in Lemma 2. With (37), we have x(k+1) −¯x(k+1) = (W −1 n11T)(x(k) −γ∇F(x(k); ξ(k+1))) = (W −1 n11T)(x(k) −¯x(k) −γ∇F(x(k); ξ(k+1))) = (W −1 n11T)k+1−τ(k)(x(τ(k)) −¯x(τ(k))) −γ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇F(x(ℓ); ξ(ℓ+1)) = −γ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇F(x(ℓ); ξ(ℓ+1)) (38) where the last equality holds becausex(τ(k)) = ¯x(τ(k)) after the global averaging at iterationτ(k). With the above inequality, we have E[∥x(k+1) −¯x(k+1)∥2 F|F(τ(k))] = γ2E[∥ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇F(x(ℓ); ξ(ℓ+1))∥2 F|F(τ(k))]Accelerating Gossip SGD with Periodic Global Averaging ≤2γ2E[∥ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇f(x(ℓ))∥2 F|F(τ(k))] +2γ2E [ ∥ k∑ ℓ=τ(k) (W−1 n11T)k+1−ℓ[∇F(x(ℓ); ξ(ℓ+1))−∇f(xℓ)]∥2 F|F(τ(k))] (13) = 2 γ2E[∥ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇f(x(ℓ))∥2 F|F(τ(k))] +2γ2 k∑ ℓ=τ(k) E [ ∥(W−1 n11T)k+1−ℓ[∇F(x(ℓ); ξ(ℓ+1))−∇f(xℓ)]∥2 F|F(τ(k))] (a) ≤2γ2(k+ 1 −τ(k)) k∑ ℓ=τ(k) β2(k+1−ℓ)E[∥∇f(x(ℓ))∥2 F|F(τ(k))] + 2nγ2σ2 k∑ ℓ=τ(k) β2(k+1−ℓ) ≤2γ2H k∑ ℓ=τ(k) β2(k+1−ℓ)E[∥∇f(x(ℓ))∥2 F|F(τ(k))] + 2nγ2β2σ2Cβ (39) where inequality (a) holds because of (14), (18) and (19), and the last inequality holds because ∑k ℓ=τ(k) β2(k+1−ℓ) ≤ ∑H ℓ=1 β2ℓ = β2(1−β2H)/(1−β2) ≤β2(1−βH)/(1−β) = β2Cβ where we deﬁne Cβ = ∑H−1 ℓ=0 βℓ = (1−βH)/(1−β). Note that ∥∇f(x(ℓ))∥2 F = ∥∇f(x(ℓ)) −∇f(¯x(ℓ)) + ∇f(¯x(ℓ)) −∇f(x⋆) + ∇f(x⋆)∥2 F ≤3∥∇f(x(ℓ)) −∇f(¯x(ℓ))∥2 F + 3∥∇f(¯x(ℓ)) −∇f(x⋆)∥2 F + 3∥∇f(x⋆)∥2 F ≤3L2∥x(ℓ) −¯x(ℓ)∥2 F + 6nL(f(¯x(ℓ)) −f(x⋆)) + 3nb2 (40) where the last inequality holds because of (3) and (16). Notation b2 is deﬁned as b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2. Substituting (40) into (39), it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E[∥x(k+1) −¯x(k+1)∥2 F|F(τ(k))] ≤6Hγ2L2 k∑ ℓ=τ(k) β2(k+1−ℓ)E[∥x(ℓ) −¯x(ℓ)∥2 F|F(τ(k))] + 12nHγ2L k∑ ℓ=τ(k) β2(k+1−ℓ)E[f(¯x(ℓ)) −f(x⋆)|F(τ(k))]+2nγ2β2Cβ(3Hb2 + σ2) (41) By taking expectations over the ﬁltration F(τ(k)), we have E∥x(k+1) −¯x(k+1)∥2 F ≤6Hβ2γ2L2 k∑ ℓ=τ(k) β2(k−ℓ)E∥x(ℓ) −¯x(ℓ)∥2 F + 12nHβ2γ2L k∑ ℓ=τ(k) β2(k−ℓ)E(f(¯x(ℓ)) −f(x⋆)) + 2nγ2β2Cβ(3Hb2 + σ2) ≤6Hβ2γ2L2 k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 12nHβ2γ2L k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆)) + 2nγ2β2Cβ(3Hb2 + σ2) (42) Bound in Lemma 3. With (37), it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E[∥x(k+1) −¯x(k+1)∥2 F|F(k)]Accelerating Gossip SGD with Periodic Global Averaging = E[∥(W −1 n11T) ( x(k) −¯x(k) −γ∇F(x(k); ξ(k+1)) ) ∥2 F|F(k)] (4) = ∥(W −1 n11T) ( x(k) −¯x(k) −γ∇f(x(k)) ) ∥2 F + γ2E[∥(W −1 n11T) ( ∇F(x(k); ξ(k+1)) −∇f(x(k)) ) ∥2 F|F(k)] ≤∥(W −1 n11T) ( x(k) −¯x(k) −γ∇f(x(k)) ) ∥2 F + nγ2β2σ2 (43) where the last inequality holds because of (5) and (18). We now bound the ﬁrst term: ∥(W −1 n11T) ( x(k) −¯x(k) −γ∇f(x(k)) ) ∥2 F (a) ≤1 t∥(W −1 n11T) ( x(k) −¯x(k)) ∥2 F + γ2 1 −t∥(W −1 n11T)∇f(x(k))∥2 F (b) = β∥x(k) −¯x(k)∥2 F + β2γ2 1 −β∥∇f(x(k))∥2 F = β∥x(k) −¯x(k)∥2 F + β2γ2 1 −β∥∇f(x(k)) −∇f(¯x(k)) + ∇f(¯x(k)) −∇f(x⋆) + ∇f(x⋆)∥2 F (c) ≤β∥x(k) −¯x(k)∥2 F + 3β2γ2L2 1 −β ∥x(k) −¯x(k)∥2 F + 6nβ2γ2L 1 −β ( f(¯x(k)) −f(x⋆) ) + 3nβ2γ2b2 1 −β (44) where (a) holds because of the Jensen’s inequality for anyt∈(0,1), (b) holds by setting t= β, and (c) holds because of (3) and (16). Quantity b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2 in the last inequality. Substituting (44) into (43), we have E[∥x(k+1) −¯x(k+1)∥2 F|F(k)] ≤β∥x(k) −¯x(k)∥2 F + 3β2γ2L2 1 −β ∥x(k) −¯x(k)∥2 F + 6nβ2γ2L 1 −β ( f(¯x(k)) −f(x⋆) ) + nγ2β2σ2 + 3nβ2γ2b2 1 −β = βk+1−τ(k)∥x(τ(k)) −¯x(τ(k))∥2 F + 3β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓ∥x(ℓ) −¯x(ℓ)∥2 F + 6nβ2γ2L 1 −β k∑ ℓ=τ(k) βk−ℓ(f(¯x(ℓ)) −f(x⋆)) + nγ2β2Cβ( 3b2 1 −β + σ2) = 3β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓ∥x(ℓ) −¯x(ℓ)∥2 F + 6nβ2γ2L 1 −β k∑ ℓ=τ(k) βk−ℓ(f(¯x(ℓ)) −f(x⋆)) + nγ2β2Cβ( 3b2 1 −β + σ2) (45) By taking expectation over the ﬁltration F(k), we have E∥x(k+1) −¯x(k+1)∥2 F ≤3β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 6nβ2γ2L 1 −β k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆)) + nγ2β2Cβ( 3b2 1 −β + σ2) < 6β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 12nβ2γ2L 1 −β k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆)) + 2nγ2β2Cβ( 3b2 1 −β + σ2) (46) B.4. Proof of Lemma 5. To simplify the notation, we deﬁne A(k) = E∥x(k) −¯x(k)∥2 F, B (k) = Ef(¯x(k)) −f(x⋆), c1 = 6Dββ2γ2L2, c 2 = 12nDββ2γ2L, c 3 = 2nβ2γ2Cβ(3Dβb2 + σ2). (47)Accelerating Gossip SGD with Periodic Global Averaging Using these notations, we rewrite (22) for any k= 0,1,2,··· that { A(k) ≤c1 ∑k−1 ℓ=τ(k) βk−1−ℓA(ℓ) + c2 ∑k−1 ℓ=τ(k) βk−1−ℓB(ℓ) + c3 if k>τ (k) A(k) = 0 if k= τ(k) (48) We next deﬁne ΓT := {k|0 ≤k≤T and mod(k,H) = 0}, Γc T := {k|0 ≤k≤T and mod(k,H) ̸= 0}. (49) By taking the running average over both sides in (48) and recalling A(τ(k)) = 0, it holds that T∑ k=0 A(k) ≤c1 ∑ k∈Γc T k−1∑ ℓ=τ(k) βk−1−ℓA(ℓ) + c2 ∑ k∈Γc T k−1∑ ℓ=τ(k) βk−1−ℓB(ℓ) + c3(T + 1) (50) We further deﬁne ΘT := {ℓ|0 ≤ℓ≤T −1 and mod(ℓ+ 1,H) = 0}, Θc T := {ℓ|0 ≤ℓ≤T −1 and mod(ℓ+ 1,H) ̸= 0}. (51) With these notations, we have T∑ k=0 A(k) ≤c1 ∑ k∈Γc T k−1∑ ℓ=τ(k) βk−1−ℓA(ℓ) + c2 ∑ k∈Γc T k−1∑ ℓ=τ(k) βk−1−ℓB(ℓ) + c3(T + 1) = c1 ∑ ℓ∈Θc T A(ℓ)(τ(ℓ)+H−1∑ k=ℓ+1 βk−1−ℓ) + c2 ∑ ℓ∈Θc T B(ℓ)(τ(ℓ)+H−1∑ k=ℓ+1 βk−1−ℓ) + c3(T + 1) (a) ≤c1Cβ ∑ ℓ∈Θc T A(ℓ) + c2Cβ ∑ ℓ∈Θc T B(ℓ) + c3(T + 1) (b) ≤c1Cβ T∑ k=0 A(k) + c2Cβ T∑ k=0 B(k) + c3(T + 1) (c) ≤c1Dβ T∑ k=0 A(k) + c2Dβ T∑ k=0 B(k) + c3(T + 1) (52) where (a) holds because ∑τ(ℓ)+H−1 k=ℓ+1 βk−1−ℓ ≤∑H−1 k=0 βk = Cβ, (b) holds because A(k) ≥0 and B(k) ≥0, and (c) holds because Cβ = (1 −βH)/(1 −β) ≤min{ 1 1−β,H}= Dβ. If step-size γis sufﬁciently small such that 1 −c1Dβ ≥1 2 , it holds that T∑ k=0 A(k) ≤2c2Dβ T∑ k=0 B(k) + 2c3(T + 1). (53) To guarantee 1 −c1Dβ ≥1 2 , it is enough to let γ ≤1/(4LβDβ). B.5. Proof of Theorem 1 Following the notation in (47), we further deﬁne F(k) := E∥¯x(k) −x⋆∥2 F. With these notations, the inequality (20) becomes B(k) ≤F(k) γ −F(k+1) γ + γσ2 n + 3L 2nA(k) (54) Taking weighted running average over the above inequality to get 1 T + 1 T∑ k=0 B(k) ≤ F(0) (T + 1)γ + 3L 2n(T + 1) T∑ k=0 A(k) + γσ2 nAccelerating Gossip SGD with Periodic Global Averaging (53) ≤ F(0) (T + 1)γ + 6Lc2Dβ n(T + 1) T∑ k=0 B(k) + 3Lc3 n + γσ2 n ≤ F(0) (T + 1)γ + 1 2(T + 1) T∑ k=0 B(k) + 3Lc3 n + γσ2 n (55) where the last inequality holds when γ ≤1/(12LβDβ). Substituting c3 into the above inequality, we have 1 T + 1 T∑ k=0 B(k) ≤ 2F(0) (T + 1)γ + 2γσ2 n + 12Lβ2γ2Cβσ2 + 36Lβ2γ2CβDβb2. (56) The way to choose step-size γis adapted from Lemma 15 in (Koloskova et al., 2020). For simplicity, we let r0 = 2F(0), r 1 = 2σ2 n , r 2 = 12Lβ2Cβσ2 + 36Lβ2CβDβb2, (57) and inequality (56) becomes 1 T + 1 T∑ k=0 B(k) ≤ r0 (T + 1)γ + r1γ+ r2γ2. (58) Now we let γ = min { 1 12βLDβ , ( r0 r1(T+1) )1 2 , ( r0 r2(T+1) )1 3 } : • If ( r0 r2(T+1) )1 3 is the smallest, we let γ = ( r0 r2(T+1) )1 3 . With ( r0 r2(T+1) )1 3 ≤ ( r0 r1(T+1) )1 2 , (58) becomes 1 T + 1 T∑ k=0 B(k) ≤2r 1 3 2 ( r0 T + 1 )2 3 + r1 ( r0 r2(T + 1) )1 3 ≤2r 1 3 2 ( r0 T + 1 )2 3 + ( r0r1 T + 1 )1 2 . (59) • If ( r0 r1(T+1) )1 2 is the smallest, we let γ = ( r0 r1(T+1) )1 2 . With ( r0 r1(T+1) )1 2 ≤ ( r0 r2(T+1) )1 3 , (58) becomes 1 T + 1 T∑ k=0 B(k) ≤2 ( r0r1 T + 1 )1 2 + r0r2 r1(T + 1) ≤2 ( r0r1 T + 1 )1 2 + r 1 3 2 ( r0 T + 1 )2 3 . (60) • If 1 12βLDβ ≤ ( r0 r1(T+1) )1 2 and 1 12βLDβ ≤ ( r0 r2(T+1) )1 3 , we let γ = 1 12βLDβ and (58) becomes 1 T + 1 T∑ k=0 B(k) ≤12βLDβr0 T + 1 + ( r0r1 T + 1 )1 2 + r 1 3 2 ( r0 T + 1 )2 3 . (61) Combining (59), (60) and (61), we have 1 T + 1 T∑ k=0 B(k) ≤12r0LDββ T + 1 + 2 ( r0r1 T + 1 )1 2 + 2r 1 3 2 ( r0 T + 1 )2 3 . (62) Substituting constants r0, r1, and r2, we have the ﬁnal result: 1 T + 1 T∑ k=0 B(k) = O ( σ√ nT + (Cβ) 1 3 β 2 3 σ 2 3 T 2 3 + (Cβ) 1 3 (Dβ) 1 3 β 2 3 b 2 3 T 2 3 + βDβ T ) . (63)Accelerating Gossip SGD with Periodic Global Averaging C. Convergence analysis for non-convex scenario C.1. Proof Outline for Theorem 2. The proof outline for Theorem 2 is similar to that for Theorem 1. The descent lemma (Koloskova et al., 2020, Lemma 10) was established follows. Lemma 6 (DESCENT LEMMA (Koloskova et al., 2020)) . Under Assumption 1–3 and step-size γ < 1 4L, it holds for k= 1,2,··· that Ef(¯x(k)) ≤Ef(¯x(k−1)) −γ 4 E∥∇f(¯x(k−1))∥2 + γ2Lσ2 2n + 3γL2 4n E∥x(k) −¯x(k)∥2 F. (64) The consensus distance is examined in the following two lemmas. Similar to Lemma 4, we use Dβ = min{H,1/(1 −β)}. Lemma 7 (UNIFIED CONSENSUS LEMMA ). Under Assumptions 1–3 and 5, it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E∥x(k+1) −¯x(k+1)∥2 F ≤6Dββ2γ2L2 k∑ ℓ=τ(k) β2(k+1−ℓ)E∥x(ℓ) −¯x(ℓ)∥2 F + 6nDββ2γ2 k∑ ℓ=τ(k) β2(k+1−ℓ)E∥∇f(¯x(ℓ))∥2 + 2nβ2γ2Cβ(3Hˆb2 + σ2) (65) where Dβ = min{H, 1 1−β}. Lemma 8 (RUNNING CONSENSUS LEMMA ). When Assumptions 1–3 and 5 hold and step-size γ < 1 4LβDβ , it holds for any T >0 that 1 T + 1 T∑ k=0 E∥x(k) −¯x(k)∥2 F ≤ 2c2Dβ T + 1 T∑ k=0 E∥∇f(¯x(k))∥2 + 2c3 (66) where c2 and c3 are constants deﬁned as c2 = 6nDββ2γ2, (67) c3 = 2nβ2γ2Cβ(3Dβˆb2 + σ2). (68) With Lemmas 6 and 8, we can establish the convergence rate in Theorem 2. C.2. Proof of Lemma 6. This lemma was ﬁrst established in (Koloskova et al., 2020, Lemma 10). We made slight improvement to tight constants appeared in step-size ranges and upper bound (64). For readers’ convenience, we repeat arguments here. Recall that ¯x(k+1) = ¯x(k) −γ n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i ). (69) Since f(x) is L-smooth, it holds that E[f(¯x(k+1))|Fk] (15) ≤f(¯x(k)) −E [ ⟨∇f(¯x(k)),γ n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i )⟩|Fk] + γ2L 2 E[∥1 n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i )∥2|Fk] (4) = f(¯x(k)) −⟨∇f(¯x(k)),γ n n∑ i=1 ∇fi(x(k) i )⟩+ γ2L 2 E[∥1 n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i )∥2|Fk] (a) ≤f(¯x(k)) −⟨∇f(¯x(k)),γ n n∑ i=1 ∇fi(x(k) i )⟩+ γ2Lσ2 2n + γ2L 2 ∥1 n n∑ i=1 ∇fi(x(k) i )∥2 (70)Accelerating Gossip SGD with Periodic Global Averaging where (a) holds because E[∥1 n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i ) −∇fi(x(k) i ) + ∇fi(x(k) i )∥2|Fk] (4) = E[∥1 n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i ) −∇fi(x(k) i )∥2|Fk] + ∥1 n n∑ i=1 ∇fi(x(k) i )∥2 (12) ≤ σ2 n + ∥1 n n∑ i=1 ∇fi(x(k) i )∥2. (71) Note that −⟨∇f(¯x(k)),γ n n∑ i=1 ∇fi(x(k) i )⟩= −⟨∇f(¯x(k)),γ n n∑ i=1 [∇fi(x(k) i ) −∇fi(¯x(k)) + ∇fi(¯x(k))]⟩ ≤−γ∥∇f(¯x(k))∥2 + γ 2 ∥∇f(¯x(k))∥2 + γ 2n n∑ i=1 ∥∇fi(x(k) i ) −∇fi(¯x(k))∥2 ≤−γ 2 ∥∇f(¯x(k))∥2 + γL2 2n ∥x(k) −¯x(k)∥2 F (72) and ∥1 n n∑ i=1 ∇fi(x(k) i )∥2 ≤2L2 n ∥x(k) −¯x(k)∥2 F + 2∥∇f(¯x(k))∥2 (73) Substituting (72) and (73) into (70), taking expectations over F(k) and using the fact that γ < 1 4L, we reach the result in (64). C.3. Proof of Lemma 7. Similar to the proof of Lemmas 2 and 3, we will derive two bounds for E∥x(k+1) −¯x(k+1)∥2 F: Bound 1. Following (35)-(39), it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E[∥x(k+1) −¯x(k+1)∥2 F|F(τ(k))] ≤2γ2H k∑ ℓ=τ(k) β2(k+1−ℓ)E[∥∇f(x(ℓ))∥2 F|F(τ(k))] + 2nγ2β2σ2Cβ (74) Note that ∥∇f(x(k))∥2 F = n∑ i=1 ∥∇fi(x(k) i )∥2 = n∑ i=1 ∥∇fi(x(k) i ) −∇fi(¯x(k)) + ∇fi(¯x(k)) −∇f(¯xk) + ∇f(¯xk)∥2 ≤3L2∥x(k) −¯x(k)∥2 F + 3nˆb2 + 3n∥∇f(¯xk)∥2. (75) where the last inequality holds because of Assumption 5. Substituting (75) into (74) and taking expectation on F(τ(k)), we get E∥x(k+1) −¯x(k+1)∥2 F ≤6Hβ2γ2L2 k∑ ℓ=τ(k) β2(k−ℓ)E∥x(ℓ) −¯x(ℓ)∥2 F+6nHβ2γ2 k∑ ℓ=τ(k) β2(k−ℓ)∥∇f(¯x(ℓ))∥2+2nγ2β2Cβ(3Hˆb2 + σ2) ≤6Hγ2L2β2 k∑ ℓ=τ(k) βk−ℓ∥x(ℓ) −¯x(ℓ)∥2 F+6nHγ2β2 k∑ ℓ=τ(k) βk−ℓ∥∇f(¯x(ℓ))∥2+2nγ2β2Cβ(3Hˆb2 + σ2) (76)Accelerating Gossip SGD with Periodic Global Averaging Bound 2. Following (43) and ﬁrst two lines in (44), it holds for any k= τ(k),··· ,τ(k) + H−1 that E[∥x(k+1) −¯x(k+1)∥2 F|F(k)] ≤β∥x(k) −¯x(k)∥2 F + β2γ2 1 −β∥∇f(x(k))∥2 F + nγ2β2σ2. (77) Substituting (75) into (77), we get E[∥x(k+1) −¯x(k+1)∥2 F|F(k)] ≤ ( β+ 3β2γ2L2 1 −β ) ∥x(k) −¯x(k)∥2 F + 3nβ2γ2∥∇f(¯xk)∥2 1 −β + nγ2β2σ2 + 3nβ2γ2ˆb2 1 −β . (78) We next follow (43)–(46) and take expectation onF(k) to get E∥x(k+1) −¯x(k+1)∥2 F ≤3β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 3nβ2γ2 1 −β k∑ ℓ=τ(k) βk−ℓE∥∇f(¯x(ℓ))∥2 + nγ2β2Cβ( 3ˆb2 1 −β + σ2) ≤6β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 6nβ2γ2 1 −β k∑ ℓ=τ(k) βk−ℓE∥∇f(¯x(ℓ))∥2 + 2nγ2β2Cβ( 3ˆb2 1 −β + σ2) (79) With bounds (76) and (79), we reach the result (65). C.4. Proof of Lemma 8. We ﬁrst simplify the notation as follows: A(k) = E∥x(k) −¯x(k)∥2, B (k) = E∥∇f(¯x(k))∥2, c1 = 6Dββ2γ2L2, c 2 = 6nDββ2γ2, c 3 = 2nβ2γ2Cβ(3Dβˆb2 + σ2). (80) With these notations, we can follow the proof of Lemma 5 to get the ﬁnal result. C.5. Proof of Theorem 2. Following the notation in (80), we further deﬁne F(k) := Ef(¯x(k)). With these notations, the inequality (64) becomes B(k) ≤4F(k) γ −4F(k+1) γ + 2γLσ2 n + 3L2 n A(k) (81) Taking the weighted running average over the above inequality and divideT + 1 to get 1 T + 1 T∑ k=0 B(k) ≤ 4F(0) (T + 1)γ + 3L2 n(T + 1) T∑ k=0 A(k) + 2γLσ2 n (66) ≤ 4F(0) (T + 1)γ + 6β2L2Hc2 n(T + 1) T∑ k=0 B(k) + 6L2c3 n + 2γLσ2 n ≤ 4F(0) (T + 1)γ + 1 2(T + 1) T∑ k=0 B(k) + 6L2c3 n + 2γLσ2 n (82) where the last inequality holds when γ ≤ 1 9LHβ. Substituting c3 into the above inequality, we have 1 T + 1 T∑ k=0 B(k) ≤ 8F(0) (T + 1)γ + 4γLσ2 n + 24L2γ2β2Cβσ2 + 72L2γ2β2CβDβˆb2. (83) By following the arguments (57) – (63), we reach the result in (8).Accelerating Gossip SGD with Periodic Global Averaging D. Transient stage and transient time D.1. Transient stage derivation. (i) Gossip SGD. We ﬁrst consider the iid scenario where b2 = 0. To make the ﬁrst term dominate the other terms (see the ﬁrst line in Table 4), T has to be sufﬁciently large such that (ignoring the affects of σ) max { β 2 3 T 2 3 (1 −β) 1 3 , β (1 −β)T } ≤ 1√ nT =⇒ T ≥max { n3β4 (1 −β)2 , nβ2 (1 −β)2 } . (84) We next consider the non-iid scenario whereb2 ̸= 0. To make the ﬁrst term dominate the other terms, T has to be sufﬁciently large such that (ignoring the affects of σand b) max { β 2 3 T 2 3 (1 −β) 1 3 , β 2 3 T 2 3 (1 −β) 2 3 , β (1 −β)T } ≤ 1√ nT =⇒ T ≥max { n3β4 (1 −β)2 , n3β4 (1 −β)4 , nβ2 (1 −β)2 } . (85) When nβ >1 which usually holds for most commonly-used network topologies, inequalities (84) and (85) will result in the transient stage T = Ω( n3β4 (1−β)2 ) and T = Ω( n3β4 (1−β)4 ) for iid and non-iid scenarios, respectively. (ii) Gossip-PGA. We ﬁrst consider the iid scenario where b2 = 0. To make the ﬁrst term dominate the other terms (see the ﬁrst line in Table 4), T has to be sufﬁciently large such that (ignoring the affects of σ) max {C 1 3 ββ 2 3 T 2 3 ,βDβ T } ≤ 1√ nT =⇒ T ≥max { n3β4C2 β,nβ2D2 β } = Ω(n3β4C2 β). (86) We next consider the non-iid scenario whereb2 ̸= 0. To make the ﬁrst term dominate the other terms, T has to be sufﬁciently large such that (ignoring the affects of σand b) max {C 1 3 ββ 2 3 T 2 3 , C 1 3 βD 1 3 ββ 2 3 T 2 3 ,βDβ T } ≤ 1√ nT =⇒ T ≥max { n3β4C2 β,n3β4C2 βD2 β,nβ2D2 β } = Ω(n3β4C2 βD2 β) (87) when nβ >1. (iii) Local SGD. We ﬁrst consider the iid scenario where b2 = 0. To make the ﬁrst term dominate the other terms (see the ﬁrst line in Table 4), T has to be sufﬁciently large such that (ignoring the affects of σ) max {H 1 3 T 2 3 ,H T } ≤ 1√ nT =⇒ T ≥max { n3H2,nH2} = Ω(n3H2). (88) We next consider the non-iid scenario whereb2 ̸= 0. To make the ﬁrst term dominate the other terms, T has to be sufﬁciently large such that (ignoring the affects of σand b) max {H 1 3 T 2 3 ,H 2 3 T 2 3 ,H T } ≤ 1√ nT =⇒ T ≥max { n3H2,n3H4,nH2} = Ω(n3H4) (89) D.2. Transient time comparison The transient time comparisons between Gossip SGD and Gossip-PGA for the iid or non-iid scenario over the grid or ring topology are listed in Tables 12, 13 and 14. E. Proof of Corollary 1 The proof of Corollary 1 closely follows Theorem 1. First, the descent lemma 7 still holds for time-varying period. Second, with the facts that k+ 1 −τ(k) ≤Hmax, ∑k ℓ=τ(k) βk+1−ℓ ≤∑Hmax k=0 βk := Cβ, and ∑τ(ℓ)+H(ℓ)−1 k=ℓ+1 βk−1−ℓ ≤Accelerating Gossip SGD with Periodic Global Averaging ∑Hmax k=0 βk = Cβ, we follow Appendix C.3 and C.4 to reach the consensus distance inequality: 1 T + 1 T∑ k=0 E∥x(k) −¯x(k)∥2 ≤ 2c2Dβ T + 1 T∑ k=0 E∥∇f(¯x(k))∥2 + 2c3 (90) where c2 and c3 are constants deﬁned as c2 = 6nDββ2γ2, (91) c3 = 2nβ2γ2Cβ(3Dβˆb2 + σ2) (92) and Dβ = min{1/(1 −β),Hmax}, Cβ = ∑Hmax k=0 βk. With Lemma 6 and inequality (90), we can follow Appendix B.5 to reach the result in Corollary 1. F. Additional Experiments F.1. Implementation Details. We implement all the aforementioned algorithms with PyTorch (Paszke et al., 2019) 1.5.1 using NCCL 2.5.7 (CUDA 10.1) as the communication backend. Each server contains 8 V100 GPUs in our cluster and is treated as one node. The inter-node network fabrics are chosen from 25 Gbps TCP (which is a common distributed training platform setting) and 4×100 Gbps RoCE (which is a high-performance distributed training platform setting). All deep learning experiments are trained in the mixed precision using Pytorch extension package NVIDIA apex (https://github.com/NVIDIA/apex). For Gossip SGD related training, we use the time-varying one-peer exponential graph following (Assran et al., 2019). Workers send and receive a copy of the model’s parameters to and from its peer, thus keeping the load balancing among workers. All data are stored in the cloud storage service and downloaded to workers using HTTP during training. Image Classﬁcation The Nesterov momentum SGD optimizer is used with a linear scaling learning rate strategy. 32 nodes (each node is with 8 V100 GPUs) are used in all the experiments and the batch-size is set as 256 per node (8,192 in total). The learning rate is warmed up in the ﬁrst 5 epochs and is decayed by a factor of 10 at 30, 60 and 90 epochs. We train 120 epochs by default (unless speciﬁed otherwise) in every experiment and record the epoch and runtime when a 76% top-1 accuracy in the validation set has reached. 25 Gbps TCP network is used for inter-node communication in ResNet-50 training. In 4×100 Gbps RoCE network, the communication overhead is negligible given the high computation-to-communication ratio nature of ResNet models and Parallel SGD with computation and communication pipeline is recommended. We use a period 6 for both Local SGD and Gossip-PGA. In Gossip-AGA, the averaging period is set to 4 in the warm-up stage and changed adaptively afterwards, roughly 9% iterations conduct global averaging. Language Modeling All experiments are based on NVIDIA BERT implementation with mixed precision support and LAMB optimizer (You et al., 2019). 8 nodes are used in all the experiments with a batch-size 64 per GPU (4096 in total). We do not use gradient accumulation as it is not vertical with Local SGD. We only do phase 1 training and indicate the decreasing of training loss as convergence speed empirically. The learning rate is scaled to 3.75e−4 initially and decayed in a polynomial policy with warm-up. The phase 1 training consists of 112,608 steps in all experiments. We use a period 6 for both Local SGD and Gossip-PGA. In Gossip-AGA, the averaging period is set to 4 in the warm-up phase and changed adaptively afterwards, roughly 9.6% iterations conduct global averaging. GOSSIP SGD G OSSIP -PGA TRANSIENT ITER . O(n5) O(n4) SINGLE COMM . O(θd+ α) O(θd+ √nα) TRANSIENT TIME O(n5θd+ n5α) O(n4θd+ n4.5α) Table 12.Transient time comparison between Gossip SGD and Gossip-PGA for iid scenario over the speciﬁc grid (1 −β = O(1/n)) topologiy. We choose H = √nas the period in Gossip-PGA.Accelerating Gossip SGD with Periodic Global Averaging GOSSIP SGD G OSSIP -PGA TRANSIENT ITER . O(n11) O(n5) SINGLE COMM . O(θd+ α) O(θd+ √nα) TRANSIENT TIME O(n11θd+ n11α) O(n5θd+ n5.5α) Table 13.Transient time comparison between Gossip SGD and Gossip-PGA for non-iid scenario over the speciﬁc ring (1−β = O(1/n2)) topologiy. We choose H = √nas the period in Gossip-PGA. GOSSIP SGD G OSSIP -PGA TRANSIENT ITER . O(n7) O(n4) SINGLE COMM . O(θd+ α) O(θd+ √nα) TRANSIENT TIME O(n7θd+ n7α) O(n4θd+ n4.5α) Table 14.Transient time comparison between Gossip SGD and Gossip-PGA for iid scenario over the speciﬁc ring (1 −β = O(1/n2)) topologiy. We choose H = √nas the period in Gossip-PGA. Transient stage (Gossip-PGA)Transient stage (Gossip SGD)Transient stage (Gossip SGD)Transient stage(Gossip-PGA) Transient stage (Gossip-PGA)Transient stage (Gossip SGD) Figure 4.Convergence comparison between Gossip-PGA, Gossip SGD and parallel SGD on the logistic regression problem in iid data distributed setting over the ring topology. 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 error n=100; non-iid data Gossip SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 error n=100; non-iid data Gossip SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 2 10 1 100 error n=100; non-iid data Gossip SGD Gossip-PGA Parallel SGD Figure 5.Convergence comparison between Gossip-PGA, Gossip SGD and parallel SGD on the logistic regression problem in non-iid data distributed setting over the exponential graph (left), grid (middle) and ring (right) topology. F.2. More experiments on convex logistic regression. In this subsection we will test the performance of Gossip-PGA with iid data distribution and on different topologies. We will also compare it with Local SGD. Experiments on iid dataset. Figure 4 illustrates how Gossip SGD and Gossip-PGA converges under the iid data distributed setting over the ring topology. Similar to the non-iid scenario shown in Figure 1, it is observed that Gossip-PGA always converges faster (or has shorter transient stages) than Gossip SGD. When network size gets larger and henceβ →1, the superiority of Gossip-PGA gets more evident. Moreover, it is also noticed that the transient stage gap between Gossip SGDAccelerating Gossip SGD with Periodic Global Averaging 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Exponential Local SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Grid Local SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Ring Local SGD Gossip-PGA Parallel SGD Figure 6.Convergence comparison between Gossip-PGA, Local SGD and parallel SGD on the logistic regression problem in non-iid data distributed setting over the exponential graph (left), grid (middle) and ring (right) topology. 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Grid; H=16 Local SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Grid; H=32 Local SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Grid; H=64 Local SGD Gossip-PGA Parallel SGD Figure 7.Convergence comparison between Gossip-PGA, Local SGD and parallel SGD on the logistic regression problem in non-iid data distributed setting over the grid topology with period H = 16 (left), H = 32 (middle), H = 64 (right). and Gossip-PGA is smaller than the non-iid scenario in all three plots in Figure 4. All these observations are consistent with the transient stage comparisons in Table 2. Experiments on different topologies. Figure 5 illustrates how Gossip SGD and Gossip-PGA converges over the exponential graph, grid and ring topology. For all plots, it is observed that Gossip-PGA is no worse than Gossip SGD. Moreover, as the network gets sparser and hence β →1 from the left plot to right, it is observed that the superiority of Gossip-PGA gets more evident, which is consistent with the transient stage comparisons between Gossip SGD and Gossip-PGA in Table 2. Comparison with Local SGD. Figure 6 illustrates how Local SGD and Gossip-PGA converges over the exponential graph, grid and ring topology. The period is set as H = 16. In all three plots, Gossip-PGA always converges faster than Local SGD because of the additional gossip communications. Moreover, since the exponential graph has the smallest β, it is observed Gossip-PGA has almost the same convergence performance as parallel SGD. Figure 7 illustrates how Local SGD and Gossip-PGA converges over the grid topology with different periods. It is observed that Gossip-PGA can be signiﬁcantly faster when H is large. All these observations are consistent with the transient stage comparisons in Table 3. F.3. More experiments on image classiﬁcation. Training accuracy. Figure F.3 shows the iteration-wise and time-wise training accuracy curves of aforementioned algorithms separately. In the left ﬁgure, it is observed Gossip-PGA/AGA converges faster (in iteration) and more accurate than local and Gossip SGD, which is consistent with our theory. In the right ﬁgure, it is observed that Gossip-PGA/AGA is the fastest method (in time) that can reach the same training accuracy as parallel SGD. The effect of averaging period. Table 15 compares the top-1 accuracy in the validation set with a different averaging period setting in Gossip-PGA SGD. Compared to Gossip SGD, a relatively large global averaging period (48), roughly 2.1% iterations with global averaging can still result in 0.32% gain in validation accuracy. With a moderate global averaging period (6/12), the validation accuracy is comparable with the parallel SGD baseline. The communication overhead of global averaging can be amortized since it happens every H iterations.Accelerating Gossip SGD with Periodic Global Averaging PARALLEL SGD G OSSIP SGD G OSSIP -PGA PERIOD H - - 3 6 12 24 48 VAL ACC.(%) 76.22 75.34 76.19 76.28 76.04 75.68 75.66 Table 15.Comparison of Top-1 validation accuracy with different averaging period setting in Gossip-PGA. 0 2500 5000 7500 10000 12500 15000 17500 Iterations 0 10 20 30 40 50 60 70 80Training Acc 16000 18000 75.0 77.5 Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA 0 1000 2000 3000 4000 5000 6000 7000 8000 Training time 0 10 20 30 40 50 60 70 80Training Acc Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA Figure 8.Convergence results on the ImageNet classiﬁcation task.(a) Iteration-wise convergence in terms of training accuracy. (b) runtime-wise convergence speed in terms of training accuracy. Experiments on SGD optimizer (without momentum). In previous Imagenet training, Nesterov momentum SGD optimizer is used. Following common practice (Lian et al., 2017; Assran et al., 2019; Tang et al., 2018), we establish the convergence rate of the non-accelerated method while running experiments with momentum. For the sake of clarity, we further add a new series of experiments on Gossip-SGD without momentum, see Table 16. Gossip-PGA still outperform Gossip-SGD utilizing the SGD optimizer. METHOD ACC. % PARALLEL SGD 69.5 GOSSIP SGD 68.47 GOSSIP -PGA 69.21 Table 16.Comparison of validation accuracy of Imagenet training on different algorithms with SGD optimizer. G. Implementation of Gossip AGA Practical consideration. The Gossip-AGA algorithm is listed in Algorithm 2. We use a counter Cto record the number of gossip iterations since last global averaging. The global averaging period H is initialized to a small value Hinit (e.g. 2∼4). Once Cequals to current H, global averaging happens. In practice, we sample loss scores for the ﬁrst fewer iterations and get a Finit estimation in a running-average fashion. We remove the exponential term in the loss score ratio for ﬂexible period adjustment. H. Comparison of communication overhead between gossip and All-Reduce Table 17 compares the overhead of different communication styles in two deep training tasks. The implementation details follow Appendix F. For each proﬁling, we run a 500 iterations and take their average as the iteration time. As typically All-Reduce implementation containing overlapping between computation and communication, we run a series of separate experiments which do not perform communication (Column 2) to get communication overhead fairly (the ﬁgures in the brackets). For ResNet-50 training, gossip introduces 150ms communication overhead while All-Reduce needs 278ms. ForAccelerating Gossip SGD with Periodic Global Averaging Algorithm 2 Gossip-AGA Require: Initialize x0,i = x0, learning rate γ >0, topology matrix W for all nodes i ∈{1,2,...,n }, global averaging period H = Hinit, C ←0, Finit ←0, warmup iterations Kw for k= 0,1,2,...,T −1, every node ido C ←C+ 1 Sample mini-batch data ξ(k+1) i from local dataset Compute stochastic gradient ∇Fi(x(k) i ; ξ(k+1) i ) and loss Fi(x(k) i ; ξ(k+1) i ) Conduct local udpate x (k+ 1 2 ) i = x(k) i −γ∇Fi(x(k) i ; ξ(k+1) i ) if C == H then C ←0 x(k+1) i ←1 n ∑n j=1 xk+ 1 2 ,j F(xk; ξk) = 1 n ∑n i=1 Fi(xk,i; ξk,i) if k<K w then Finit ←1 2 (Finit + F(xk; ξk)) else H ← ⌈ Finit F(xk;ξk) Hinit ⌉ else x(k+1) i = ∑ j∈Ni wijx (k+ 1 2 ) j MODEL ITERATION TIME (MS) NO COMMUNICATION ALL-REDUCE GOSSIP RESNET-50 146 424 (278) 296 (150) BERT 445 1913.8 (1468.8) 1011.5 (566.5) Table 17.Comparison of communication overhead between gossip and All-Reduce in terms of runtime. BERT training, gossip introduces 566.5ms communication overhead while All-Reduce needs 1468.8ms with the tremendous model size of BERT-Large.",
      "meta_data": {
        "arxiv_id": "2105.09080v1",
        "authors": [
          "Yiming Chen",
          "Kun Yuan",
          "Yingya Zhang",
          "Pan Pan",
          "Yinghui Xu",
          "Wotao Yin"
        ],
        "published_date": "2021-05-19T11:59:25Z",
        "pdf_url": "https://arxiv.org/pdf/2105.09080v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the scalability issues of Gossip SGD in large-scale distributed training, particularly its slow convergence on sparse networks due to communication overhead. It introduces Gossip-PGA, which integrates Periodic Global Averaging into Gossip SGD to accelerate its convergence. The main contributions include: (1) Theoretical proof that Gossip-PGA significantly shortens the transient stage convergence from Ω(β^4n^3/(1-β)^4) to Ω(β^4n^3H^4) for non-convex problems, and demonstrates its superiority over Local SGD by controlling network topology influence with the averaging period H. (2) Establishment of comprehensive convergence rates for Gossip-PGA for both smooth convex and non-convex problems. (3) Proposition of Gossip-AGA, an adaptive extension of Gossip-PGA with convergence guarantees for dynamically adjusting averaging periods. (4) Extensive empirical validation showing that Gossip-PGA/AGA achieves similar convergence speed in iterations to parallel SGD but with 1.3 to 1.9 times faster runtime, while also remedying accuracy degradation observed in standard Gossip SGD and Local SGD.",
        "methodology": "The core methodology involves introducing \"Periodic Global Averaging\" (PGA) into the Gossip SGD framework, resulting in the Gossip-PGA algorithm. The algorithm combines three key steps: (1) Local SGD updates on each node. (2) If the current iteration (k+1) is a multiple of the global averaging period (H), a global average of all model parameters across all 'n' nodes is performed using efficient All-Reduce primitives (e.g., Ring All-Reduce). (3) Otherwise, a standard gossip step is performed where each node averages its parameters with its direct neighbors using a doubly stochastic weight matrix W. For further improvement, Gossip-AGA is proposed as an adaptive extension that dynamically adjusts the global averaging period H based on the observed training loss, typically increasing H as training progresses. The theoretical analysis establishes convergence rates for smooth convex and non-convex objective functions under standard assumptions, including L-smoothness, bounded gradient noise, and a strongly connected network characterized by the parameter β, leveraging descent and consensus lemmas.",
        "experimental_setup": "The experimental setup includes both convex optimization and large-scale deep learning tasks, performed on clusters of GPUs: (1) Convex Logistic Regression: Used for fundamental validation of transient stage convergence on different network topologies (ring, grid, exponential graph) and data distributions (IID and non-IID), with varying network sizes (n=20, 50, 100). (2) Image Classification: Trained a ResNet-50 model on the ImageNet-1k dataset (1.28M training, 50K validation images, 1000 classes) using up to 256 GPUs. The batch size was 256 per node (8192 total). Nesterov momentum SGD was used with a linear learning rate scaling, warmed up for 5 epochs and decayed at 30, 60, and 90 epochs over 120 total epochs. Inter-node communication utilized 25 Gbps TCP. Validation focused on Top-1 accuracy and wall-clock training time to reach 76% accuracy. (3) Language Modeling: Trained a BERT-Large model (330M parameters) on Wikipedia and BookCorpus datasets using 64 GPUs. The batch size was 64 per GPU (4096 total). LAMB optimizer was used with a polynomial learning rate decay and warm-up for Phase 1 training (112,608 steps). Validation focused on final training loss and runtime. Comparison baselines included Parallel SGD, Gossip SGD, Local SGD, and OSGP. For Gossip-AGA, the adaptive period was initialized to 4, with global averaging occurring for approximately 9-9.6% of iterations.",
        "limitations": "The limitations highlighted include: (1) The Gossip-PGA algorithm requires manual tuning of the global averaging period H, which can be challenging (though Gossip-AGA is proposed to address this adaptively). (2) Existing general topology-changing Gossip SGD analysis (Koloskova et al., 2020) fails to fully capture the theoretical benefits of periodic global averaging in Gossip-PGA, potentially leading to incomplete or counter-intuitive conclusions regarding its performance. (3) Related work, such as SlowMo (Wang et al., 2019), does not consider heterogeneous data distributions, making its analysis less applicable or incomparable to Gossip-PGA's results in such settings. (4) The paper's analysis and proposed methods do not inherently integrate other orthogonal communication efficiency techniques like quantization, gradient compression, or lazy communication within its immediate scope.",
        "future_research_directions": "The authors suggest that orthogonal communication efficiency techniques, such as quantization (e.g., QSGD, SignSGD), gradient compression, and lazy communication (e.g., LAG, communication-censored ADMM), can be integrated into Gossip-PGA/AGA to further enhance its performance and communication efficiency in distributed training."
      }
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors",
      "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood\naggregation that assigns weights to neighbors according to their importance. In\npractice, however, GATs are often unable to switch off task-irrelevant\nneighborhood aggregation, as we show experimentally and analytically. To\naddress this challenge, we propose GATE, a GAT extension that holds three major\nadvantages: i) It alleviates over-smoothing by addressing its root cause of\nunnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits\nfrom higher depth as it can still utilize additional layers for (non-)linear\nfeature transformations in case of (nearly) switched-off neighborhood\naggregation. iii) By down-weighting connections to unrelated neighbors, it\noften outperforms GATs on real-world heterophilic datasets. To further validate\nour claims, we construct a synthetic test bed to analyze a model's ability to\nutilize the appropriate amount of neighborhood aggregation, which could be of\nindependent interest.",
      "full_text": "GATE: How to Keep Out Intrusive Neighbors Nimrah Mustafa 1 Rebekka Burkholz 1 Abstract Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neigh- borhood aggregation, as we show experimen- tally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over- smoothing by addressing its root cause of un- necessary neighborhood aggregation. ii) Simi- larly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non- )linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neigh- bors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to an- alyze a model’s ability to utilize the appropri- ate amount of neighborhood aggregation, which could be of independent interest. 1. Introduction Graph neural networks (GNNs) (Gori et al., 2005) are a standard class of models for machine learning on graph- structured data that utilize node feature and graph struc- ture information jointly to achieve strong empirical per- formance, particularly on node classification tasks. In- put graphs to GNNs stem from various domains of real- world systems such as social (Bian et al., 2020), com- mercial (Zhang & Chen, 2020), academic (Hamaguchi et al., 2017), economic (Monken et al., 2021), biochem- ical(Kearnes et al., 2016), physical (Shlomi et al., 2021), and transport (Wu et al., 2019) networks that are diverse in their node feature and graph structure properties. 1CISPA Helmholtz Center for Information Security, 66123 Saarbr¨ucken, Germany. Correspondence to: Nimrah Mustafa <nimrah.mustafa@cispa.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). The message-passing mechanism of GNNs (Kipf & Welling, 2017; Xu et al., 2019) involves two key steps: a transformation of the node features, and the aggregation of these transformed features from a node’s neighborhood to update the node’s representation during training. While this has proven to be largely successful in certain cases, it gen- erally introduces some problems for learning with GNNs, the most notorious of which is over-smoothing (Li et al., 2018). The enforced use of structural information in ad- dition to node features may be detrimental to learning the node classification task, as shown by recent results where state-of-the-art GNNs perform the same as or worse than multi-layer perceptrons (MLPs) (Gomes et al., 2022; Yan et al., 2022; Ma et al., 2022). One such task is where node labels can be easily determined by informative node fea- tures and require no contribution from the neighborhood. Here, standard neighborhood aggregation, as in most GNN architectures, would impair model performance, particu- larly with an increase in model depth. A popular standard GNN architecture that, in principle, tries to resolve this problem is the Graph Attention Net- work (GAT) (Veliˇckovi´c et al., 2018; Brody et al., 2022). By design, neighborhood aggregation in GATs is charac- terized by learnable coefficients intended to assign larger weights to more important neighboring nodes (including the node itself) in order to learn better node representations. Therefore, in the above example, GATs should ideally re- sort to assigning near-zero importance to neighbor nodes, effectively switching off neighborhood aggregation. How- ever, we find that, counter-intuitively, GATs are unable to do this in practice and continue to aggregate the uninforma- tive features in the neighborhood which impedes learning. One may ask why one would employ a GAT (or any GNN architecture) if an MLP suffices. In practice, we do not know whether neighborhood aggregation (of raw features or features transformed by a perceptron or MLP), would be beneficial or not beforehand. This raises a pertinent ques- tion for the GNN research community: How much neigh- borhood aggregation is needed for a given task? . Ideally, it is what we would want a model to learn. Otherwise, the right task-specific architecture would need to be identified by time and resource-intensive manual tuning. We address the challenge faced by GAT to effectively de- 1 arXiv:2406.00418v2  [cs.LG]  29 Jul 2024GATE: How to Keep Out Intrusive Neighbors termine how well a node is represented by its own features compared to the features of neighboring nodes, i.e., dis- tinguish between the relative importance of available node features and graph structure information for a given task. Firstly, we provide an intuitive explanation for the problem based on a conservation law of GAT gradient flow dynam- ics derived by Mustafa & Burkholz (2023). Building on this insight, we present GATE, an extension of the GAT architecture that can switch neighborhood aggregation on and off as necessary. This allows our proposed architecture to gain the following advantages over GAT: 1. It alleviates the notorious over-smoothing problem by addressing the root cause of unnecessarily repeated neighborhood aggregation. 2. It allows the model to benefit from more meaningful representations obtained solely by deeper non-linear transformations, similarly to perceptrons, in layers with little to no neighborhood aggregation. 3. It often outperforms GATs on real-world heterophilic datasets by weighing down unrelated neighbors. 4. It offers interpretable learned self-attention coeffi- cients, at the node level, that are indicative of the rel- ative importance of feature and structure information in the locality of the node. In order to validate these claims, we construct a syn- thetic test bed of two opposite types of learning problems for node classification where label-relevant information is completely present only in a node’s i) own features and ii) neighboring nodes’ features (see Fig. 2). GATE is able to adapt to both cases as necessary. On real-world datasets, GATE performs competitively on homophilic datasets and is substantially better than GAT on heterophilic datasets. Furthermore, up to our knowledge, it achieves a new state of the art on the relatively large OGB-arxiv dataset (Hu et al., 2021) (i.e., 79.57 ± 0.84% test accuracy). In sum- mary, our contributions are as follows: • We identify and experimentally demonstrate a struc- tural limitation of GAT, i.e., its inability to switch off neighborhood aggregation. • We propose GATE, an extension of GAT, that over- comes this limitation and, in doing so, unlocks several benefits of the architecture. • We update an existing conservation law relating the structure of gradients in GAT to GATE. • We construct a synthetic test bed to validate our claims, which could be of independent interest to mea- sure progress in developing adaptive neighborhood aggregation schemes. Figure 1: MLP only performs node feature transforma- tions, whereas GAT also always aggregates over the neigh- borhood. With the ability to switch off neighborhood ag- gregation, GATE can learn to emulate MLP behavior and potentially interleave effective perceptron and standard lay- ers in a flexible manner. This allows for more expressive power that we find to benefit real-world tasks (see Table 3). 2. Related Work To relieve GNNs from the drawbacks of unnecessarily re- peated neighborhood aggregation in deeper models, ini- tial techniques were inspired by classical deep learning of MLPs such as normalization (Cai et al., 2021; Zhao & Akoglu, 2020; Zhou et al., 2020; 2021) and regularization (Papp et al., 2021; Rong et al., 2020; Yang et al., 202; Zou et al., 2019). More recently, the need for deeper models and architectural changes to limit neighborhood aggregation as necessary has been recognized. Some approaches use linear combi- nations of initial features and current layer representation (Gasteiger et al., 2019), add skip connections and identity mapping (Chen et al., 2020; Cong et al., 2021), combine representations of all previous layers at the last layer (Xu et al., 2018), aggregate information from a node-wise de- fined range of k-hop neighbors(Liu et al., 2020), and limit the number of aggregation iterations based on node influ- ence scores (Zhang et al., 2021). However, these architec- tures are not flexible enough to utilize additional network layers to simulate perceptron behavior, which, as we find, benefits heterophilic tasks. (Ma et al., 2023) discuss ‘good’ and ‘bad’ heterophily, which are also task-dependent. Other contemporary works for general GNNs propose the use of bilevel optimization to determine a node’s strategic discrete action to a received message (Finkelshtein et al., 2023) and a variational inference framework for adaptive message passing (Errica et al., 2023). While these non- attentive architectures improve message passing in GNNs, we focus on identifying and explaining a structural limita- tion of self-attention in GAT, that continues to be used as a strong baseline architecture. An orthogonal line of research uses graph structural learn- ing (Yang et al., 2019; Stretcu et al., 2019; Franceschi et al., 2020) to amend the input graph structure such that neighborhood aggregation benefits the given task. Such 2GATE: How to Keep Out Intrusive Neighbors approaches are difficult to scale, more susceptible to over- smoothing, and potentially destroy any inherent informa- tion in the original graph structure. On the contrary, a standard GNN architecture equipped to selectively per- form neighborhood aggregation avoids these pitfalls. Self- supervision of the attention mechanism has also been pro- posed (Wang et al., 2019; Kim & Oh, 2021). Methods such as graph rewiring (Deac et al., 2022) to overcome problems such as over-squashing (Alon & Yahav, 2021) are comple- mentary and may also be combined with GATE. While we focus our insights on GAT, architectures based on GAT such as ωGAT (Eliasof et al., 2023) also suffer from the same problem (see Fig. 14 in Appendix C). This further confirms that the universal problem with GAT has been correctly identified. In general, recent works direct ef- fort to understand the current limitations of graph attention (Lee et al., 2023; Fountoulakis et al., 2023). 3. Architecture Notation Consider a graph G = ( V, E) with node set V and edge set E ⊆ V × V, where for a node v ∈ V the neighborhood is N(v) = {u|(u, v) ∈ E} and input features are h0 v. A GNN layer updates each node’s repre- sentation by aggregating over its neighbors’ representation and combining it with its own features. The aggregation and combination steps can be performed together by in- troducing self-loops in G such that, ∀v ∈ V, (v, v) ∈ E. We assume the presence of self-loops in G unless spec- ified otherwise. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indi- cate the importance of node u for v. A network is con- structed by stacking L layers, defined as follows, using a non-linear activation function ϕ that is homogeneous (i.e ϕ(x) = xϕ′(x)) and consequently, ϕ(ax) = aϕ(x) for positive scalars a) such as ReLU ϕ(x) = max {x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. GAT Given input representations hl−1 v for v ∈ V, a GAT 1 layer l ∈ [L] transforms those to: hl v = ϕ   X u∈N(v) αl uv · Wl shl−1 u  , where (1) αl uv = exp \u0000 el uv \u0001 P u′∈N(v) exp \u0000 el u′v \u0001, and (2) el uv = \u0000 al\u0001⊤ · ϕ \u0000 Wl shl−1 u + Wl thl−1 v \u0001 (3) The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that Ws = Wt. We denote this variant of GAT by GATS. 1Throughout, we refer to GATv2 (Brody et al., 2022) as GAT. GATE In addition, we propose GATE, a GAT variant that flexibly weights the importance of node features and neigh- borhood features. A GATE layer is also defined by Eq. (1) and (2) but modifies euv in Eq. (3) to Eq. (4). el uv = \u0000 1u̸=val s + 1u=val t \u0001⊤ ·ϕ \u0000 Ulhl−1 u + Vlhl−1 v \u0001 . (4) We denote euv in Eq. (3) and (4) as el vv if u = v. For GATE, Wl s in Eq. (1) is denoted as Wl. A weight- sharing variant of GATE, GATE S, is characterized by all feature transformation parameters being shared in a layer (i.e. Wl = Ul = Vl). Note that, then, for a d-dimensional layer, GATE adds onlyd more parameters to GAT. We next present theoretical insights into the reasoning be- hind the inability of GATs to switch off neighborhood ag- gregation, which is rooted in norm constraints imposed by the inherent conservation law for GATs. The gradients of GATE fulfill an updated conservation law (Thoerem 4.3) that enables switching off neighborhood aggregation in a parameter regime with well-trainable attention. 4. Theoretical Insights For simplicity, we limit our discussion here to GATs with weight sharing. We derive similar arguments for GATs without weight sharing in Appendix A.1. The following conservation law was recently derived for GATs to explain trainability issues of standard initialization schemes. Even with improved initializations, we argue that this law lim- its the effective expressiveness of GATs and their ability to switch off neighborhood aggregation when necessary. Theorem 4.1 (Thm. 2.2 by Mustafa & Burkholz (2023)) . The parameters θ of a layer l ∈ [L − 1] in a GAT network and their gradients ∇θL w.r.t. loss L fulfill: ⟨Wl [i,:], ∇Wl [i,:] ⟩ = ⟨Wl+1 [:,i] , ∇Wl+1 [:,i] ⟩ + ⟨al [i], ∇al [i] ⟩. (5) Intuitively, this equality limits the budget for the relative change of parameters and imposes indirectly a norm con- straint on the parameters. Under gradient flow that assumes infinitesimally small learning rates, this law implies that the relationship \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c stays constant during training, where c is defined by the initial norms. Other gradient-based optimizers fulfill this norm balance also approximately. Note that the norms\r\rWl[i, :] \r\r generally do not assume arbitrary values but are determined by the required scale of the output. Deeper models are especially less flexible in varying these norms as deviations could lead to exploding or diminishing out- puts and/or gradients. In consequence, the norms of the attention parameters are also bounded. Furthermore, a pa- rameter becomes harder to change during training when its 3GATE: How to Keep Out Intrusive Neighbors magnitude increases. This can be seen by transforming the law with respect to the relative change of a parameter de- fined as ∆θ = ∇θL/θ for θ ̸= 0 or ∆θ = 0 for θ = 0. nl+1X j=1 Wl ij 2 ∆Wl ij = nl+2X k=1 Wl+1 ki 2 ∆Wl+1 ki + al i 2 ∆al i. (6) The higher the magnitude of an attention parameter (al i)2, the smaller will be the relative change ∆al i and vice versa. This restricts the attention mechanism in the network to a less-trainable regime without converging to a meaningful model. We next explain why large \r\ral\r\r are required to switch off neighborhood aggregation in a layer. Insight 4.2 (Effective expressiveness of GATs). GATs are challenged to switch off neighborhood aggregation during training, as this requires the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. An intuitive derivation of this insight is presented in the appendix. Here, we outline the main argument based on the observation that to make the contribution of neighbor j insignificant relative to node i, we require αij/αii << 1. We use relativeαij/αii instead of αij and αii to cancel out normalization constants and simplify the analysis. Our key observation is that, given an insignificant link (i, j), its relative contribution to its two neighborhoods αij/αii << 1 and αji/αjj << 1 are affected in oppo- site ways by a feature f of the attention parameters a, i.e. if a[f] contributes to reducing αij/αii, it automatically in- creases αji/αjj . However, we require multiple features that contribute to reducing only αij without strengthen- ing αji that may only be possible in a high dimensional space requiring large norms of a. Yet, the norms ∥a∥2 are constrained by the parameter initialization and cannot in- crease arbitrarily due to the derived conservation law. Note that to switch off all neighborhood aggregation, we require αij/αii << 1, ∀ j ∈ N(i), further complicating the task. To address this challenge, we modify the GAT architecture by GATE that learns separate attention parameters for the node and the neighborhood contribution. As its conserva- tion law indicates, it can switch off neighborhood aggrega- tion in the well-trainable parameter regime. Theorem 4.3 (Structure of GATE gradients). The param- eters and gradients of a GATE network w.r.t. to loss L for layer l ∈ [L − 1] are conserved according to the following laws. Given Θ(θ) = ⟨θ, ∇θL⟩, it holds that: Θ(Wl [i,:]) − Θ(al+1 s [i]) − Θ(al+1 t [i]) = Θ(Wl+1 [:,i] ). (7) and, if additional independent matrices Ul and Vl are trainable, it also holds that: Θ(al s[i]) + Θ(al t[i]) = Θ(Ul [i,:]) + Θ(Vl [i,:]). (8) The proof is provided in the appendix. We utilize this the- orem for two purposes. Firstly, it induces an initialization that enables at least the initial trainability of the network. Similarly to GAT (Mustafa & Burkholz, 2023), we initial- ize all a parameters with zero and matrices W with ran- dom orthogonal looks-linear structure in GATE. This also ensures that we have no initial inductive bias or preference for specific neighbor or node features. As an ablation, we also verify that the initialization of the attention parameters in GAT with zero alone can not switch off neighborhood aggregation in GAT (see Fig. 7 in Appendix C). Secondly, the conservation law leads to the insight that a GATE network is more easily capable of switching off neighborhood aggregation or node feature contributions in comparison with GAT. Insight 4.4 (GATE is able to switch off neighborhood ag- gregation.). GATE can flexibly switch off neighborhood aggregation or node features in the well-trainable regime of the attention parameters. This insight follows immediately from the related conser- vation law for GATE that shows that at and as can inter- change the available budget for relative change among each other. Furthermore, the contribution of neighbors and the nodes are controlled separately. We show how the respec- tive switch-off can be achieved with relatively small atten- tion parameter norms that correspond to the well-trainable regime in Appendix A.3. To verify these insights in exper- iments, we next design synthetic data generators that can test the ability of GNNs to take structural infromation into account in a task-appropriate manner. 5. Experiments We validate the ability of GATE to perform the appropri- ate amount of neighborhood aggregation, as relevant for the given task and input graph, on both synthetic and real- world graphs. In order to gauge the amount of neighbor- hood aggregation, we study the distribution of αvv values (over the nodes) at various epochs during training and lay- ers in the network. This serves as a fair proxy since ∀ v ∈ V, αvv = 1 − P u∈N(v),u̸=v αuv. Thus, αvv = 1 implies no neighborhood aggregation (i.e. only hv is used) whereas αvv = 0 implies only neighborhood aggregation (i.e. hv is not used). Figure 2 shows an exemplary con- struction of both these cases. We defer a discussion of the experimental setup to Appendix B. 5.1. Synthetic Test Bed We construct the synthetic test bed as a node classification task for two types of problems: self-sufficient learning and neighbor-dependent learning. In the self-sufficient learning problem, complete label-relevant information is present in 4GATE: How to Keep Out Intrusive Neighbors (a) No neighborhood contribution required. (b) Only neighborhood contribution required. Figure 2: Examples of synthetic input graphs constructed for learning tasks that are (a) self-sufficient and can be bet- ter solved by switching off neighborhood aggregation, i.e. αvv = 1 and (b) neighbor-dependent that benefit from ig- noring the node’s own features, i.e.αvv = 0. In both cases, ∀ v ∈ V, P u∈N(v),u̸=v αuv + αvv = 1. These represent opposite ends of the spectrum whereas real-world tasks of- ten lie in between and require αii ∈ [0, 1]. GATE’s atten- tion mechanism is more flexible than GAT’s in learning the level of neighborhood aggregation required for a task. a node’s own features. On the contrary, in the neighbor- dependent learning problem, label-relevant information is present in the node features of the k-hop neighbors. We discuss both cases in detail, beginning with the simpler one. Learning self-sufficient node labels In order to model this task exactly, we generate an Erd ˝os–R´enyi (ER) graph structure G with N = 1000 nodes and edge probabil- ity p = 0 .01. Node labels yv are assigned uniformly at random from C = [2 , 8] classes. Input node features h0 v are generated as one-hot encoded node labels in both cases, i.e., h0 v = 1yv . Nodes are divided randomly into train/validation/test split with a 2 : 1 : 1 ratio. We also use a real-world graph structure of the Cora dataset. Two cases using this graph structure are tested: i) using the original node labels consisting of7 classes, and ii) randomized labels of 7 classes. Input node features are generated as one-hot encoding of node labels in both cases. The standard splits of Cora are used. Table 1: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of net- work layers, respectively. Original (Orig.) and randomized (Rand.) labels are used for the Cora structure. In all cases, 100% train accuracy is achieved except in ones marked with ∗ and GATE eventually achieves 100% test accuracy a few epochs later except in one marked with ‡. GAT S and GAT models marked with † also eventually achieve 100% test accuracy. GATE S behaves similarly to GATE and achieves 100% train and test accuracy. . S C L Test Acc.@Epoch of Min. Train Loss GATS GAT GATE Cora O,7 1 99.1@215† 97.7@166† 99.0@127 2 93.4@218 94.5@158 99.6@35 5 85.9@92 85.5@72 98.4@36‡ R,7 1 99.4@263 † 99.8@268† 100@104 2 61.7@2088 ∗ 52.8@341∗ 99.9@36 5 35.1@609 32.1@1299 99.9@23 ER(p = 0.01) R,2 1 100@341 † 100@182† 100@1313 2 99.2@100 † 99.2@119† 99.6@79 5 64.0@7778 ∗ 99.6@239 100@45 R,8 1 88.8@9578 ∗ 98.4@3290 99.2@1755 2 90.4@2459 ∗ 94.8@2237 99.6@44 5 23.6@8152 26.0@8121 100@28 As evident in Table 1, GAT is unable to perfectly learn this task whereas GATE easily achieves100% train and test ac- curacy, and often in fewer training epochs. In line with the homophilic nature of Cora, GAT achieves reasonably good accuracy when the original labels of the Cora graph structure are used as neighborhood aggregation is relatively less detrimental, particularly in a single-layer model. Nevertheless, in the same case, GATE generalizes better than GAT with an increase in model depth. This in- dicates that over-smoothing, a major cause of performance degradation with model depth in GNNs, is also alleviated due to reduced neighborhood aggregations (see Fig. 3). On the contrary, random labels pose a real challenge to GAT. Since the neighborhood features are fully uninforma- tive about a node’s label in the randomized case, aggrega- tion over such a neighborhood distorts the fully informa- tive features of the node itself. This impedes the GAT net- work from learning the task, as it is unable to effectively switch off aggregation (see Fig. 3), whereas GATE is able to adapt to the required level of neighborhood aggregation (i.e. none, in this case). Interestingly, note that a single 5GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 3: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 1 and 2 layer models. Due to space limitation, we defer the plots of 5 layer networks to Fig. 8 in Appendix C. layer GAT in the case of random labels can almost, though not completely, switch off neighborhood aggregation (see Fig. 3) and achieve (near) perfect accuracy in the simpler cases. This is in line with our theoretical analysis (see In- sight 4.2), as the norms of a single-layer model are not con- strained and thus the attention parameters have more free- dom to change. Overall, the accuracy of GAT worsens drastically along two dimensions simultaneously: i) an increase in the depth of the model (due to increased unnecessary aggregation), and ii) an increase in the complexity of the task (due to an in- crease in the number of classes in an ER graph and conse- quently in node neighborhoods). In the interest of space, we defer results for GATE S to Fig. 9 in Appendix C as aggregation patterns similar to GATE are observed. Having established that GATE excels GAT in avoiding task-irrelevant neighborhood aggregation, it is also impor- tant to verify whether GATE can perform task-relevant neighborhood aggregation when required, and as much as required. We answer this question next by studying the be- havior of GATE, in comparison to GAT, on a synthetically constructed neighbor-dependent learning problem. Learning neighbor-dependent node labels To model this task, we generate an ER graph structure with N = 1000 nodes and edge probability p = 0 .01. Input node features h0 v ∈ Rd are sampled from a multivariate normal distribution N(0d, Id). For simplicity, d = 2. This input graph G is fed to a random GAT network Mk with k layers of width d. Note that this input graph G has no self-loops on nodes (i.e. v /∈ N(v)).The parameters of Mk are initialized with the standard Xavier (Glorot & Ben- gio, 2010) initialization. Thus, for each node v, the node embedding output by Mk, hMk v is effectively a function f Table 2: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Mean test accuracy ±95% confi- dence interval over 5 runs is reported. In all cases, a GATE variant outperforms the GAT variants. We further analyze one experimental run in detail in Table 8 in Appendix C. k L GATS GAT GATE S GATE 1 1 93 .6±1.3 92 .3±1.3 96.4 ± 0.7 93.5±1.3 2 93 .5±0.7 92 .7±2.7 97.9 ± 0.8 94.6±2.1 3 88 .2±4.9 91 .8±3.4 92 .1±4.6 94.0 ± 1.5 2 2 90 .4±1.3 87 .7±1.6 93.8 ± 0.5 88.7±2.5 3 82 .2±4.5 88 .9±2.1 85 .8±2.5 93.4 ± 3.3 4 84 .0±5.0 83 .0±4.8 89.2 ± 2.3 87.8±2.4 3 3 84 .3±3.2 83 .8±2.7 87 .5±1.8 88.6 ± 2.0 4 71 .4±3.9 75 .9±7.6 89.2 ± 1.0 89.0±0.5 5 80 .2±4.8 83 .9±2.2 86 .1±0.8 87.8 ± 1.6 of the k-hop neighboring nodes of node v represented by a random GAT network. Let Nk(v) denote the set of k-hop neighbors of v and v /∈ Nk(v). Finally, we run K-means clustering on the neighborhood aggregated representation of nodes hMk v to divide nodes into C clusters. For simplicity, we set C = 2. This cluster- ing serves as the node labels (i.e. yv = argc∈[C](v ∈ c) for our node classification task. Thus, the label yv of a node v to be learned is highly dependent on the input features of the neighboring nodes h0 u ∈ Nk(v) rather than the node’s own input features h0 v. The generated input data and the real decision boundary for varying k are shown in Fig. 4. Corresponding results in Table 2 and Fig. 5 exhibit that GATE can better detect the amount of necessary neighborhood aggregation than GAT. However, this task is more challenging than the previous one, and GATE too can not achieve perfect100% test accu- racy. This could be attributed to data points close to the real decision boundary which is not crisply defined (see Fig. 4). 5.2. Real-World Data To demonstrate that the ability of GATE to switch-off neighborhood aggregation has real application relevance, we evaluate GATE on relatively large-scale real-world node classification tasks, namely on five heterophilic benchmark datasets (Platonov et al., 2023) (see Table 3) and three OGB datasets (Hu et al., 2021) (see Table 5). We defer results and discussion on five small-scale datasets with varying homophily levels to Table 7 in Appendix C. To analyze 6GATE: How to Keep Out Intrusive Neighbors (a) k = 1  (b) k = 2  (c) k = 3 Figure 4: (a)-(c): Distribution of node labels of a synthetic dataset, with neighbor-dependent node labels, based on nodes’ own random features (left) and neighbors’ features aggregated k times (right). Figure 5: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 3. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 3, 4, and 5 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 3 layers of the 4 and 5 layer models perform neighborhood aggregation. the potential benefits of combining MLP and GAT layers in GATE, we compare its behavior with GAT and MLP. We argue that the better performance of GATE, by a large margin in most cases, can be attributed to down-weighting unrelated neighbors, leveraging deeper non-linear feature transformations, and reducing over-smoothing. While we focus our exposition on the neighborhood aggre- gation perspective of GATs, we also consider the FAGCN architecture (Bo et al., 2021), which relies on a similar at- tention mechanism and, in theory, could switch off neigh- borhood aggregation when positive and negative contribu- tions of neighbors cancel out. In contrast to GATE, it re- quires tuning a hyperparameter ϵ, which controls the con- tribution of raw node features to each layer. Furthermore, on our synthetic tasks, we find that, like GAT, FAGCN is also unable to limit neighborhood contribution. We also provide a detailed qualitative and quantitative discussion comparing GATE and FAGCN using the synthetic testbed in Appendix C. Next, we analyze GATE’s ability to mix MLP and GAT layers. To this end, we evaluate a GNN architecture con- structed by alternately placing GAT and MLP layers in the network that we denote by MLP +GAT on various het- erophilic tasks. The purpose of this experiment is twofold. Firstly, we observe in Table 3 that MLP+GAT outperforms both GAT and MLP in most cases. This highlights the benefit of only performing non-linear transformations on raw or aggregated neighborhood features without immedi- ate further neighborhood aggregation to learn potentially more complex features. Secondly, we find GATE to outper- form MLP+GAT (see Table 3). This illustrates that rigidly embedding MLP layers in a GNN with arbitrary predefined roles is not ideal as the appropriate degree and placement of neighborhood aggregation is unknown a-priori. In con- trast, GATE offers more flexibility to learn intricate com- binations of GNN layers and nonlinear feature transforma- tions that define more adequate models for a given task, as exemplified in Fig. 6. The distributions ofαvv in Fig. 6 across layers in GATE re- veal information about the relative importance of node fea- ture and graph structure at the node level, which allows us to analyze the question to which degree graph information is helpful for a task. For example, in Fig. 6, we observe that the αvv values are mostly lower in the minesweeper dataset than the roman-empire dataset. This indicates that aggregation, particularly over the input node features and the final layer’s learned representations, is more beneficial compared to the node’s own features for the minesweeper dataset. On the other hand, for roman-empire, the model has a higher preference to utilize features of the node it- self (as most values of αvv approach 1) over features of the neighbors. This aligns with the homophily levels, 0.05 and 0.68, of the roman-empire and minesweeper datasets, re- spectively. A similar analysis for datasets Texas and Actor can be found in Fig. 13 in Appendix C. We also observe in Fig. 6 that when neighborhood aggrega- 7GATE: How to Keep Out Intrusive Neighbors Table 3: We report mean test accuracy ±95% confidence interval for roman-empire and amazon-ratings and AUC-ROC for the other three datasets over the standard 10 splits, following (Platonov et al., 2023). All architectures were run with networks of depth 5 and 10 layers. The better performance for each architecture is shown with the number of network layers used in parentheses. GATE outperforms GAT and other baselines on all datasets, mostly by a significant margin. roman-empire amazon-ratings questions minesweeper tolokers GAT 26.10 ± 1.25 (5) 45 .58 ± 0.41 (10) 57 .72 ± 1.58 (5) 50 .83 ± 0.41 (5) 63 .57 ± 1.03 (10) MLP 65.12 ± 0.25 (5) 43 .26 ± 0.34 (5) 59 .44 ± 0.94 (10) 50 .74 ± 0.56 (5) 62 .67 ± 1.06 (10) MLP+GAT 70.83 ± 0.39 (5) 45 .25 ± 0.17 (10) 59 .12 ± 1.57 (10) 60 .07 ± 1.11 (5) 65 .85 ± 0.64 (10) FAGCN 67.55 ± 0.81 (5) 42 .85 ± 0.83 (10) 60 .38 ± 1.21 (5) 63 .38 ± 0.91 (10) 60 .89 ± 1.12 (5) GATE 75.55 ± 0.30 (5) 45.73 ± 0.24(10) 62.95 ± 0.71 (5) 66.14 ± 1.57 (5) 66.63 ± 1.15(10) (a) roman-empire, GAT:28.96% test accuracy.  (b) roman-empire, GATE: 75.94% test accuracy. (c) Minesweeper, GAT:50.50% test AUROC.  (d) Minesweeper, GATE:67.57% test AUROC. Figure 6: Distribution of αvv against training epoch for a run of 5 layer networks on real-world heterophilic tasks. As ex- pected, GAT is unable to significantly vary neighborhood aggregation whereas GATE exhibits varying aggregation patterns across layers and tasks. These could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods for a given task. We defer the plots of the 10-layer models for tolokers dataset to Fig. 11 in Appendix C. tion takes place, the level of aggregation across all nodes, as indicated by the shape of αvv distribution, varies over network layers. This is expected as different nodes need different levels of aggregation depending on where they are situated in the graph topology. For example, peripheral nodes would require more aggregation than central nodes to obtain a similar amount of information. Therefore, as already observed with purposefully constructed synthetic data, GATE offers a more interpretable model than GAT in a real-world setting. While we focus our evaluation in Table 3 on comparison with the most relevant baselines such as attention-based ar- chitectures, we next present a more extensive comparison with 14 other baseline architectures in Table 4. For the re- sults reported in Table 3, we conduct experiments in a sim- ple setting without additional elements that may impact the performance such as skip connections, normalization, etc., to isolate the effect of the architecture and evaluate solely the impact of GATE’s ability to switch off neighborhood aggregation on real-world data. However, for the results in Table 4, we adopt the original codebase of (Platonov et al., 2023), which utilizes such elements to evaluate the performance of baseline GNNs and architectures specifi- cally designed for heterophilic datasets. We evaluate GATE in the same settings optimized for their experiments. For easy comparison, we replicate their results from Table 4 in (Platonov et al., 2023). We observe in Table 4 that while GATE outperforms GAT (and other baselines) significantly, GATE has comparable performance to GAT-sep, a variant of GAT, despite GATE being more parameter efficient by an order of magnitude. More specifically, GAT-sep and GATE introduce d2 and d additional parameters, respectively, in a layer. By corre- spondingly adapting GATE, we find GATE-sep to achieve the best performance in most cases. Therefore, additional techniques generally employed to boost performance are compatible and complementary to GATE. GATE’s ability to benefit from depth in terms of generaliza- tion is demonstrated on OGB datasets (see Table 5). In par- 8GATE: How to Keep Out Intrusive Neighbors Table 4: An extensive comparison of GATE with baseline GNNs using the experimental setup of (Platonov et al., 2023). Accuracy is reported for roman-empire and amazon-ratings, and ROC AUC is reported for the remaining three datasets. roman-empire amazon-ratings minesweeper tolokers questions GATE 89.51 ± 0.49 52.49 ± 0.46 92.82 ± 0.90 84.62 ± 0.69 78.46 ± 1.17 GAT 80.87 ± 0.30 49.09 ± 0.63 92.01 ± 0.68 83.70 ± 0.47 77.43 ± 1.20 GATE-sep 89.78 ± 0.54 54.51 ± 0.38 94.18 ± 0.43 84.48 ± 0.57 78.20 ± 1.00 GAT-sep 88.75 ± 0.41 52.70 ± 0.62 93.91 ± 0.35 83.78 ± 0.43 76.79 ± 0.71 GT 86.51 ± 0.73 51.17 ± 0.66 91.85 ± 0.76 83.23 ± 0.64 77.95 ± 0.68 GT-sep 87.32 ± 0.39 52.18 ± 0.80 92.29 ± 0.47 82.52 ± 0.92 78.05 ± 0.93 GCN 73.69 ± 0.74 48.70 ± 0.63 89.75 ± 0.52 83.64 ± 0.67 76.09 ± 1.27 SAGE 85.74 ± 0.67 53.63 ± 0.39 93.51 ± 0.57 82.43 ± 0.44 76.44 ± 0.62 H2GCN 60.11 ± 0.52 36.47 ± 0.23 89.71 ± 0.31 73.35 ± 1.01 63.59 ± 1.46 CPGNN 63.96 ± 0.62 39.79 ± 0.77 52.03 ± 5.46 73.36 ± 1.01 65.96 ± 1.95 GPR-GNN 64.85 ± 0.27 44.88 ± 0.34 86.24 ± 0.61 72.94 ± 0.97 55.48 ± 0.91 FSGNN 79.92 ± 0.56 52.74 ± 0.83 90.08 ± 0.70 82.76 ± 0.61 78.86 ± 0.92 GloGNN 59.63 ± 0.69 36.89 ± 0.14 51.08 ± 1.23 73.39 ± 1.17 65.74 ± 1.19 FAGCN 65.22 ± 0.56 44.12 ± 0.30 88.17 ± 0.73 77.75 ± 1.05 77.24 ± 1.26 GBK-GNN 74.57 ± 0.47 45.98 ± 0.71 90.85 ± 0.58 81.01 ± 0.67 74.47 ± 0.86 JacobiConv 71.14 ± 0.42 43.55 ± 0.48 89.66 ± 0.40 68.66 ± 0.65 73.88 ± 1.16 ResNet 65.88 ± 0.38 45.90 ± 0.52 50.89 ± 1.39 72.95 ± 1.06 70.34 ± 0.76 ResNet+SGC 73.90 ± 0.51 50.66 ± 0.48 70.88 ± 0.90 80.70 ± 0.97 75.81 ± 0.96 ResNet+adj 52.25 ± 0.40 51.83 ± 0.57 50.42 ± 0.83 78.78 ± 1.11 75.77 ± 1.24 Table 5: Mean test accuracy ±95% confidence interval (and number of network layers). We replicate the results for GAT reported by (Brody et al., 2022). GATE leverages deeper networks to substantially outperform GAT. OGB- GAT GATE arxiv 71.87 ± 0.16 (3) 79.57 ± 0.84 (12) products 80.63 ± 0.46 (3) 86.24 ± 1.01 (8) mag 32.61 ± 0.29 (2) 35.29 ± .36 (5) ticular, GATE improves the SOTA test accuracy (78.03%) on the arxiv dataset achieved by a model using embed- dings learned by a language model instead of raw node features(Duan et al., 2023), as reported on the OGB leader- board. While the better performance of deeper models with limited neighborhood aggregation in certain layers indi- cates reduced over-smoothing, we also verify this insight quantitatively (see Table 9 in Appendix C). Our experimental code is available at https:// github.com/RelationalML/GATE.git. 6. Conclusion We experimentally illustrate a structural limitation of GAT that disables the architecture, in practice, to switch off task- irrelevant neighborhood aggregation. This obstructs GAT from achieving its intended potential. Based on insights from an existing conservation law of gradient flow dynam- ics in GAT, we have explained the source of this problem. To verify that we have identified the correct issue, we re- solve it with a modification of GAT, which we call GATE, and derive the corresponding modified conservation law. GATE holds multiple advantages over GAT, as it can lever- age the benefits of depth as in MLPs, offer interpretable, learned self-attention coefficients, and adapt the model to the necessary degree of neighborhood aggregation for a given task. We verify this on multiple synthetic and real- world tasks, where GATE significantly outperforms GAT and also achieves a new SOTA test accuracy on the OGB- arxiv dataset. Therefore, we argue that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. 9GATE: How to Keep Out Intrusive Neighbors Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. Bian, T., Xiao, X., Xu, T., Zhao, P., Huang, W., Rong, Y ., and Huang, J. Rumor detection on social media with bi- directional graph convolutional networks. In AAAI Con- ference on Artificial Intelligence, 2020. Bo, D., Wang, X., Shi, C., and Shen, H. Beyond low- frequency information in graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. Burkholz, R. and Dubatovka, A. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. In Graph Representation Learning Workshop, International Conference on Machine Learn- ing, 2020. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y ., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Simple and deep graph convolutional networks. InInternational Conference on Machine Learning, 2020. Cong, W., Ramezani, M., and Mahdavi, M. On provable benefits of depth in training graph convolutional net- works. In Advances in Neural Information Processing Systems, 2021. Deac, A., Lackenby, M., and Veliˇckovi´c, P. Expander graph propagation. In Learning on Graphs Conference, 2022. Duan, K., Liu, Q., Chua, T.-S., Yan, S., Ooi, W. T., Xie, Q., and He, J. Simteg: A frustratingly simple approach improves textual graph learning, 2023. Eliasof, M., Ruthotto, L., and Treister, E. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general framework to mitigate oversmoothing, over- squashing, and underreaching, 2023. Finkelshtein, B., Huang, X., Bronstein, M., and ˙Ismail ˙Ilkan Ceylan. Cooperative graph neural networks, 2023. Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. In Journal of Machine Learning Research, 2023. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learn- ing discrete structures for graph neural networks. In In- ternational Conference on Machine Learning, 2020. Gasteiger, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2019. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 9, pp. 249–256, May 2010. Gomes, D., Ruelens, F., Efthymiadis, K., Nowe, A., and Vrancx, P. When are graph neural networks better than structure agnostic methods? In Neural Information Pro- cessing Systems Workshop ICBINB, 2022. Gori, M., Monfardini, G., and Scarselli, F. A new model for learnig in graph domains. InIEEE International Joint Conference on Neural Networks, 2005. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach. In International Joint Conference on Artificial Intelligence, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021. 10GATE: How to Keep Out Intrusive Neighbors Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. Kim, D. and Oh, A. How to find your friendly neighbor- hood: Graph attention design with self-supervision. In International Conference on Learning Representations , 2021. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. InInternational Conference on Learning Representations, 2017. Lee, S. Y ., Bu, F., Yoo, J., and Shin, K. Towards deep attention in graph neural networks: Problems and reme- dies. In International Conference on Machine Learning, 2023. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Liu, M., Gao, H., and Ji, S. Towards deeper graph neu- ral networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2022. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2023. Monken, A., Haberkorn, F., Gopinatha, M., Freeman, L., and Batarseh, F. A. Graph neural networks for modeling causality in international trade. In AAAI Conference on Artificial Intelligence, 2021. Mustafa, N. and Burkholz, R. Are GATS out of balance? In Advances in Neural Information Processing Systems, 2023. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. In Advances in Neural Infor- mation Processing Systems, 2021. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations , 2020. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evalua- tion of gnns under heterophily: are we really making progress? In International Conference on Learning Rep- resentations, 2023. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node clas- sification. In International Conference on Learning Rep- resentations, 2020. Shlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. 2021. Stretcu, O., Viswanathan, K., Movshovitz-Attias, D., Pla- tanios, E., Ravi, S., and Tomkins, A. Graph agreement models for semi-supervised learning. In Advances in Neural Information Processing Systems, 2019. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations , 2018. Wang, G., Ying, R., Huang, J., and Leskovec, J. Improving graph attention networks with large margin-based con- straints. In Graph Representation Learning Workshop, Neural Information Processing Systems, 2019. Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Yan, Y ., Hashemi, M., Swersky, K., Yang, Y ., and Koutra, D. Two sides of the same coin: Heterophily and over- smoothing in graph convolutional neural networks. In IEEE International Conference on Data Mining, 2022. Yang, H., Ma, K., and Cheng, J. Rethinking graph regular- ization for graph neural networks. InAdvances in Neural Information Processing Systems, 202. Yang, L., Kang, Z., Cao, X., Jin, D., Yang, B., and Guo, Y . Topology optimization based graph convolutional net- work. In International Joint Conference on Artificial In- telligence, 2019. Zhang, M. and Chen, Y . Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. Zhang, W., Yang, M., Sheng, Z., Li, Y ., Ouyang, W., Tao, Y ., Yang, Z., and Cui, B. Node dependent local smooth- ing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. 11GATE: How to Keep Out Intrusive Neighbors Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth- ing in gnns. In International Conference on Learning Representations, 2020. Zhou, K., Huang, X., Li, Y ., Zha, D., Chen, R., and Hu, X. Towards deeper graph neural networks with differen- tiable group normalization. In Advances in Neural Infor- mation Processing Systems, 2020. Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in graph convolutional networks. InConfer- ence on Information and Knowledge Management, 2021. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 12GATE: How to Keep Out Intrusive Neighbors A. Theoretical Derivations A.1. Derivation of Insight 4.2 Statement (Restated Insight 4.2) . GATs are challenged to switch off neighborhood aggregation during training, as this would require the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. We have to distinguish GATs with and without weight sharing in our analysis. GATs with weight sharing: To investigate the ability of a GAT to switch off neighborhood aggregation, let us focus on a link(i, j) that should neither contribute to the feature transformation of i nor j. This implies that we need to find attention parameters a (and potentially feature transformations W) so that αij/αii << 1 with αij/αii = exp ( eij − eii). This implies that we require eij − eii << 0 and thus aT ϕ (W (hi + hj)) − 2aT ϕ (W (hi)) << 0. Since we also require αij/αjj << 1, it follows from adding both inequalities that aT [ϕ (W (hi + hj)) − (ϕ (Whi) + ϕ (Whj))] << 0. This inequality can only be fulfilled if there exists at least one feature f for which ∆fij ; = a[f] [ϕ (W[f, :] (hi + hj)) − (ϕ (W[f, :]hi) + ϕ (W[f, :]hi))] fulfills ∆fij << 0. Yet, note that if both ϕ (W[f, :]hi) and ϕ (W[f, :]hj) are positive or both are negative, we just get ∆fij = 0 because of the definition of a LeakyReLU. Thus, there must exist at least one feature f so that without loss of generality ϕ (W[f, :]hi) < 0 and ϕ (W[f, :]hj) > 0. It follows that if a[f] > 0 that 0 > a[f]ϕ (W[f, :]hi) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hj)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hj)) also receives a negative contribution that makes αij/αjj smaller. Yet, what happens to αij/αii? By distinguishing two cases, namely W[f, :] (hi + hj) > 0 or W[f, :] (hi + hj) < 0 and computing a[f] [ϕ (W (hi + hj)) − 2ϕ (W[f, :]hj)] > 0 we find the feature contribution to be positive. If a[f] < 0, then 0 > a[f]ϕ (W[f, :]hj) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hi)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hi)) and αij/αjj is reduced. Similarly, we can derive that at the same time αij/αii is increased, however. This implies that any feature that contributes to reducing ∆fij automatically increases one feature while it increases another. We therefore need multiple featuresf to contribute to reducing either αij/αii or αij/αjj to compensate for other increases. This implies, in order to switch off neighborhood aggregation, we would need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Yet, all these norms are constrained by the derived conservation law and therefore prevent learning a representation that switches off full neighborhoods. GATs without weight sharing: The flow of argumentation without weight sharing is very similar to the one above with weight sharing. Yet, we have to distinguish more cases. 13GATE: How to Keep Out Intrusive Neighbors Similarly to before, we require αij/αjj << 1 and αji/αii << 1. It follows from adding both related inequalities that aT [ϕ (Wshi + Wthj) + ϕ (Wshj + Wthi) − ϕ ((Ws + Wt) hi) − ϕ ((Ws + Wt) hj)] << 0. This implies that for at least one feature f, we require a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ ((Ws[f, :] + Wt[f, :]) hi) − ϕ ((Ws[f, :] + Wt[f, :]) hj)] << 0. (9) Again, our goal is to show that this feature automatically decreases the contribution of one feature while it increases another. As argued above, switching off neighborhood aggregation would therefore need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Our derived norm constraints, however, prevent learning such a model representation. Concretely, without loss of generality, we therefore have to show that if a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 < 0, (10) at the same time, we receive a[f][ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 > 0, (11) (or vice versa). In principle, we have to show this for 16 different cases of pre-activation sign configurations for the four terms in Eq. (9). Yet, since the argument is symmetric with respect to exchanging i and j, only 8 different cases remain. Two trivial cases are identical signs for all four terms. These are excluded, as the left hand side (LHS) of Eq. (9) would become zero and thus not contribute to our goal to switch off neighborhood aggregation. In the following, we will discuss the remaining six cases. Please note that for the remainder of this derivation α >0 denotes the slope of the leakyReLU and not the attention weights αij. 1. Case (+ − ++): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj > 0. From this assumption and the fact that ϕ is a leakyReLU it follows that the LHS of Eq. (9) be- comes: a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 ] = a[f](α −1)[Ws[f, :]hj + Wt[f, :]hi]. Since α −1 < 0 and [Ws[f, :]hj + Wt[f, : ]hi] < 0 according to our assumption, Eq. (9) demands a[f] < 0. To switch off neighborhood aggregation, we would need to be able to make the LHS of Eq. (10) and Eq. (11) Eq. (11) negative. Yet, a negative a[f] leads to a positive LHS of Eq. (11). Thus, the assumed sign configuration cannot support switching off neighborhood aggregation. 2. Case (+ − −−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hi + Wt[f, :]hj], which demands a[f] < 0. Accordingly, the LHS of Eq. (10) is clearly negative, while the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that implyWs[f, :]hj < Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj < 0 and Ws[f, :]hi + Wt[f, :]hj > 0. Again, this result implies that the considered sign configuration does not support switching off neighborhood aggregation. 3. Case (+ + + −): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi > 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes positive, which hampers switching-off neighborhood aggregation as discussed. 14GATE: How to Keep Out Intrusive Neighbors 4. Case (− − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj < 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. The LHS of Eq. (9) becomes a[f](α −1)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes clearly negative. However, the LHS of Eq. (11) is positive, asa[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that imply Ws[f, :]hj > Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj > 0 and Ws[f, :]hi + Wt[f, :]hj < 0. Again, this analysis implies that the considered sign configuration does not support switching off neighborhood aggregation. 5. Case (+ − +−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. According to our assumptions the LHS of Eq. (10) can only be negative if a[f] < 0. Yet, the LHS of Eq. (11) can only be negative if a[f] > 0. Thus, this case clearly cannot contribute to switching off neighborhood aggregation. 6. Case (+ − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. Eq. (9) becomes a[f](1 − α)Ws[f, :] (hi − hj) < 0. At the same time, the LHS of Eq. (10) simplifies to a[f]Ws[f, : ](hi − hj) and the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. Hence, a negative Eq. (9) leads to a positive Eq. (11). Accordingly, the last possible sign configuration also does not support switching off neighborhood aggregation, which concludes our derivation. A.2. Proof of Theorem 4.3 Statement (Restated Theorem 4.3). The gradients and parameters of GATE for layer l ∈ [L − 1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al+1 s [i], ∇al+1 s [i]L⟩ + ⟨al+1 t [i], ∇al+1 t [i]L⟩. (12) and, if additional independent matrices Ul and Vl are trainable, it also holds ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩ = ⟨Ul[i, :], ∇Ul[i,:]L⟩ + ⟨V l[i, :], ∇V l[i,:]L⟩. (13) The proof is analogous to the derivation of Theorem 2.2 by (Mustafa & Burkholz, 2023) that is restated in this work as Theorem 4.1. For ease, we replicate their notation and definitions here. Statement (Rescale invariance: Def 5.1 by Mustafa & Burkholz (2023)). The loss L(θ) is rescale-invariant with respect to disjoint subsets of the parametersθ1 and θ2 if for everyλ >0 we haveL(θ) = L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). Statement (Gradient structure due to rescale invariance Lemma 5.2 in (Mustafa & Burkholz, 2023)). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1L⟩ − ⟨θ2, ∇θ2L⟩ = 0. (14) We first consider the simpler case of GATES, i.e. W = U = V Theorem A.1 (Structure of GATES gradients). The gradients and parameters of GATES for layer l ∈ [L−1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩. (15) Following a similar strategy to (Mustafa & Burkholz, 2023), we identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Given the following definition of disjoint subsets θ1 and θ2 of the parameter set θ, associated with neuron i in layer l, 15GATE: How to Keep Out Intrusive Neighbors θ1 = {x|x ∈ Wl[i, :]} θ2 = {w|w ∈ Wl+1[:, i]} ∪ {al s[i]} ∪ {al t[i]} We show that the loss of GATES remains invariant for any λ >0. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. The scaled network parameters are denoted with a tilde as ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i], and ˜Wl[i, j] = λWl[i, j], and the corresponding networks components scaled as a result are denoted by ˜hl u[i], ˜hl+1 v [k], and ˜αl uv. We show that the parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] = hl+1 v [k]. Also recall Eq. (4) for W = U = V as: el uv = ((1 − quv)al s + (quv)al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) where quv = 1 if u = v and quv = 0 if u ̸= v. For simplicity, we rewrite this as: el uv,u̸=v = (al s)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (16) el uv,u=v = (al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (17) We show that ˜αl uv = exp(˜el uv)P u′∈N(v) exp(˜eluv) = αl uv , because (18) ˜el uv,u̸=v = el uv,u̸=v , and ˜el uv,u=v = el uv,u=v (19) which follows from the positive homogeneity of ϕ that allows ˜el uv,u=v = λ−1al s[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (20) = λ−1λal s[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (21) = el uv,u̸=v. (22) and similarly, 16GATE: How to Keep Out Intrusive Neighbors ˜el uv,u=v = λ−1al t[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (23) = λ−1λal t[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (24) = el uv,u=v. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] = ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i]. In the next layer, we therefore have ˜hl+1 v [k] = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k]. Thus, the output node representations of the network remain unchanged, and the loss L is rescale-invariant. Next consider the case that Wl, Ul, and V l are independent matrices. Similarly to the previous reasoning, we see that if we scale ˜Wl[i, :] = Wl[i, :]λ, then also scaling ˜Wl+1[:, i] = Wl+1[:, i]λ−1 and ˜al+1 s [i] = al+1 s [i]λ−1 and ˜al+1 t [i] = al+1 t [i]λ−1 will keep the GATE layer unaltered. In this case, we obtain an additional rescaling relationship between al s, al t and Ul, V l. A rescaling of the form ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i] could be compensated by ˜Ul[i, :] = Ul[i, :]λ and ˜V l[i, :] = V l[i, :]λ. It follows immediately that ˜euv = euv. A.3. Derivation of Insight 4.4 Following the analysis in A.1, in contrast to GAT, αij/αii << 1 can be easily realized in GATE with as[f] < 0 and at[f] > 0 for all or only a subset of the features. Note that for the non-weight-sharing case, U and V in GATE would simply correspond to Ws and Wt, respectively, in GATE and the same line of reasoning holds. Large norms are usually not required to create a notable difference in size between eii and eij. 17GATE: How to Keep Out Intrusive Neighbors B. Experimental Settings Our complete experimental setup is described as follows. Non-linearity For GATS and GAT networks, we substitute ϕ in Eq. (3) with LeakyReLU as defined in the standard architecture. For GATE, we substitute ϕ in Eq. (4) with ReLU in order to be able to interpret the sign of as and at parameters as contributing positively or negatively to neighborhood aggregation. MLP, MLP +GAT , and FAGCN also all use ReLU after every hidden layer. Network Width We vary the depth of GAT and GATE networks in all our experiments as specified. For synthetic datasets, the network width is fixed to 64 in all cases. For OGB datasets, we use the hidden dimensions used by (Brody et al., 2022). For the remaining datasets, the network width is also fixed to 64. Initialization The feature transformation parameter matrices, i.e.,W, U, and V are initialized randomly with an orthog- onal looks-linear structure (Burkholz & Dubatovka, 2019) for MLP, MLP+GAT , GAT(S) and GATE(S). The parameters a in GAT(S) use Xavier initialization (Glorot & Bengio, 2010), as is the standard. In GATE(S), as and at are initialized to 0 to initially give equal weights to the features of a node itself and its neighboring nodes. Optimization Synthetic, OGB, and remaining real-world tasks are run for a maximum of 10000, 2000, 5000 epochs, respectively, using the Adam optimizer. To isolate the effect of the architecture and study the parameter dynamics during training as best as possible, we do not use any additional elements such as weight decay and dropout regularization. We also do not perform any hyperparameter optimization. However, the learning rate is adjusted for different real-world datasets to enable stable training of models as specified in Table 6. Nevertheless, for a fair comparison, the same learning rate is used for a given problem across all architectures. For all synthetic data, a learning rate of 0.005 is used. Real-world datasets use their standard train/test/validation splits, i.e. those provided by Pytorch Geometric for Planetoid datasets Cora and Citeseer, by OGB framework for OGB datasets, and by (Platonov et al., 2023) for all remaining real-world datasets. Code Our experimental code and synthetic data generators are available at https://github.com/ RelationalML/GATE.git. Table 6: Details of real-world datasets used in experiments. dataset # nodes # edges # features # classes learning rate used for L layer networks ogb-arxiv 169, 343 2 , 315, 598 128 40 L = [12] : 0.001 ogb-products 2, 449, 029 123 , 718, 152 100 47 L = [8] : 0.001 ogb-mag 736, 389 10 , 792, 672 128 349 L = [5] : 0.005 roman-empire 22, 662 32 , 927 300 18 L = [5] : 0.001, L= [10] : 0.0005 amazon-ratings 24, 492 93 , 050 300 5 L = [5] : 0.001, L= [10] : 0.0005 questions 48, 921 153 , 540 301 2 L = [5] : 0.001, L= [10] : 0.0005 minesweeper 10, 000 39 , 402 7 2 L = [5] : 0.001, L= [10] : 0.0005 tolokers 11, 758 519 , 000 10 2 L = [5] : 0.001, L= [10] : 0.0005 cora 2, 708 10 , 556 1 , 433 7 L = [2, 5] : 0.005, L= [10, 20] : 0.0005 citeseer 3, 327 9 , 104 3 , 703 6 L = [2, 5] : 0.001, L= [10, 20] : 0.0001 actor 7, 600 26 , 659 932 5 L = [2, 5, 10, 20] : 0.005 texas 183 279 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.0005 wisconsin 251 450 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.005 18GATE: How to Keep Out Intrusive Neighbors C. Additional Results Smaller Real-World Datasets We evaluate GAT and GATE on five small-scale real-world datasets with varying ho- mophily levels β as defined in (Pei et al., 2020) and report results in Table 7. Higher values ofβ indicate higher homophily, i.e. similar nodes (with the same label) tend to be connected. We note that a 2-layer network of a baseline method for het- erophilic datasets, Geom-GCN (Pei et al., 2020), attains test accuracy (%) of64.1, 67.6, and 31.6 for Wisconsin, Texas, and Actor datasets, respectively, which is in line with that achieved by GATE. Except for Citeseer, the best overall performance for each dataset is achieved on a shallow model. This is not surprising as these datasets are small-scale and potentially prone to over-fitting in large models, particularly since we do not use any skip connections or regularization to retain model performance. Furthermore, the three heterophilic datasets have been recently shown to be problematic (Platonov et al., 2023). Therefore, a better evaluation of GATE is on relatively large-scale OGB datasets (Hu et al., 2021) and more recent heterophilic datasets (Platonov et al., 2023) that can exploit the flexibility of GATE. Although GATE is more pa- rameterized than GAT, it usually requires fewer training epochs and generalizes better, in addition to other advantages over GAT as discussed in the paper. Table 7: Test accuracy (%) of GAT and GATE models for network depthL on small-scale real-world datasets with varying homophily levels β. Entries marked with * indicate models that achieve 100% training accuracy and stable test accuracy. Otherwise, test accuracy at max. validation accuracy is reported. Data β L = 2 L = 5 L = 10 L = 20 GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE Wisc. 0.21 62 .7* 80.4 70.5* 51.0 70.5 60.7* 45.1 62.7 58.8 47 .1 62.7 60.7 Texas 0.11 56 .7* 67.6* 67.6* 51.4 67.6* 67.6* 56.7* 62.2* 62.3* 59.4* 62.1* 64.9 Actor 0.24 27 .1 32.2 31.6 25 .4 27 .5 29.2 25.3 27 .4 27.9 24.5 24 .6 29.4 Cora 0.83 80 .0 81.0* 80.8 79 .8 80.8* 80.4 77 .6 80.0* 79.2 77 .7 77 .2* 79.0 Cite. 0.71 68 .0 67 .6* 68.3 67.2 68.7* 67.8 66 .9 67.6* 67.6 68.2 67 .1* 69.2 Initialization of attention parameters in GAT We show in Fig. 7 that setting the initial value of attention parameters as and at in GATE to zero is, in fact, not what enables neighborhood aggregation but rather the separation ofa into as and at as discussed in Insight 4.4. Figure 7: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels for GAT with attention parameters a initialized to zero. Left to right:1, 2 and 5 layer models that achieve test accuracy of100%, 52.7%, and 36.2%, respectively, which is similar to the results obtained by standard Xavier initialization of attention parameters in GAT. Further analysis of experiments We present the analysis of α coefficients learned for some experiments in the main paper that were deferred to the appendix due to space limitations. 19GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 8: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 5 layer models. (a) original labels (b) Random labels Figure 9: Distribution of αvv against training epoch for the self-sufficient learning problem using Cora graph structure with original (top) and random (bottom) node labels and input node features as a one-hot encoding of labels. Left to right: 1, 2, and 5 layer GATES models that all 100% test accuracy except in the case of 5 layer model using original labels. In this case, although a training accuracy if 100% is achieved at 32 epochs with test accuracy 97.3%, a maximum test accuracy of 98.4% is reached at 7257 epochs. Training the model to run to 15000 epochs only increases it to 98.4%. An increased learning rate did not improve this case. However, we also run the GAT model for 15000 epochs for this case, and it achieves 85.9% test accuracy at epoch 47 where the model achieves 100% accuracy and only achieves a maximum test accuracy of 89.3% briefly at epoch 8. 20GATE: How to Keep Out Intrusive Neighbors Figure 10: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 1. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 1, 2, and 3 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 1 layer of the 2 and 3 layer models perform neighborhood aggregation. Table 8: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Entries marked with * identify models where 100% train accuracy is not achieved. Underlined entries identify the model with the highest train accuracy at the epoch of max. test accuracy. This provides an insight into how similar the function represented by the trained model is to the function used to generate node labels, i.e. whether the model is simply overfitting to the train data or really learning the task. Higher training and test accuracy simultaneously indicate better learning. In this regard, the difference in train accuracy at max. test accuracy between GATE and GATS or GAT is only 0.4, 1.0 and 0.6 for the settings (k = 1, L= 3), (k = 2, L= 4) and (k = 3, L= 3), respectively. k L Test Acc. @ Epoch of Max. Train Acc. Max Test Acc. @ Epoch GATS GAT GATE GAT S GAT GATE 1 1 92.0@2082* 91.2@6830* 93.2@3712* 93.2@1421 92.0@9564 93.6@3511 2 89.6@8524* 88.0@8935 91.2@942 91.6@5188 92.8@4198 95.6@111 3 86.4@9180* 88.8@997 92.8@618 91.2@6994 92.8@437 97.2@82 2 2 88.8@6736* 89.6@3907 88.8@467 93.2@151 93.2@95 92.0@105 3 82.0@7612 89.2@1950 91.6@370 91.6@1108 93.2@856 95.2@189 4 84.8@4898 82.4@739 87.2@639 88.0@1744 88.4@423 90.4@447 3 3 80.8@8670 80.4@737 85.2@391 86.4@1578 88.8@285 92.0@47 4 78.0@3012 80.4@767 89.6@480 86.8@1762 85.6@469 91.6@139 5 80.0@6611 74.4@1701 86.0@447 85.6@921 83.6@1098 91.2@243 21GATE: How to Keep Out Intrusive Neighbors (a) Tolokers, GAT:61.6% test AUROC.  (b) Tolokers, GATE:69.2% test AUROC. Figure 11: Distribution of αvv against training epoch for one run of 10 layer networks on real-world heterophilic task. Figure 12: Distribution of αvv against training epoch of 2-layer (left) and 5-layer (right) GAT networks for heterophilic datasets Texas (top) and Actor (bottom) 2-layer modes. Despite having connections to unrelated neighbors, GAT is unable to switch off neighborhood aggregation. Figure 13: Distribution of αvv, against training epoch of 2-layer (left) and 5-layer (right) GATE networks for heterophilic datasets Texas (top) and Actor (bottom), across layers could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods. For instance, in the case of Texas, GATE carries out little to no neighborhood aggregation in the first layer over input node features. Instead, aggregation is mainly done over node features transformed in earlier layers that effectuate non-linear feature learning as in perceptrons. However, in the case of Actor, GATE prefers most of the neighborhood aggregation to occur over the input node features, indicating that they are more informative for the task at hand. 22GATE: How to Keep Out Intrusive Neighbors Over-smoothing analysis In the main paper, we have already established the superior performance of GATE, compared to GAT, on several tasks. Intuitively, this can partially be attributed to reduced over-smoothing as its root cause, unnecessary neighborhood aggregation, is alleviated. Here, we verify this insight quantitatively. A widely accepted measure of over-smoothing is the Dirichlet energy (DE) (Cai & Wang, 2020). However, Eliasof et al. (2023) propose a modification of DE to measure GAT energy EGAT , that we use to evaluate over-smoothing in our experiments (see Table 9). We note that the notion of ‘over-smoothing’ is itself task-dependent. It is difficult to determine the optimal degree of smoothing for a task and the threshold that determines ‘over’-smoothing. This merits an in-depth analysis and curation of task-dependent smoothness measures that are not our focus. To show that GATE reduced over- smoothing relative to GAT, it suffices that a decrease in smoothing and an increase in accuracy occur simultaneously. Table 9: The measures Einput, EGAT and EGATE denote the smoothness of input node features, node features at the last layer L of the trained GAT and GATE models, respectively. Two cases are considered: All node pairs and only adjacent node pairs to measure smoothing at the global graph and local node level. Higher values indicate less smoothing. Node representations learned by GATE achieve higher test accuracy on all these tasks, as reported in the main paper, and are simultaneously less smooth than GAT in most cases, indicating that GATE potentially alleviates over-smoothing in GATs. (a) Synthetic self-sufficient task: varying graph structure and label distribution, node features as one-hot encoding of labels, L = 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE Cora, Original Labels 6.016 e + 06 1 .281 e + 08 2.037 e + 09 2.006 e + 03 7 .903 e + 03 1.971 e + 05 Cora, Random Labels 6.283 e + 06 3 .472 e + 09 3.747 e + 09 9.080 e + 03 9 .306 e + 05 3.966 e + 06 ER(p = .01), 2 Classes 4.994 e + 05 2 .701 e + 06 4.478 e + 07 5.042 e + 03 2 .272 e + 04 3.229 e + 05 ER(p = .01), 8 Classes 8.745 e + 05 3 .350 e + 07 2.615 e + 08 8.694 e + 03 1 .762 e + 05 1.960 e + 06 (b) Synthetic neighbor-dependent task: graph structure, node features, generator parameter k, and label distribution as in Section 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE k = 1, L= 3 1 .953 e + 06 5.306 e + 07 5.095 e + 07 1 .957 e + 05 4.234 e + 05 3.610 e + 05 k = 2, L= 4 1 .975 e + 06 1 .193 e + 07 2.198 e + 07 2.012 e + 04 1 .016 e + 05 1.939 e + 05 k = 3, L= 5 1 .951 e + 06 1 .645 e + 07 1.053 e + 08 1.966 e + 04 1 .408 e + 05 9.096 e + 05 (c) Real-world tasks. Dataset L All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE roman-empire 5 1 .274 e + 09 1 .002 e + 11 7.491 e + 11 7.878 e + 04 2 .441 e + 06 4.009 e + 07 amazon-ratings 10 3 .844 e + 08 1 .187 e + 10 2.272 e + 10 4.933 e + 04 3 .848 e + 05 7.430 e + 05 minesweeper 5 6 .869 e + 07 1 .386 e + 09 2.531 e + 10 2.628 e + 04 1 .946 e + 05 7.017 e + 06 tolokers 10 1 .391 e + 08 1 .044 e + 11 1.042 e + 11 3.423 e + 05 1 .249 e + 08 1.397 e + 08 cora 10 5 .088 e + 05 1 .437 e + 07 2.783 e + 08 6.490 e + 02 5 .959 e + 02 1.226 e + 04 citeseer 10 3 .463 e + 05 2 .916 e + 05 1.126 e + 07 2.360 e + 02 2 .426 e + 00 1.030 e + 02 texas 10 1 .945 e + 06 3 .280 e + 04 3.877 e + 04 1.758 e + 04 8 .487 e + 01 9.695 e + 01 actor 10 2 .612 e + 08 1.800 e + 07 1.364 e + 07 1 .237 e + 05 2.810 e + 03 2.215 e + 03 wisconsin 10 4 .438 e + 06 1 .057 e + 07 1.008 e + 08 3.299 e + 04 3 .765 e + 04 7.363 e + 08 23GATE: How to Keep Out Intrusive Neighbors Comparison with other GNNs Other GNN architectures could potentially switch off neighborhood aggregation, as we show here. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatena- tion operation). We evaluate the performance of three such architectures that, in principle, employ different aggregation methods, which are likely to be capable of switching off neighborhood aggregation, on synthetic datasets empirically and discuss their ability or inability to switch off neighborhood aggregation qualitatively as follows. 1. ωGAT (Eliasof et al., 2023) introduces an additional feature-wise layer parameter ω that can, in principle, switch off neighborhood aggregation by setting ω parameters to 0, in addition to the attention mechanism based on GAT. How- ever, in practice, as we verify on our synthetic dataset in Figure 14, it is unable to effectively switch off neighborhood aggregation. Although it outperforms GAT, it is still substantially worse than GATE, especially for the deeper model due to unnecessary neighborhood aggregations. Another architecture based on graph attention, superGAT(Kim & Oh, 2021), falls under the paradigm of structural learning as it uses a self-supervised attention mechanism essentially for link prediction between nodes, and therefore its comparison with GATE is infeasible. 2. GraphSAGE (Hamilton et al., 2018) uses the concatenation operation to combine the node’s own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neigh- borhood aggregation for the synthetic datasets designed for the self-sufficient learning task (see Table 10). Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing (see Table 11). 3. FAGCN (Bo et al., 2021) requires a slightly more detailed analysis. Authors of FAGCN state in the paper that: ‘When αG ij ≈ 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.’ where αG ij defined in the paper can be considered analogous to αij in GAT, though they are defined differently. Thus, from an expressivity point of view, FAGCN should be able to assign parameters such that all αG ij = 0. However, we empirically observe on synthetic datasets designed for the self-sufficient learning task, values of αG ij do not, in fact, approach zero. Despite being unable to switch off neighborhood aggregation, FAGCN, in its default implementation, achieves 100% test accuracy on the task. We discover this is so because FAGCN introduces direct skip connections of non-linearly transformed raw node features to every hidden layer. Given the simplicity of the one-hot encoded features in the datasets and the complete dependence of the label on these features, FAGCN is able to represent the desired function. In order to better judge its ability to switch off neighborhood aggregation by setting αG ij = 0, we remove this skip connection. From an expressivity point of view, FAGCN should still be able to achieve 100% test accuracy by using only the (non-)linear transformations of raw features initially and performing no neighborhood aggregation in the hidden layers. However, we find that FAGCN was unable to emulate this behavior in practice. For a fair comparison of the differently designed attention mechanism in FAGCN with GATE, we introduce self-loops in the data so FAGCN may also receive a node’s own features in every hidden layer. Even then, FAGCN fails to achieve perfect test accuracy as shown in Table 10. Therefore, we suspect the attention mechanism in FAGCN may also be susceptible to the trainability issues we have identified for the attention mechanism in GAT. Nevertheless, the capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE and both could be combined. It would be interesting to derive conservation laws inherent to other architectures such as FAGCN and GraphSAGE and study how they govern the behaviour of parameters. Furthermore, by design, FAGCN does not perform any non-linear transformations of aggregated neighborhood features which may be necessary in some tasks, such as our synthetic dataset for the neighbor-dependent learning task. As Table 11 shows, GATE outperforms FAGCN on such a task. Lastly, we would like to emphasize that our aim is to provide insights into the attention mechanism of GAT and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. 24GATE: How to Keep Out Intrusive Neighbors Figure 14: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels, for theωGAT architecture for the1, 2 and 5 layer models that achieve test accuracy of 100%, 98.5%, and 49.3%, respectively. Table 10: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of network layers, respectively. Original (Orig.) and Randomized (Rand.) labels are used for the Cora structure. The FAGCN model is implemented without skip connections from the input layer to every other layer and without any self-loops in input data, whereas FAGCN* denotes the model also without skip connections but with self-loops introduced for all nodes in input data. Structure C L Max. Test Accuracy (%) GAT GATE FAGCN FAGCN* SAGE Cora O,7 1 100 100 90.1 97 .6 100 2 94 .6 100 94.2 94 .9 98 .8 5 88 .5 99.7 87.1 89 .1 92 .4 R,7 1 100 100 61.6 97 .8 100 2 57 .0 100 69.2 70 .5 100 5 36 .7 100 21.2 36 .7 99 .6 ER (p = 0.01) R,2 1 100 100 100 100 100 2 100 100 100 100 100 5 99 .6 100 96.4 99 .2 100 R,8 1 99 .2 100 86.4 98 .8 100 2 97 .6 100 86.0 91 .6 100 5 38 .4 100 31.6 40 .4 100 25GATE: How to Keep Out Intrusive Neighbors Table 11: Neighbor-dependent learning: k and L denote the number of hops aggregated in the neighborhood to generate labels, and the number of layers of the evaluated network, respectively. k L Max Test Accuracy (%) @ Epoch GAT GATE SAGE FAGCN 1 1 92@9564 93.6 @ 3511 93.2@2370 93 .2@1618 2 92 .8@4198 95.6 @ 111 95.6@723 94 .1@1455 3 92 .8@437 97.2 @ 82 96.8@100 81 .2@573 2 2 93.2 @ 95 92.0@105 90 .8@199 90 .4@170 3 93 .2@856 95.2 @ 189 94.4@113 88 .8@283 4 88 .4@423 90 .4@447 92.4 @ 139 87.6@549 3 3 88 .8@285 92.0 @ 47 87.6@45 89 .2@528 4 85 .6@469 91.6 @ 139 88@60 89 .2@3191 5 83 .6@1098 91.2 @ 243 86.0@35 88 .8@205 26",
      "meta_data": {
        "arxiv_id": "2406.00418v2",
        "authors": [
          "Nimrah Mustafa",
          "Rebekka Burkholz"
        ],
        "published_date": "2024-06-01T12:31:15Z",
        "pdf_url": "https://arxiv.org/pdf/2406.00418v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies and experimentally demonstrates a structural limitation in Graph Attention Networks (GATs): their inability to switch off task-irrelevant neighborhood aggregation. This leads to over-smoothing and limits the benefits of deeper models for non-linear feature transformations. To address this, the authors propose GATE, an extension of GAT that can selectively switch neighborhood aggregation on or off. GATE alleviates over-smoothing, allows the model to benefit from higher depth for feature transformations, often outperforms GATs on real-world heterophilic datasets by down-weighting unrelated neighbors, and provides interpretable self-attention coefficients. The work also constructs a synthetic test bed for analyzing adaptive neighborhood aggregation and achieves a new state-of-the-art test accuracy on the OGB-arxiv dataset.",
        "methodology": "GATE modifies the GAT architecture, specifically GATv2, by altering its attention mechanism. Instead of a single attention parameter vector 'a', GATE introduces separate attention parameters: 'as' for neighborhood contributions and 'at' for the node's own features. This modification enables flexible weighting of the importance of node and neighborhood features. The design is based on theoretical insights derived from an updated conservation law of gradient flow dynamics, which shows that GATE can switch off neighborhood aggregation in a well-trainable parameter regime, unlike GAT. ReLU is used as the non-linearity to allow interpretation of the signs of 'as' and 'at'. Feature transformation matrices (W, U, V) are initialized with an orthogonal looks-linear structure, and 'as' and 'at' parameters are initialized to zero.",
        "experimental_setup": "The validation of GATE's capabilities was performed on both synthetic and real-world graphs. The synthetic test bed included two node classification problems: 'self-sufficient learning' (label-relevant information only in node's own features, using Erdős–Rényi graphs or Cora structure with original/randomized labels) and 'neighbor-dependent learning' (label-relevant information in k-hop neighbors' features, using ER graphs with K-means clustered labels). Real-world datasets included five heterophilic benchmarks (roman-empire, amazon-ratings, questions, minesweeper, tolokers), three OGB datasets (OGB-arxiv, OGB-products, OGB-mag), and five smaller-scale datasets (Cora, Citeseer, Actor, Texas, Wisconsin) with varying homophily levels. Performance was evaluated using test accuracy and AUC-ROC. Comparisons were made against GAT, MLP, MLP+GAT, FAGCN, and a wide range of other GNN baselines. Training involved the Adam optimizer for up to 10000 epochs (synthetic), 2000 (OGB), or 5000 (other real-world), without weight decay or dropout regularization, and with adjusted learning rates. The distribution of αvv values and Dirichlet energy (EGAT) were analyzed to measure neighborhood aggregation and over-smoothing.",
        "limitations": "The paper primarily highlights limitations of GAT that GATE aims to solve. For GATE itself, one limitation observed is that on the synthetic neighbor-dependent task, it could not achieve perfect 100% test accuracy, possibly due to the non-crisply defined decision boundary of the synthetic data. For smaller real-world datasets, deeper GATE models, without explicit skip connections or regularization (which were intentionally excluded to isolate architectural effects), showed a tendency towards overfitting, suggesting that additional techniques are generally beneficial. The evaluation of 'over-smoothing' is acknowledged as challenging due to its task-dependent nature and the difficulty in defining an optimal degree or threshold. The comparison with FAGCN suggested that while FAGCN could achieve high accuracy through direct skip connections of raw node features, its core attention mechanism might still be susceptible to trainability issues similar to GAT.",
        "future_research_directions": "Future research directions include leveraging GATE to answer fundamental questions about the importance of graph structure for various tasks, deriving conservation laws for other GNN architectures like FAGCN and GraphSAGE to understand their parameter behaviors, combining GATE with complementary techniques such as graph rewiring to address problems like over-squashing, and exploring combinations with architectures like FAGCN to potentially enhance the learning of negative associations. The constructed synthetic test bed is also proposed as a tool for measuring progress in developing adaptive neighborhood aggregation schemes. Furthermore, a more in-depth analysis and curation of task-dependent smoothness measures for GNNs is suggested."
      }
    },
    {
      "title": "GOAT: A Global Transformer on Large-scale Graphs"
    },
    {
      "title": "Fast Attention Requires Bounded Entries",
      "abstract": "In modern machine learning, inner product attention computation is a\nfundamental task for training large language models such as Transformer, GPT-1,\nBERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as\ninput three matrices $Q, K, V \\in [-B,B]^{n \\times d}$, and the goal is to\nconstruct the matrix $\\mathrm{Att}(Q,K,V) := \\mathrm{diag}(A {\\bf 1}_n)^{-1} A\nV \\in \\mathbb{R}^{n \\times d}$, where $A = \\exp(QK^\\top/d)$ is the `attention\nmatrix', and $\\exp$ is applied entry-wise. Straightforward methods for this\nproblem explicitly compute the $n \\times n$ attention matrix $A$, and hence\nrequire time $\\Omega(n^2)$ even when $d = n^{o(1)}$ is small.\n  In this paper, we investigate whether faster algorithms are possible by\nimplicitly making use of the matrix $A$. We present two results, showing that\nthere is a sharp transition at $B = \\Theta(\\sqrt{\\log n})$.\n  $\\bullet$ If $d = O(\\log n)$ and $B = o(\\sqrt{\\log n})$, there is an\n$n^{1+o(1)}$ time algorithm to approximate $\\mathrm{Att}(Q,K,V)$ up to\n$1/\\mathrm{poly}(n)$ additive error.\n  $\\bullet$ If $d = O(\\log n)$ and $B = \\Theta (\\sqrt{\\log n})$, assuming the\nStrong Exponential Time Hypothesis from fine-grained complexity theory, it is\nimpossible to approximate $\\mathrm{Att}(Q,K,V)$ up to $1/\\mathrm{poly}(n)$\nadditive error in truly subquadratic time $n^{2 - \\Omega(1)}$.\n  This gives a theoretical explanation for the phenomenon observed in practice\nthat attention computation is much more efficient when the input matrices have\nsmaller entries.",
      "full_text": "arXiv:2302.13214v2  [cs.LG]  9 May 2023 Fast Attention Requires Bounded Entries Josh Alman∗ Zhao Song† Abstract In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BE RT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices Q, K, V ∈[−B, B ]n×d, and the goal is to construct the matrix Att( Q, K, V ) := diag( A1n)−1AV ∈Rn×d, where A = exp(QK⊤/d) is the ‘attention matrix’, and exp is applied entry-wise. Straightfor ward methods for this problem explicitly compute the n ×n attention matrix A, and hence require time Ω( n2) even when d = no(1) is small. In this paper, we investigate whether faster algorithms are possib le by implicitly making use of the matrix A. We present two results, showing that there is a sharp transition a t B = Θ(√log n). • If d = O(log n) and B = o(√log n), there is an n1+o(1) time algorithm to approximate Att(Q, K, V ) up to 1 /poly(n) additive error. • If d = O(log n) and B = Θ(√log n), assuming the Strong Exponential Time Hypothesis from ﬁne-grained complexity theory, it is impossible to approximate A tt(Q, K, V ) up to 1/poly(n) additive error in truly subquadratic time n2−Ω(1) . This gives a theoretical explanation for the phenomenon observed in practice that attention computation is much more eﬃcient when the input matrices have smalle r entries. ∗ josh@cs.columbia.edu. Columbia University. † zsong@adobe.com. Adobe Research.1 Introduction Large language models (LLMs) such as Transformer [ VSP+17], BERT [DCLT18], GPT-3 [BMR+20], PaLM [CND+22], and OPT [ ZRG+22] can process natural language more eﬀectively than smaller models or traditional algorithms. This means that they can understand and generate more complex and nuanced language, which can be useful for a variety of tasks such as language translation, ques- tion answering, and sentiment analysis. LLMs can also be ada pted to multiple purposes without needing to be retained from scratch. Their power is particul arly exempliﬁed by the recent success of ChatGPT, a chat software by OpenAI built on top of GPT-3 [ cha22]. The key technical backbone of LLMs is the attention matrix [VSP+17, RNS+18, DCLT18, RWC+19, BMR+20]. An attention matrix is a square matrix whose rows and colum ns correspond to words or “tokens”, and whose entries correspond to the cor relations between these tokens in natural text. The attention matrix is then used to calculate the importance of each input token in a sequence when producing an output. In an attention mecha nism, each input token is given a weight or score, which reﬂects its importance or relevance t o the current output being generated. These scores are calculated based on a comparison between th e current output state and the input states, using a similarity function. More formally, the attention matrix is deﬁned as follows. Le t Q ∈Rn×d be the matrix of query tokens, and K ∈Rn×d be the matrix of key tokens. (We focus here on the case when d = no(1), so d ≪n.) The attention matrix is an n ×n matrix A where the rows and columns correspond to the input tokens in the sequence. Each entry in the matrix rep resents the attention weight or score between a particular input token (query token Q) and a particular output token (key token K). The diagonal entries of the matrix represent self-attentio n scores, which measure the importance of each token with respect to itself. The major bottleneck to speeding up LLM operations (in the ca se of modeling long sequences with large n) is the time to perform attention matrix computations [ VSP+17, RNS+18, DCLT18, RWC+19, BMR+20, WLK+20, KKL20]. These computations ask us to multiply the attention matrix A with another value token matrix V ∈Rn×d. We formally deﬁne Attention computation as follows. Throug hout this paper, we write exp to denote the entry-wise exponential for matrices. Deﬁnition 1.1 (Exact Attention ComputationEAttC(n, d)). Given three matrices Q, K, V ∈Rn×d, output the n ×d matrix Att(Q, K, V ) deﬁned by Att(Q, K, V ) := D−1AV where A ∈Rn×n and diagonal matrix D ∈Rn×n are deﬁned as A := exp(QK⊤/d), and D := diag(A1n). The straightforward algorithm for this problem computes th e matrix A and then performs the multiplications D−1AV , in time n2+o(1). Since A is an n ×n matrix with n2 entries, it is impossible to improve on this much while explicitly computing the matri x A. However, the input to the problem is not A, but rather the three matrices Q, K, V which each have only n1+o(1) entries. An algorithm which only implicitly makes use of A, without explicitly computing all its entries, could hope to run in almost linear time! In this paper, we investigate the possibility of accelerati ng attention computations in this way. The two main questions we address are: • Q1. When can we perform attention computations in almost linear time n1+o(1)? 1• Q2. When can we prove that subquadratic-time algorithms for att ention computations are impossible? In most LLMs, it suﬃces to approximately perform attention computations throughout the in- ference process as long as there are reasonable precision gu arantees [CGRS19, KKL20, WLK+20, DKOD20, KVPF20, CDW+21, CDL+22, LWD+23]. We therefore focus here on approximate at- tention computation, which can potentially be performed ev en faster than exact computation. Mathematically, we deﬁne the approximate version of EAttC as follows. Deﬁnition 1.2 (Approximate Attention Computation AAttC(n, d, B, ǫ a)). Let ǫa > 0 and B > 0 be parameters. Given three matrices Q, K, V ∈Rn×d, with the guarantees that ∥Q∥∞ ≤B, ∥K∥∞ ≤B, and ∥V ∥∞ ≤B, output a matrix T ∈Rn×d which is approximately equal to D−1AV , meaning, ∥T −D−1AV ∥∞ ≤ǫa. Here, for a matrix M ∈Rn×n, we write ∥M∥∞ := maxi,j |Mi,j |. Again, the straightforward algorithm for this problem runs in time O(n2d) ≤n2+o(1), but the input size is only O(nd) ≤n1+o(1). Our goal is to investigate when faster algorithms are possi ble in terms of the parameters d, B, and ǫa. 1.1 Our Results We focus on the natural setting whered = O(log n) (the setting where we model long sequences) and ǫa = 1/ poly(n) (low enough error so that attention computations over an en tire network can be combined). Our main results show that whether or not there is a fast algorithm for AAttC critically depends on B, the magnitudes of the entries in the input matrices. We ﬁrst show a lower bound, that when B ≥Ω(√ log n), it is impossible to design a truly subquadratic-time algorithm. Our lower bound makes use of t he Strong Exponential Time Hy- pothesis ( SETH) [ IP01], a popular conjecture [ Wil18] from the area of ﬁne-grained complexity regarding the time required to solve k-SAT. (See Section 4 below where we discuss SETH in more detail.) Theorem 1.3(Lower bound, informal version of Theorem 4.6). Assuming SETH, for every q > 0, there are constants C, Ca, Cb > 0 such that: there is no O(n2−q) time algorithm for the problem AAttC(n, d = C log n, B = Cb √log n, ǫa = n−Ca ). Our second complementary result is a new algorithm, showing that when B < o (√log n), the problem can be solved very eﬃciently, in almost linear time. Theorem 1.4(Upper bound, informal version of Theorem 3.8). There is an algorithm (Algo- rithm 1) that solves AAttC(n, d = O(log n), B = o(√log n), ǫa = 1/ poly(n)) in time n1+o(1). Our Theorems 1.3 and 1.4 show that the attention computation problem AAttC exhibits a very tight transition at B = Θ(√log n) from almost linear time to trivial quadratic time. When B < o (√log n) is smaller, the problem can be solved in almost linear time n1+o(1) in the input size, using our algorithm for Theorem 1.4. When B ≥Ω(√log n) is greater, our algorithm from Theorem 1.4 no longer applies, and furthermore our lower bound from Theorem 1.3 shows that it is impossible to solve the problem in truly subquadratic time, no matter wh at algorithmic techniques one uses (assuming SETH). It has been observed in LLM implementations in practice that computations are much faster when one assumes that the matrix entries are bounded or can be well-approximated using a small 2number of bits (see, e.g., [ ZBIW19, Section 2] and [ KVPF20, Section 3.2.1]). Our work can be viewed as giving a theoretical explanation for this phenome non, and helping to explain why tech- niques like quantization [ZBIW19] and low-degree polynomial approximation [ KVPF20] have been so eﬀective in practice. Related Work. A recent work by Zandieh, Han, Daliri, and Karbasi [ ZHDK23] was the ﬁrst to give an algo- rithm with provable guarantees for attention approximation. Their algorithm makes use of locality sensitive hashing (LSH) techniques [CKNS20] which, as we will discuss next, is quite diﬀerent from our algorithm for Theorem 1.4 which uses the polynomial method [ ACSS20, AA22]. In the case when d = o(log2 n), they achieve a running time of roughly O(n1.17 ·d/ǫ2 r), where ǫr is a relative error parameter (which is similar, though not exactly the sa me, as our ǫa from Deﬁnition 1.2). In particular, their algorithm applies for larger d than ours (we require d = O(log n)), but we achieve almost linear time n1+o(1) (whereas their running time is bounded below by Ω(n1.17)), and our algorithm can handle any polynomial error ǫa = 1/ poly(n) (whereas they require ǫr ≥1/no(1) to not increase the running time by a polynomial factor). It is natural to wonder whether further improvements are possible by combining our techniques with those of [ ZHDK23]. However, our lower bound of Theorem 1.3 shows that our algorithm of Theorem 1.4 is already essentially tight and cannot be substantially im proved. Another recent work by Keles, Wijewardena, and Hedge [KWH23] was the ﬁrst to prove a lower bound for attention computation assuming SETH. They prove, among other results, that AAttC cannot be solved in truly subquadratic time in the case when d = ω(log n). Our Theorem 1.3 improves their result to also hold for d = Θ(log n), and to show how the complexity changes with the magnitude of entries B (which is not studied by [ KWH23]). As we discuss more shortly, both our lower bound proof and [ KWH23] use the high-level technique of [ BIS17], although our more ﬁne-grained analysis of the parameters d, B requires a more intricate analysis and the use of other techniques from ﬁne-grained complexity related to approxi mate nearest neighbor search [ Rub18] and the polynomial method [ AA22]. 1.2 Technique Overview Our high-level approach is to make use of similarities between attention computation and other computational problems related to Kernel Density Estimati on (KDE). Such a relationship was investigated by recent work [ TBY+19, ZHDK23]. In particular, [ ZHDK23] was inspired to ap- ply LSH techniques to attention computation because of the p revalence of LSH in KDE algo- rithms [ CS17, BCIS18, CS19, CKNS20]. The main conceptual idea behind our results is that diﬀerent techniques from the KDE literature, other than LSH, can be modiﬁed to apply in this setting and yield tight algoriths and lower bounds. To design our algorithm for Theorem 1.3, we instead build oﬀ of a diﬀerent line of work on KDE which makes use of the ‘polynomial method in algorithm desig n’. Suppose M ∈Rn×n is a matrix, f : R →R is a function, and let f(M) denote the matrix one gets by applying f entry-wise to M. The polynomial method is a technique for ﬁnding low-rank ap proximations of f(M). It shows that if M has low rank, and if f can be approximated by a low-degree polynomial, then the mat rix f(M) is very close to a low-rank matrix whose low-rank decomposition can be computed eﬃciently. To use this to solve AAttC, we make use of a recent result which bounds the degree requir ed to approximate the exponential function by a polynomial [ AA22] in order to ﬁnd a low-rank approxi- mation of the attention matrix A. Prior work [ACSS20, ACM+20, AA22] applied these polynomials in a similar way to solve the Gaussian KDE problem; our main observation is that by an appropriate rescaling, this approach can be modiﬁed to apply to AAttC as well. 3The proof of our lower bound Theorem 1.3 builds oﬀ of another line of work on the ﬁne-grained complexity of KDE problems [ BIS17, ACSS20, AA22]. The main idea is to give a ﬁne-grained reduction from the well-studied problem of Approximate Nea rest Neighbor search ANN. In ANN, one is given as input n vectors of dimension d, and an error parameter ǫ > 0, and the goal is to ﬁnd a pair of vectors whose distance is at most (1 + ǫ) times the minimum distance between any pair of the vectors. The straightforward algorithm for ANN runs in quadratic time, and it is known that it is impossible to solve ANN in truly subquadratic time assuming SETH [Rub18]. In order to prove our lower bound, we show that AAttC can be used to solve ANN. The key idea is that, if the matrices Q and K from AAttC are formed by concatenating the input vectors to the ANN problem, then the nearest neighbor vectors correspond to th e largest entries of the attention matrix A. It is not immediately clear that AAttC can be used to detect large entries of A, since the output is rescaled by the matrix D−1, but we show that this can be overcome with some modiﬁcations to the input vectors which approxima tely balance the rows of A. Prior work [BIS17, ACSS20, AA22] used a very similar approach to give lower bounds for KDE problems, although KDE doesn’t involve any rescaling factors. Roadmap. In Section 2, we introduce relevant notation and tools from prior work. I n Section 3, we present and analyze our attention algorithm. In Section 4, we prove our ﬁne-grained attention lower bound. In Section 5, we provide a conclusion for this paper. 2 Preliminaries We work in the standard real-RAM model and assume arithmeticoperations on real numbers can be performed in constant time in our algorithms. We use Tmat(a, b, c) to denote the time to multiply an a ×b matrix with another b ×c matrix. In fact, we will only make use of the straightforward, practi cal bound Tmat(a, b, c) ≤O(abc). In principle, fast theoretical matrix multiplication algori thms could be used instead to improve this bound and speed up our algorithms here (in exchange for makin g them less practical). That said, because of our parameter settings 1, we will see that faster matrix multiplication could only improve low-order terms in our running times. For any positive integer, we use [ n] to denote set {1, 2, ··· , n}. For a matrix M, we write ∥M∥∞ to denote its ℓ∞ norm, i.e., ∥M∥∞ := maxi,j |Mi,j |. For a matrix M, we use M⊤ to denote its transpose. We use 1n to denote a length-n vector whose entries are all 1s. We use 0n to denote a length-n vector whose entries are all 0s. For any matrix A ∈Rn×n, we use exp( A) ∈Rn×n to denote the matrix where exp( A)i,j = exp(Ai,j ). In other words, all the exp() operators in this paper are ap plied entry-wise to matrices. In particular, we will not use matrix exponentials in this pa per. For a vector x ∈Rn, we use ∥x∥0 to denote its number of non-zero entries, we use ∥x∥1 to denote its ℓ1 norm, i.e., ∥x∥1 := ∑ n i=1 |xi|, and we use ∥x∥2 to denote its ℓ2 norm, i.e., ∥x∥2 := (∑ n i=1 |xi|2)1/2. For a vector x, we use x⊤ to denote its transpose. 1We will make use ofTmat(n, no(1), no(1)), which can be solved straightforwardly in timen1+o(1), and which cannot be solved much faster since it has input sizen1+o(1). 42.1 Additive Error for Polynomial Approximation Our algorithm for attention computation will critically make use of a polynomial approximation for the exponential function. In particular, we use the foll owing tight construction from previous work [ AA22]. Lemma 2.1 ([AA22]). Let B > 1 and let ǫ ∈(0, 0.1). There is a polynomial P : R →R of degree g := Θ ( max { log(1/ǫ) log(log(1/ǫ)/B) , B }) such that for all x ∈[0, B], we have |P (x) −exp(x)|< ǫ. Moreover, P can be computed eﬃciently: its coeﬃcients are rational numb ers with poly(g)-bit integer numerators and denominators which can be computed i n poly(g) time. 2.2 From Additive Error to Relative Error We note that in our setting, Lemma 2.1 can be used to give a relative error approximation as well: Corollary 2.2. Let B > 1 and let ǫ ∈(0, 0.1). There is a polynomial P : R →R of degree g := Θ(max{ log(1/ǫ) log(log(1/ǫ)/B) , B}) such that for all x ∈[−B, B ], we have |P (x) −exp(x)|< ǫ ·exp(x). Proof. By Lemma 2.1, there is a polynomial Q : R →R of degree g = Θ({ log(1/ǫ) log(log(1/ǫ)/B) , B}) such that, for all y ∈[0, 2B] we have |Q(y) −exp(y)| ≤ǫ. Our desired polynomial is the rescaled P (x) := Q(x + B)/ exp(B). Indeed, for any x ∈[−B, B ], we have exp(x) ≥exp(−B), and so |P (x) −exp(x)|= |Q(x + B)/ exp(B) −exp(x)| = |Q(x + B) −exp(x + B)|/ exp(B) ≤ǫ/ exp(B) ≤ǫ ·exp(x), as desired. 3 Attention Algorithm In this section, we show how to use polynomial approximations for the exponential function in order to approximately perform attention computations. In Secti on 3.1, we deﬁne the type of low-rank matrix approximation which we will use. In Section 3.2, we show how polynomial approximations can give rise to such low-rank matrix approximations. In Sec tion 3.3, we bound the entries of the matrix QK⊤ ∈Rn×n (before converting it to the attention matrix) to conﬁrm tha t our polynomial approximation applies. In Section 3.4, we state our main technique for approximating the attention matrix. In Section 3.5, we show how to control the error propagation fromA to the rescaling matrix D. In Section 3.6, we further explain how to control the error propagation fro m D and A to the resulting attention matrix. Finally, in Section 3.7, we conclude our general algorithm, and in Section 3.8, we appropriately select the parameters to achieve almost l inear time. 53.1 Matrix Low-Rank Approximation Deﬁnition 3.1.Let r ≥1 denote a positive integer. Let ǫ ∈(0, 0.1) denote an accuracy parameter. Given a matrix A ∈Rn×n ≥0 , we say ˜A ∈Rn×n ≥0 is an (ǫ, r)-approximation of A if • ˜A = U1 ·U⊤ 2 for some matrices U1, U2 ∈Rn×r (i.e., ˜A has rank at most r), and • |˜Ai,j −Ai,j|≤ ǫ ·Ai,j for all (i, j) ∈[n]2. 3.2 From Low Degree Polynomials to Low Rank Matrices Lemma 3.2.Let M = XY ⊤ ∈Rn×n denote a matrix with X, Y ∈Rn×d. Let P (x) denote a degree-g polynomial, and deﬁne r = (2(g+d) 2g ) . There is an algorithm that runs in O(nrg) time and, given as input the matrix X, Y , constructs matrices U1, U2 ∈Rn×r such that P (M) = U1U⊤ 2 . (Here, P (M) denotes the entry-wise application of P to M.) Proof. Let P (x) denote the degree- g polynomial. Expand it in terms of its coeﬃcients as P (x) = d∑ i=0 ci ·xi. Consider the function K : Rd ×Rd →R deﬁned by, for u, v ∈Rd, K(u, v) := P (⟨u, v⟩). K is a degree-2g polynomial in the 2d entries u1, ···ud, v1, ··· , vd of the vectors u, v. Deﬁne the set V of its variables, V := {u1, ··· , ud, v1, ··· , vd}. Let Fdenote the set of functions F:= { f : V →{0, 1, 2, ··· , 2g}| ∑ v∈V f(v) ≤2g } . We can count that |F|= (2d + 2g 2g ) . Hence, there are coeﬃcients ct ∈R for each t ∈F such that K(u, v) = ∑ t∈F ct · ∏ v∈V vt(v). Deﬁne Vu := {u1, ··· , ud} and Vv = V \\Vu. 6We deﬁne φu : Rd →R|F| by, for any t ∈F, φu(u1, ··· , ud)t = ct · ∏ vi∈Vu ut(ui) i . Similarly, we deﬁne φv : Rd →R|F| by, for any t ∈F, φv(v1, ··· , vd)t = ∏ vi∈Vv vt(ui) i . Thus, we have K(u, v) = ⟨φu(u), φv(v)⟩. For i ∈[n], let Xi ∈Rd denote the i-th row of X, and let Yi ∈Rd denote the i-th row of Y . Our algorithm can thus construct • the matrix U1 ∈Rn×|F| whose i-th row is the vector φu(xi) for i ∈[n], and • the matrix U2 ∈Rn×|F| whose i-th row is the vectors φv(yi) for i ∈[n]. Each entry of these matrices can be constructed by multiplyi ng together at most g variables, so these n ×r matrices can be constructed in time O(nrg) as desired. 3.3 Matrix QK⊤ Has Bounded Entries Lemma 3.3 (Bounded entry). Suppose B ≥1 and matrices Q, K ∈Rn×d have ∥Q∥∞ ≤B and ∥K∥∞ ≤B. Then, we have ∥QK⊤/d∥∞ ≤B2. Proof. For each (i, j) ∈[n] ×[n], we have |(QK⊤)i,j |= | d∑ l=1 Qi,lKj,l| ≤d ·∥Q∥∞ ·∥K∥∞ ≤d ·B2, as desired. 3.4 Key Lemma Our key lemma shows that, even though the attention matrixA may have full rank, it has a low-rank approximation that is easy to compute: Lemma 3.4.Suppose Q, K ∈Rn×d, with ∥Q∥∞ ≤B, and ∥K∥∞ ≤B. Let A := exp(QK⊤/d) ∈ Rn×n. For accuracy parameter ǫ ∈(0, 1), there is a positive integer g bounded above by g = O ( max { log(1/ǫ) log(log(1/ǫ)/B), B2 }) , 7and a positive integer r bounded above by r ≤ (2(g + d) 2g ) such that: There is a matrix ˜A ∈Rn×n that is an (ǫ, r)-approximation (Deﬁnition 3.1) of A ∈Rn×n. Furthermore, the matrices U1 and U2 deﬁning ˜A can be computed in O(n ·r) time. Proof. Let M := QK⊤/d. From Lemma 3.3, we know that ∥M∥∞ ≤B2. Thus, applying Corol- lary 2.2 (with bound B2 on its entries), there is a degree- g polynomial P such that the matrix ˜A = P (M) is an ( ǫ, r)-approximation to A (See the deﬁnition of ( ǫ, r)-approximation in Deﬁni- tion 3.1.) We can then compute U1, U2 using Lemma 3.2, which gives the bound r ≤ (2(g + d) 2g ) . This completes the proof. 3.5 From A to D Lemma 3.5. Let A ∈Rn×n be any matrix whose entires are all positive and ǫA ∈(0, 0.1) be any parameter. Let ˜A ∈Rn×n be any matrix such that, for all (i, j) ∈[n] ×[n], we have |˜Ai,j −Ai,j |≤ ǫA ·Ai,j. Deﬁne the matrices D, ˜D ∈Rn×n by D = diag(A1n) and ˜D = diag( ˜A1n). Then, for all i ∈[n] we have |˜Di,i −Di,i|≤ ǫA ·Di,i. Proof. We calculate that |˜Di,i −Di,i|= | n∑ j=1 ˜Ai,j − ∑ j=1 Ai,j| ≤ n∑ j=1 |˜Ai,j −Ai,j| ≤ n∑ j=1 ǫAAi,j = ǫA ·Di. This completes the proof. 3.6 From A and D to Attention Matrix Lemma 3.6. Let ǫA, ǫD ∈(0, 0.1) and B > 1 be parameters, and let V ∈Rn×d denote a matrix with ∥V ∥∞ ≤B. Let A ∈Rn×n be any matrix whose entires are all positive, and let ˜A ∈Rn×n be a matrix such that, for all (i, j) ∈[n] ×[n] we have |˜Ai,j −Ai,j |≤ ǫA ·Ai,j. 8Let D, ˜D ∈Rn×n be any diagonal matrices with positive entries on their diag onals, with the property that, for all i ∈[n], we have |˜Di,i −Di,i|≤ ǫD ·Di,i. Then, we have ∥˜D−1 ˜AV −D−1AV ∥∞ ≤(ǫA + ǫD) ·B. Proof. We have ∥˜D−1 ˜AV −D−1AV ∥∞ ≤∥˜D−1 ˜AV −D−1 ˜AV ∥∞ + ∥D−1 ˜AV −D−1AV ∥∞. (1) We now bound each of these two terms separately. First, for each ( i, j) ∈[n] ×[d], |( ˜D−1 ˜AV −D−1 ˜AV )i,j|= | n∑ l=1 ( ˜D−1 i,i −D−1 i,i ) ·˜Ai,l ·Vl,j | ≤ n∑ l=1 |( ˜D−1 i,i −D−1 i,i ) ·˜Ai,l|·∥V ∥∞ = n∑ l=1 |Di,i −˜Di,i Di,i ˜Di,i ˜Ai,l|·∥V ∥∞ ≤ǫD · D∑ l=1 |˜D−1 i ˜Ai,l|·∥V ∥∞ = ǫD ·| D∑ l=1 ˜D−1 i ˜Ai,l|·∥V ∥∞ = ǫD ·∥V ∥∞ ≤ǫD ·B (2) where the second step follows from the triangle inequality, the forth step follows from |(Di,i − ˜Di,i)/Di,i|≤ ǫD, the ﬁfth step follows from ˜D−1 i > 0 and ˜Ai,l > 0, and the last step follows from our assumption on V . Second, for each ( i, j) ∈[n] ×[d], |(D−1 ˜AV −D−1AV )i,j|= | n∑ l=1 D−1 i,i ( ˜Ai,l −Ai,l) ·Vl,j | ≤ n∑ l=1 |D−1 i,i |·|( ˜Ai,l −Ai,l)|·∥V ∥∞ = n∑ l=1 D−1 i,i ·|( ˜Ai,l −Ai,l)|·∥V ∥∞ ≤ n∑ l=1 D−1 i,i ·ǫAAi,l ·B = ǫA ·B, (3) where the second step follows from triangle inequality, the third step follows from D−1 i,i > 0, the forth step follows from |˜Ai,l −Ai,l|≤ ǫA ·Ai,l and the last step follows from deﬁnition of Di,i. The result follows by combining Eq. ( 1), and two inequalities (Eq. ( 2) and Eq. ( 3)). 93.7 Main Upper Bound Theorem 3.7.For positive integers n, d and real parameters ǫ > 0 and B > 1, there are pos- itive integers g = Θ(max { log(1/ǫ) log(log(1/ǫ)/B) , B2}) and r = (2(g+d) 2d ) such that: There is an algorithm (Algorithm 1) that runs in O(Tmat(n, r, d) + nrg) time to solve AAttC(n, d, B, ǫ ) (Deﬁnition 1.2). Proof. The running time of each step is shown in Algorithm 1; its running time follows from Lemma 3.4. Its correctness follows from Lemma 3.5 and Lemma 3.6. Algorithm 1 Our Algorithm 1: procedure PolyAttention(Q ∈Rn×d, K ∈Rn×d, V ∈Rn×d, n, d, B, ǫ ) ⊲ Theorem 1.4 2: ⊲ ǫ is the accuracy output 3: ⊲ ∥Q∥∞, ∥K∥∞, ∥V ∥∞ ≤B 4: g ←O(max{ log(1/ǫ) log(log(1/ǫ)/B) , B2}) 5: r ← (2(g+d) 2d ) 6: Construct U1, U2 ∈Rn×r via Lemma 3.4 ⊲ O (nrg) time 7: ˜w ←U1 ·(U⊤ 2 1n) ⊲ O (nr) time 8: ˜D−1 = diag( ˜w−1) ⊲ O (n) time 9: Compute U⊤ 2 V ∈Rr×d ⊲ Takes Tmat(r, n, d) time 10: Compute U1 ·(U⊤ 2 V ) ⊲ Tmat(n, r, d) time 11: T ←˜D−1 ·(U1 ·(U⊤ 2 V )) ⊲ O (nd) time 12: return T ⊲ T ∈Rn×d 13: end procedure 3.8 Proof of Theorem 1.4 Theorem 3.8 (Upper bound, formal statement of Theorem 1.4). AAttC(n, d = O(log n), B = o(√log n), ǫa = 1/ poly(n)) can be solved in time Tmat(n, no(1), d) = n1+o(1). Proof. If we select the parameters B = o( √ log n), ǫ = 1/ poly(n), d = O(log n) in Theorem 3.7, then we see that g = O(max{ log(1/ǫ) log(log(1/ǫ)/B), B2}) = O(max{ log(n) log(log(n)/B), B2}) = O(max{ log n log logn, o(log n)}) = o(log n), where the second step follows from ǫ = 1/ poly(n) and the third step follows from B = o(√log n). Since g = o(log n), let us write g = (log n)/f for some f = ω(1). We thus have that r = (2(d + g) 2g ) ≤ (e(d + g) g )2g ≤2O(g log((log n)/g)) ≤2O(log n log(f)/f) < 2o(log n) < n o(1). 10The second step follows from the generic bound (a b ) ≤(ea/b)b for 1 ≤b ≤a, and the third step uses that d = O(log n). Since d, r, g are all bounded by no(1), our ﬁnal running time is n1+o(1) as desired. 4 Hardness In this section, we prove our ﬁne-grained lower bound for attention computation. In Section 4.1, we state the Strong Exponential Time Hypothesis ( SETH), the main hardness assumption we will use. In Section 4.2, we deﬁne the approximate nearest neighbor search problem, and its known hardness assuming SETH. Finally, in Section 4.3, we give a reduction from approximate nearest neighbor search to attention computation, which implies ou r hardness result. 4.1 Fine-Grained Hypotheses The Strong Exponential Time Hypothesis (SETH) was introduced by Impagliazzo and Paturi [ IP01] over 20 years ago. It is a strengthening of the P ̸= NP conjecture, which asserts that our current best SAT algorithms are roughly optimal: Hypothesis 4.1 (Strong Exponential Time Hypothesis (SETH)). For every ǫ > 0 there is a positive integer k ≥3 such that k-SAT on formulas with n variables cannot be solved in O(2(1−ǫ)n) time, even by a randomized algorithm. SETHis a popular conjecture which has been used to prove ﬁne-grai ned lower bounds for a wide variety algorithmic problems. See, for instance, the surve y [ Wil18]. 4.2 Nearest Neighbor Search We will make use of a known relationship betweenSETH and approximate nearest neighbor search. Deﬁnition 4.2 (Approximate Hamming Nearest Neighbor Search (ANN)). For a parameter ǫ > 0, in the (1 + ǫ)-Approximate Hamming Nearest Neighbor Search problem for n vectors of dimension d, we are given as input two sets A, B ⊂{0, 1}d with |A|= |B|= n, and our goal is to ﬁnd an a∗ ∈A and b∗ ∈B satisfying ∥a∗ −b∗∥0 ≤(1 + ǫ) ·mina∈A,b∈B ∥a −b∥0. (This is sometimes called the ‘bichromatic’ANN problem, and a monochromatic version has also been studied; see, for instance, [ SM19].) Rubinstein [ Rub18] showed that for certain parameters, it is impossible to substantially improve on the straightfo rward quadratic-time algorithm for ANN assuming SETH: Lemma 4.3 ([Rub18]). Assuming SETH, for every q > 0, there are ǫ ∈(0, 1) and C > 0 such that (1 +ǫ)-Approximate Hamming Nearest Neighbor Search in dimension d = C log n requires Ω(n2−q) time. Remark 4.4.We may assume that 4.3 holds even in the special case where each input vector from A and B has half its entries equal to 0 and half equal to 1. Indeed, for any vector a ∈{0, 1}d, we can construct a new vector ˜a ∈{0, 1}2d given by ˜a = [ a⊤ a⊤] ⊤. Here a ∈{0, 1}d is the binary complement of vector a, i.e., ai = 1−ai for all i ∈[d]. Thus, ∥˜a∥0 = d. We can similarly construct a new vector ˜b ∈{0, 1}2d for each b ∈B. After this transformation, for any a ∈A and b ∈B, we have ∥˜a −˜b∥0 = 2 ·∥a −b∥0, so it suﬃces to ﬁnd an approximate nearest neighbor among th ese transformed vectors. 11For convenience in our the analysis, we deﬁne a gap version of approximate nearest neighbor search problem Gap−ANN(n, d, t, ǫ). Deﬁnition 4.5 (Gap approximate nearest neighbor search (Gap−ANN(n, d, t, ǫ))). Let n, d denote two positive integers. Let t > 0 denote a threshold parameter. Let ǫ denote a accuracy parameter. Given two sets of points A = {a1, ··· , an}⊂{ 0, 1}d and B = {b1, ··· , an}⊂{ 0, 1}d: For each i ∈[n], we need to distinguish the following two cases • Case 1. There exists a j ∈[n] such that ∥ai −bj∥0 < t . • Case 2. For all j ∈[n] we have ∥ai −bj ∥2 2≥(1 + ǫ) ·t. An algorithm for Gap−ANN(n, d, t, ǫ) can be called log(nd) times to binary search for the answer to ANN, so Lemma 4.3 holds as well for Gap−ANN(n, d, t, ǫ). 4.3 Hardness Result In the remainder of this section, we prove our lower bound forattention computation: Theorem 4.6 (Main Result, formal version of Theorem 1.3). Assuming SETH, for every suﬃciently small q > 0, there are constants C > 0 and Cα > 0 and Cβ > 1 such that Approximate Attention Computation AAttC (Deﬁnition 1.2) for parameters (n, d = C log n, B = Cβ √log n, ǫa = n−Cα ) requires Ω(n2−q) time. Proof. This follows from combining Lemma 4.3 (hardness for approximation nearest neighbor search) and Lemma 4.7 (a reduction from approximate nearest neighbor search to ap proximate attention computation) which we prove below. Lemma 4.7. For any constant Cγ ∈(0, 0.1): For every ǫ > 0 and C > 0, there exist constants Ca > 0 and Cb > 0 and such that, if AAttC (Deﬁnition 1.2) for parameters (2n, d = 2C log n, B = Cb √log n, ǫa = n−Ca ) can be solved in time T , then Gap−ANN(n, d = C log n, t, ǫ) (Deﬁnition 4.5) can be solved in time O(T + n2−Cγ ). Proof. We give an algorithm with the stated running time for Gap−ANN(n, d = C log n, t, ǫ). Let c > 0 be a parameter we will choose later (it will be a function of C and Cγ). Our algorithm will proceed to one of two cases depending on the value of t. If t < c log n, then we will use one algorithm which runs in time O(n2−Cγ ). Otherwise, if t ≥c log n, we will use another algorithm which runs in time O(T ). Case 1 : t < c log n. Let a1, ··· , an, b1, ··· , bn ∈{0, 1}d be the input vectors to Gap−ANN, and let t ∈[0, d] denote the target distance. Recall that d = C log n. In this t < c log n case, we will simply brute-force for the answer in the follow ing way: We ﬁrst store the vectors b1, ··· , bn in a lookup table, then for each i ∈[n], we iterate over all vectors b′ ∈{0, 1}d which have Hamming distance at most t from ai and check whether b′ is in the lookup table. This determines whether there is a b ∈B at distance at most t from ai, as desired. For each i ∈[n], we need to iterate over (d t ) choices for the vector b′, so the total running time will be O(n · (d t ) ). By standard bounds on binomial coeﬃcients, we know that n · (d t ) ≤n · (C log n c log n ) ≤n1+f(C,c) 12for some function f : R>0 ×R>0 →R>0 with the property that, for any ﬁxed C > 0, we have lim c→0 f(C, c) = 0. We can thus pick a suﬃciently small constant c > 0, depending only on Cγ and C such that f(C, c) < 1 −Cγ and this entire brute-force takes O(n2−Cγ ) time. Case 2: t ≥c log n. Let a1, ··· , an, b1, ··· , bn ∈{0, 1}d denote the input of Gap−ANN(n, d, t, ǫ) (Deﬁnition 4.5), and recall from Remark 4.4 that we may assume each has half its entries 0 and half its entr ies 1. We will explain how to construct an Attention matrix using this instance. Let C0 ≥c be such that t := C0 log n. (4) Let β > 0 and ˜d ≥d denote parameters we will choose later (see Eq. ( 9) and Eq. ( 6), respec- tively). Deﬁne τ > 0 by τ := exp(β). (5) Intuitively, our goal in picking these parameters is that τ will be an upper bound on entries of the attention matrix, i.e., we will have: τ ≥ max i∈[n],j∈[n] exp(β⟨ai, bj ⟩/ ˜d). We will make use of an algorithm for the AAttC(˜n, ˜d, B, ǫ a) problem, for the following parame- ters: ˜n := 2n, ˜d := 2d, (6) B := Cb √ log n, where Cb := √ 40C/(C0ǫ), (7) ǫa := n−Ca , where Ca := 2 + C2 b (1 + C0/C). (8) Furthermore, set β := B2. (9) We deﬁne Q ∈R˜n× ˜d and K ∈R˜n× ˜d as Q := √ β ·               a⊤ 1 1⊤ d a⊤ 2 1⊤ d . . . . . . a⊤ n 1⊤ d 0⊤ n 1⊤ d 0⊤ n 1⊤ d . . . . . . 0⊤ n 1⊤ d               and K := √ β ·               b⊤ 1 0 b⊤ 2 . . . . . . b⊤ n 0 0⊤ n 1⊤ d 0⊤ n 1⊤ d . . . . . . 0⊤ n 1⊤ d               . 13Since each entry of Q and K is either √β or 0, it follows that ∥Q∥∞ ≤ √ β = B ∥K∥∞ ≤ √ β = B ∥QK⊤/ ˜d∥∞ ≤β ·˜d ˜d = β = B2. Using the above construction of Q ∈R˜n× ˜d and K ∈R˜n× ˜d, we note that A := exp(QK⊤/ ˜d) ∈R˜n×˜n is given by A =                exp(β⟨a1, b1⟩/ ˜d) exp( β⟨a1, b2⟩/ ˜d) ··· exp(β⟨a1, bn⟩/ ˜d) τ τ ··· τ exp(β⟨a2, b1⟩/ ˜d) exp( β⟨a2, b2⟩/ ˜d) ··· exp(β⟨a2, bn⟩/ ˜d) τ τ ··· τ . . . . . . ... . . . . . . . . . ... . . . exp(β⟨an, b1⟩/ ˜d) exp( β⟨an, b2⟩/ ˜d) ··· exp(β⟨an, bn⟩/ ˜d) τ τ ··· τ 0 0 ··· 0 τ τ ··· τ 0 0 ··· 0 τ τ ··· τ . . . . . . ... . . . . . . . . . ... . . . 0 0 ··· 0 τ τ ··· τ                . (Note that we do not explicitly compute all the entries of A in our algorithm; we will make use of it only through calling our algorithm for the Attention prob lem.) For each (i, j) ∈[n] ×[n], we know that Ai,j = exp( β⟨ai, bj ⟩/ ˜d) ≤exp(β∥ai∥∞ ·∥bj ∥∞ ·d/ ˜d) ≤exp(β) = τ (10) where the ﬁrst step follows from deﬁnition of A ∈R˜n×˜n, the second step follows from ⟨ai, bj ⟩≤ ∥ai∥∞ ·∥bj ∥∞d, the third step follows from d < ˜d (see Eq. ( 6)), and the last step follows from deﬁnition of τ (see Eq. (5)). On the other hand, we know that that for each ( i, j) ∈[n] ×[n], Ai,j ≥0 (11) since it is an exponential of an entry of QK⊤/ ˜d. Using Eq. (10) and Eq. ( 11), combined with our expression for A, it thus follows that nτ ≤(A1˜n)i ≤2nτ, ∀i ∈[˜n]. Since Di,i = (A1˜n)i, thus we know that nτ ≤Di,i ≤2nτ, ∀i ∈[˜n]. 14Choose the vector v ∈R˜n deﬁned as v = [ 1n 0n ] . We deﬁne ˜t as ˜t := 1 3 exp(0.25β(1 −t/d))/(2nτ). (12) We can show that ˜t ≥ǫa as follows: ˜t = 1 6n exp(0.25β(1 −t/d) −β) = 1 6n exp(−0.75β −0.25βt/d) = 1 6n exp(−0.75β −0.25βC0/C) = 1 6 exp(−0.75β −0.25βC0/C −log n) = 1 6 exp(−0.75C2 b log n −0.25C2 b (C0/C) logn −log n) ≥n−Ca = ǫa, where the second step follows from simple algebra, the third step follows from t = C0 log n (Eq. (4)) and d = C log n (assumption in Lemma statement), the second step follows fr om choice of β (Eq. (7)), and the sixth step follows from choice of Ca (Eq. (8)), and the last step follows from Eq. (8). Since ˜t ≥ǫa, if we run an algorithm for Approximation Attention Computation (Deﬁnition 1.2) AAttC(˜n, ˜d, B, ǫ a), where we pick V to be a matrix with one row v and the rest 0, we can output a vector u ∈R˜n such that, for all i ∈[˜n], |ui −(D−1Av)i|< ˜t. Note that using Remark 4.4, we have ∥ai∥2 2/d = 0.5, ∀i ∈[n], ∥bj ∥2 2/d = 0.5, ∀j ∈[n]. Therefore, for any ( i, j) ∈[n] ×[n], 1 d⟨ai, bj ⟩= 1 2d(∥ai∥2 2+ ∥bj ∥2 2−∥ai −bj ∥2 2) = 1 2d(0.5d + 0.5d −∥ai −bj∥2 2) = 0.5 −0.5∥ai −bj∥2 2/d, where the second step follows from ∥ai∥2 2 = ∥bj ∥2 2 = d/2, and the last step follows from simple algebra. Recall that our goal is to determine, for each i ∈[n], whether there is a j ∈[n] such that ∥ai −bj ∥2 2≤t, or whether ∥ai −bj∥2 2≥(1 + ǫa)t for all j ∈[n]. We will show next that we can distinguish these two cases by seeing whether ui is greater than or less than the value ˜t0 := 2˜t. 15Case 2a. If there exists an (i, j) ∈[n] ×[n] such that ∥ai −bj ∥2 2≤t, then β⟨ai, bj ⟩/ ˜d = 0.5 ·β⟨ai, bj ⟩/d ≥0.25 ·β(1 −t/d), where the ﬁrst step follows from 2 d = ˜d (see Eq. ( 6)). This means that ui ≥ exp(0.25β(1 −t/d))/(2nτ) −˜t = 3˜t −˜t = 2˜t = ˜t0, where the second step follows from the deﬁnition of ˜t (see Eq. (12)), and the last step follows from the deﬁnition of ˜t0. Case 2b. If for all (i, j) ∈[n] ×[n], we have ∥ai −bj∥2 2> t (1 + ǫ), this implies β⟨ai, bj ⟩/d ≤0.25β ·(1 −t(1 + ǫ)/d). Then, for all i ∈[n], ui < (n ·exp(0.25β(1 −(1 + ǫ)t/d)))/(nτ) + ˜t = exp(0 .25β(1 −t/d))/(2nτ) ·(n/ exp(0.25βǫt/d)) + ˜t = 3˜t ·(n/ exp(0.25βǫt/d)) + ˜t ≤3˜t ·1 4 + ˜t = 2˜t = ˜t0, where the third step follows from deﬁnition of ˜t (see Eq. ( 12)), the forth step follows from the calculation in Eq. ( 13) below, and the last step follows from ˜t = ˜t0/2. Finally, b our choice of β and t, we can see that exp(0.25βǫt/d) = exp((0 .25βǫC0 log n)/d) = exp(0 .25βǫC0/C) = exp(10 log n) > 4n, (13) where the ﬁrst step follows t = C0 log n (Eq. (4)), the second step follows from d = C log n, and the third step follows from β = B2 (Eq. (9)) and the choice of B (Eq. (7)). 5 Conclusion In this work, we showed that how quickly one can perform attention computation depends critically on the magnitude, B, of the entries of the input matrices. Our main idea was to mak e use of similarities between attention computation and KDE, and to show how many known techniques for KDE can also be used in this setting. Since KDE is a very wel l-studied problem, it would be exciting to see what other techniquues can be applied to atte ntion computation as well. 16Acknowledgments The authors would like to thank Beidi Chen for helpful discus sions related to LLMs, and Feyza Duman, Chinmay Hegde, and Piotr Indyk for h elpful comments on an earlier draft. References [AA22] Amol Aggarwal and Josh Alman. Optimal-degree polyno mial approximations for ex- ponentials and gaussian kernel density estimation. In 37th Computational Complexity Conference (CCC 2022) . Schloss Dagstuhl-Leibniz-Zentrum f¨ ur Informatik, 2022. [ACM+20] Josh Alman, Timothy Chu, Gary Miller, Shyam Narayanan, M ark Sellke, and Zhao Song. Metric transforms and low rank matrices via represent ation theory of the real hyperrectangle. arXiv preprint arXiv:2011.11503 , 2020. [ACSS20] Josh Alman, Timothy Chu, Aaron Schild, and Zhao Son g. Algorithms and hardness for linear algebra on geometric graphs. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS) , pages 541–552. IEEE, 2020. [BCIS18] Arturs Backurs, Moses Charikar, Piotr Indyk, and P aris Siminelakis. Eﬃcient density evaluation for smooth kernels. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS) , pages 615–626. IEEE, 2018. [BIS17] Arturs Backurs, Piotr Indyk, and Ludwig Schmidt. On the ﬁne-grained complexity of empirical risk minimization: Kernel methods and neural net works. Advances in Neural Information Processing Systems , 30, 2017. [BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J ared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry , Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems (NeurIPS) , 33:1877–1901, 2020. [CDL+22] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao So ng, Atri Rudra, and Christopher Re. Pixelated butterﬂy: Simple and eﬃcient spa rse training for neural network models. In International Conference on Learning Representations (ICLR) , 2022. [CDW+21] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, a nd Christopher R´ e. Scat- terbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems (NeurIPS) , 34:17413–17426, 2021. [CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 , 2019. [cha22] Chatgpt: Optimizing language models for dialogue. In OpenAI Blog . https://openai.com/blog/chatgpt/, Nov 2022. [CKNS20] Moses Charikar, Michael Kapralov, Navid Nouri, an d Paris Siminelakis. Kernel density estimation through density constrained near neighbor search. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS) , pages 172–183. IEEE, 2020. 17[CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maar ten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas- tian Gehrmann, et al. Palm: Scaling language modeling with p athways. arXiv preprint arXiv:2204.02311, 2022. [CS17] Moses Charikar and Paris Siminelakis. Hashing-base d-estimators for kernel density in high dimensions. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS) , pages 1032–1043. IEEE, 2017. [CS19] Moses Charikar and Paris Siminelakis. Multi-resolu tion hashing for fast pairwise sum- mations. In 2019 IEEE 60th Annual Symposium on Foundations of Computer Sci ence (FOCS), pages 769–792. IEEE, 2019. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kris tina Toutanova. Bert: Pre- training of deep bidirectional transformers for language u nderstanding. arXiv preprint arXiv:1810.04805, 2018. [DKOD20] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf- eﬃcient attention using asymmetric clustering. Advances in Neural Information Pro- cessing Systems (NeurIPS) , 33:6476–6489, 2020. [IP01] Russell Impagliazzo and Ramamohan Paturi. On the com plexity of k-sat. Journal of Computer and System Sciences , 62(2):367–375, 2001. [KKL20] Nikita Kitaev, /suppress Lukasz Kaiser, and Anselm Levskaya. Reformer: The eﬃcient trans- former. arXiv preprint arXiv:2001.04451 , 2020. [KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap pas, and Fran¸ cois Fleuret. Trans- formers are rnns: Fast autoregressive transformers with li near attention. In Interna- tional Conference on Machine Learning (ICLR) , pages 5156–5165. PMLR, 2020. [KWH23] Feyza Duman Keles, Pruthuvi Mahesakya Wijewardena , and Chinmay Hegde. On the computational complexity of self-attention. In International Conference on Algorithmic Learning Theory, pages 597–619. PMLR, 2023. [LWD+23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhuang Yuan, Zhao Song, Anshumali Shrivastava, Yuandong Tian Ce Zhang, Christopher Re, and Be idi Chen. Deja vu: Contextual sparsity for eﬃcient llms at inference time. In ICML, 2023. [RNS+18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Su tskever, et al. Improving language understanding by generative pre-training. 2018. [Rub18] Aviad Rubinstein. Hardness of approximate nearest neighbor search. In Proceedings of the 50th annual ACM SIGACT symposium on theory of computing (S TOC), pages 1260–1268, 2018. [RWC+19] Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Am odei, Ilya Sutskever, et al. Language models are unsupervised multitask learners . OpenAI blog, 1(8):9, 2019. [SM19] Karthik C. S. and Pasin Manurangsi. On closest pair in euclidean metric: Monochro- matic is as hard as bichromatic. 10th Innovations in Theoretical Computer Science (ITCS), 2019. 18[TBY+19] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis -Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An uniﬁed u nderstanding for trans- former’s attention via the lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9t h International Joint Con- ference on Natural Language Processing (EMNLP-IJCNLP) , pages 4344–4353, 2019. [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor eit, Llion Jones, Aidan N Gomez, /suppress Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems (NeurIPS) , 30, 2017. [Wil18] Virginia Vassilevska Williams. On some ﬁne-graine d questions in algorithms and com- plexity. In Proceedings of the international congress of mathematicia ns: Rio de janeiro 2018, pages 3447–3487. World Scientiﬁc, 2018. [WLK+20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Ha o Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020. [ZBIW19] Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wa sserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Eﬃcient Machine Learning and Co gnitive Computing-NeurIPS Edition (EMC2-NIPS) , pages 36–39. IEEE, 2019. [ZHDK23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karb asi. Kdeformer: Accelerating transformers via kernel density estimation. arXiv preprint arXiv:2302.02451 , 2023. [ZRG+22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022. 19",
      "meta_data": {
        "arxiv_id": "2302.13214v2",
        "authors": [
          "Josh Alman",
          "Zhao Song"
        ],
        "published_date": "2023-02-26T02:42:39Z",
        "pdf_url": "https://arxiv.org/pdf/2302.13214v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies a sharp complexity transition for inner product attention computation at B = Θ(√log n), where B is the bound on input matrix entries. It presents an algorithm achieving n^(1+o(1)) time for approximate attention when d = O(log n) and B = o(√log n), with 1/poly(n) additive error (Theorem 1.4). Complementary to this, it provides a theoretical lower bound, demonstrating that truly subquadratic time (n^(2-Ω(1))) is impossible for approximate attention when d = O(log n) and B = Θ(√log n), assuming the Strong Exponential Time Hypothesis (SETH) (Theorem 1.3). This work offers a theoretical explanation for the practical observation that attention computation is more efficient when input matrices have smaller entries.",
        "methodology": "The research leverages similarities between attention computation and Kernel Density Estimation (KDE) problems. For the upper bound (fast algorithm), it employs the 'polynomial method' to approximate the entry-wise exponential function in the attention matrix (A = exp(QKᵀ/d)) with a low-degree polynomial. This allows for constructing a low-rank approximation of the attention matrix (Ã = U1 U2ᵀ) that can be computed efficiently, adapting prior work on Gaussian KDE through appropriate rescaling and careful control of error propagation. For the lower bound (hardness), a fine-grained reduction from the Approximate Nearest Neighbor (ANN) search problem, specifically a gap version of Approximate Hamming Nearest Neighbor Search (Gap-ANN), is used. This reduction constructs specific Q and K matrices for AAttC from ANN input vectors such that nearest neighbor distances correlate with attention matrix entry magnitudes, managing the D⁻¹ scaling factor by modifying input vectors. The proof relies on the Strong Exponential Time Hypothesis (SETH).",
        "experimental_setup": "This is a theoretical paper focusing on algorithmic complexity and lower bounds. It does not include empirical evaluations, specific datasets, benchmarks, or traditional experimental setups. Validation is provided through mathematical proofs of correctness and complexity analysis.",
        "limitations": "The proposed efficient algorithm (Theorem 1.4) is effective only when the magnitude of input matrix entries (B) is o(√log n). For B = Θ(√log n), the algorithm’s runtime reverts to trivial quadratic, which is proven to be optimal under SETH. The efficient algorithm also has a dimensionality constraint, primarily applying to d = O(log n). The lower bound's validity is contingent upon the Strong Exponential Time Hypothesis (SETH), an unproven but widely accepted conjecture in fine-grained complexity theory. The study focuses on additive error approximation, which may not cover all approximation scenarios in attention mechanisms.",
        "future_research_directions": "The authors suggest exploring further applications of techniques from the well-studied Kernel Density Estimation (KDE) literature to attention computation, building on the similarities identified in this work."
      }
    },
    {
      "title": "On the Connection Between MPNN and Graph Transformer",
      "abstract": "Graph Transformer (GT) recently has emerged as a new paradigm of graph\nlearning algorithms, outperforming the previously popular Message Passing\nNeural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022)\nshows that with proper position embedding, GT can approximate MPNN arbitrarily\nwell, implying that GT is at least as powerful as MPNN. In this paper, we study\nthe inverse connection and show that MPNN with virtual node (VN), a commonly\nused heuristic with little theoretical understanding, is powerful enough to\narbitrarily approximate the self-attention layer of GT.\n  In particular, we first show that if we consider one type of linear\ntransformer, the so-called Performer/Linear Transformer (Choromanski et al.,\n2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1)\nwidth can approximate a self-attention layer in Performer/Linear Transformer.\nNext, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN\nwith O(n^d) width and O(1) depth can approximate the self-attention layer\narbitrarily well, where d is the input feature dimension. Lastly, under some\nassumptions, we provide an explicit construction of MPNN + VN with O(1) width\nand O(n) depth approximating the self-attention layer in GT arbitrarily well.\nOn the empirical side, we demonstrate that 1) MPNN + VN is a surprisingly\nstrong baseline, outperforming GT on the recently proposed Long Range Graph\nBenchmark (LRGB) dataset, 2) our MPNN + VN improves over early implementation\non a wide range of OGB datasets and 3) MPNN + VN outperforms Linear Transformer\nand MPNN on the climate modeling task.",
      "full_text": "On the Connection Between MPNN and Graph Transformer Chen Cai 1 Truong Son Hy1 Rose Yu1 Yusu Wang1 Abstract Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022) shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understand- ing, is powerful enough to arbitrarily approximate the self-attention layer of GT. In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Trans- former (Choromanski et al., 2020; Katharopoulos et al., 2020b), then MPNN + VN with only O(1) depth and O(1) width can approximate a self- attention layer in Performer/Linear Transformer. Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with O(nd) width and O(1) depth can approximate the self-attention layer arbitrarily well, where d is the input feature dimension. Lastly, under some assumptions, we provide an explicit construction of MPNN + VN with O(1) width and O(n) depth approximating the self-attention layer in GT ar- bitrarily well. On the empirical side, we demon- strate that 1) MPNN + VN is a surprisingly strong baseline, outperforming GT on the recently pro- posed Long Range Graph Benchmark (LRGB) dataset, 2) our MPNN + VN improves over early implementation on a wide range of OGB datasets and 3) MPNN + VN outperforms Linear Trans- former and MPNN on the climate modeling task. 1University of California San Diego, San Diego, USA. Corre- spondence to: Chen Cai <c1cai@ucsd.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). VN  T ransformer  (a) (b)  Figure 1: MPNN + VN and Graph Transformers. 1. Introduction MPNN (Message Passing Neural Network) (Gilmer et al., 2017) has been the leading architecture for processing graph- structured data. Recently, transformers in natural language processing (Vaswani et al., 2017; Kalyan et al., 2021) and vision (d’Ascoli et al., 2021; Han et al., 2022) have extended their success to the domain of graphs. There have been several pieces of work (Ying et al., 2021; Wu et al., 2021; Kreuzer et al., 2021; Rampášek et al., 2022; Kim et al., 2022) showing that with careful position embedding (Lim et al., 2022), graph transformers (GT) can achieve compelling empirical performances on large-scale datasets and start to challenge the dominance of MPNN. MPNN imposes a sparsity pattern on the computation graph and therefore enjoys linear complexity. It however suffers from well-known over-smoothing (Li et al., 2018; Oono & Suzuki, 2019; Cai & Wang, 2020) and over-squashing (Alon & Yahav, 2020; Topping et al., 2021) issues, limiting its usage on long-range modeling tasks where the label of one node depends on features of nodes far away. GT relies purely on position embedding to encode the graph structure and uses vanilla transformers on top.1 It models all pairwise interactions directly in one layer, making it computationally more expensive. Compared to MPNN, GT shows promising results on tasks where modeling long-range interaction is the key, but the quadratic complexity of self-attention in GT 1GT in this paper refers to the practice of tokenizing graph nodes and applying standard transformers on top (Ying et al., 2021; Kim et al., 2022). There exists a more sophisticated GT (Kreuzer et al., 2021) that further conditions attention on edge types but it is not considered in this paper. 1 arXiv:2301.11956v4  [cs.LG]  21 Jun 2023On the Connection Between MPNN and Graph Transformer Table 1: Summary of approximation result of MPNN + VN on self-attention layer. n is the number of nodes and d is the feature dimension of node features. The dependency on d is hidden. Depth Width Self-Attention Note Theorem 4.1 O(1) O(1) Approximate Approximate self attention in Performer (Choromanski et al., 2020) Theorem 5.5 O(1) O(nd) Full Leverage the universality of equivariant DeepSets Theorem 6.3 O(n) O(1) Full Explicit construction, strong assumption on X Proposition B.9 O(n) O(1) Full Explicit construction, more relaxed (but still strong) assumption on X limits its usage to graphs of medium size. Scaling up GT to large graphs remains an active research area (Wu et al., 2022). Theoretically, it has been shown that graph transformers can be powerful graph learners (Kim et al., 2022), i.e., graph transformers with appropriate choice of token embeddings have the capacity of approximating linear permutation equiv- ariant basis, and therefore can approximate 2-IGN (Invariant Graph Network), a powerful architecture that is at least as expressive as MPNN (Maron et al., 2018). This raises an important question that whether GT is strictly more powerful than MPNN. Can we approximate GT with MPNN? One common intuition of the advantage of GT over MPNN is its ability to model long-range interaction more effectively. However, from the MPNN side, one can resort to a simple trick to escape locality constraints for effective long-range modeling: the use of an additional virtual node (VN) that connects to all input graph nodes. On a high level, MPNN + VN augments the existing graph with one virtual node, which acts like global memory for every node exchanging messages with other nodes. Empirically this simple trick has been observed to improve the MPNN and has been widely adopted (Gilmer et al., 2017; Hu et al., 2020; 2021) since the early beginning of MPNN (Gilmer et al., 2017; Battaglia et al., 2018). However, there is very little theoretical study of MPNN + VN (Hwang et al., 2022). In this work, we study the theoretical property of MPNN + VN, and its connection to GT. We systematically study the representation power of MPNN + VN, both for certain approximate self-attention and for the full self-attention layer, and provide a depth-width trade-off, summarized in Table 1. In particular, • With O(1) depth and O(1) width, MPNN + VN can approximate one self-attention layer of Performer (Choromanski et al., 2020) and Linear Transformer (Katharopoulos et al., 2020b), a type of linear trans- formers (Tay et al., 2020). • Via a link between MPNN + VN with DeepSets (Za- heer et al., 2017), we prove MPNN + VN with O(1) depth and O(nd) width (d is the input feature dimen- sion) is permutation equivariant universal, implying it can approximate self-attention layer and even full- transformers. • Under certain assumptions on node features, we prove an explicit construction of O(n) depth O(1) width MPNN + VN approximating 1 self-attention layer ar- bitrarily well on graphs of size n. Unfortunately, the assumptions on node features are rather strong, and whether we can alleviate them will be an interesting future direction to explore. • Empirically, we show 1) that MPNN + VN works sur- prisingly well on the recently proposed LRGB (long- range graph benchmarks) datasets (Dwivedi et al., 2022), which arguably require long-range interaction reasoning to achieve strong performance 2) our imple- mentation of MPNN + VN is able to further improve the early implementation of MPNN + VN on OGB datasets and 3) MPNN + VN outperforms Linear Trans- former (Katharopoulos et al., 2020b) and MPNN on the climate modeling task. 2. Related Work Virtual node in MPNN. The virtual node augments the graph with an additional node to facilitate the information exchange among all pairs of nodes. It is a heuristic proposed in (Gilmer et al., 2017) and has been observed to improve the performance in different tasks (Hu et al., 2021; 2020). Surprisingly, its theoretical properties have received little study. To the best of our knowledge, only a recent paper (Hwang et al., 2022) analyzed the role of the virtual node in the link prediction setting in terms of 1) expressiveness of the learned link representation and 2) the potential impact on under-reaching and over-smoothing. Graph transformer. Because of the great successes of Transformers in natural language processing (NLP) (Vaswani et al., 2017; Wolf et al., 2020) and recently in computer vision (Dosovitskiy et al., 2020; d’Ascoli et al., 2021; Liu et al., 2021), there is great interest in extending transformers for graphs (Müller et al., 2023). One common belief of advantage of graph transformer over MPNN is its capacity in capturing long-range interactions while alleviat- ing over-smoothing (Li et al., 2018; Oono & Suzuki, 2019; 2On the Connection Between MPNN and Graph Transformer Cai & Wang, 2020) and over-squashing in MPNN (Alon & Yahav, 2020; Topping et al., 2021). Fully-connected Graph transformer (Dwivedi & Bresson, 2020) was introduced with eigenvectors of graph Laplacian as the node positional encoding (PE). Various follow-up works proposed different ways of PE to improve GT, ranging from an invariant aggregation of Laplacian’s eigenvectors in SAN (Kreuzer et al., 2021), pair-wise graph distances in Graphormer (Ying et al., 2021), relative PE derived from dif- fusion kernels in GraphiT (Mialon et al., 2021), and recently Sign and Basis Net (Lim et al., 2022) with a principled way of handling sign and basis invariance. Other lines of re- search in GT include combining MPNN and GT (Wu et al., 2021; Rampášek et al., 2022), encoding the substructures (Chen et al., 2022), GT for directed graphs (Geisler et al., 2023), and efficient graph transformers for large graphs (Wu et al., 2022). Deep Learning on Sets. Janossy pooling (Murphy et al., 2018) is a framework to build permutation invariant ar- chitecture for sets using permuting & averaging paradigm while limiting the number of elements in permutations to be k < n. Under this framework, DeepSets (Zaheer et al., 2017) and PointNet (Qi et al., 2017) are recovered as the case of k = 1. For case k = 2, self-attention and Relation Network (Santoro et al., 2017) are recovered (Wagstaff et al., 2022). Although DeepSets and Relation Network (Santoro et al., 2017) are both shown to be universal permutation invariant, recent work (Zweig & Bruna, 2022) provides a finer characterization on the representation gap between the two architectures. 3. Preliminaries We denote X ∈ Rn×d the concatenation of graph node features and positional encodings, where node i has feature xi ∈ Rd. When necessary, we use x(l) j to denote the node j’s feature at depth l. Let M be the space of multisets of vectors in Rd. We use X ⊆Rn×d to denote the space of node features and the Xi be the projection of X on i-th coordinate. ∥ · ∥denotes the 2-norm. [x, y, z] denotes the concatenation of x, y, z. [n] stands for the set {1, 2, ..., n}. Definition 3.1 (attention). We denote key and query matrix as WK, WQ ∈ Rd×d′ , and value matrix as WV ∈ Rd×d 2. Attention score between two vectors u, v ∈ Rd×1 is de- fined as α(u, v) = softmax(uT WQ(WK)T v). We denote A as the space of attention α for different WQ, WK, WV . We also define unnormalized attention score α′(·, ·) to be α′(u, v) = uT WQ(WK)T v. Self attention layer is a ma- 2For simplicity, we assume the output dimension of self- attention is the same as the input dimension. All theoretical results can be extended to the case where the output dimension is different from d. trix function L : Rn×d → Rn×d of the following form: L(X) = softmax(XWQ(XWK)T )XWV . 3.1. MPNN Layer Definition 3.2 (MPNN layer (Gilmer et al., 2017)) . An MPNN layer on a graph G with node features x(k) at k-th layer and edge features e is of the following form x(k) i = γ(k) \u0010 x(k−1) i , τj∈N(i)ϕ(k) \u0010 x(k−1) i , x(k−1) j , ej,i \u0011\u0011 Here γ : Rd × Rd′ → Rd is update function, ϕ : Rd × Rd × Rde → Rd′ is message function where de is the edge feature dimension, τ : M →Rd is permutation invariant aggregation function and N(i) is the neighbors of node i in G. Update/message/aggregation functions are usually parametrized by neural networks. For graphs of different types of edges and nodes, one can further extend MPNN to the heterogeneous setting. We use 1, ..., nto index graph nodes and vn to denote the virtual node. Definition 3.3 (heterogeneous MPNN + VN layer) . The heterogeneous MPNN + VN layer operates on two types of nodes: 1) virtual node and 2) graph nodes, denoted as vn and gn, and three types of edges: 1) vn-gn edge and 2) gn-gn edges and 3) gn-vn edges. It has the following form x(k) vn = γ(k) vn \u0010 x(k−1) i , τj∈[n]ϕ(k) vn-gn \u0010 x(k−1) i , x(k−1) j , ej,i \u0011\u0011 (1) for the virtual node, and x(k) i = γ(k) gn (x(k−1) i , τj∈N1(i)ϕ(k) gn-vn \u0010 x(k−1) i , x(k−1) j , ej,i \u0011 + τj∈N2(i)ϕ(k) gn-gn \u0010 x(k−1) i , x(k−1) j , ej,i) \u0011 (2) for graph node. Here N1(i) for graph node i is the virtual node and N2(i) is the set of neighboring graph nodes. Our proof of approximating self-attention layer L with MPNN layers does not use the graph topology. Next, we introduce a simplified heterogeneous MPNN + VN layer, which will be used in the proof. It is easy to see that set- ting ϕ(k) gn-gn to be 0 in Definition 3.3 recovers the simplified heterogeneous MPNN + VN layer. Definition 3.4 (simplified heterogeneous MPNN + VN layer). A simplified heterogeneous MPNN + VN layer is the same as a heterogeneous MPNN + VN layer in Defini- tion 3.3 except we set θgn-gn to be 0. I.e., we have x(k) vn = γ(k) vn \u0010 x(k−1) i , τj∈[n]ϕ(k) vn-gn \u0010 x(k−1) i , x(k−1) j , ej,i \u0011\u0011 for the virtual node, and x(k) i = γ(k) gn \u0010 x(k−1) i , τj∈N1(i)ϕ(k) gn-vn \u0010 x(k−1) i , x(k−1) j , ej,i \u0011\u0011 3On the Connection Between MPNN and Graph Transformer for graph nodes. Intuitively, adding the virtual node (VN) to MPNN makes it easy to compute certain quantities, for example, the mean of node features (which is hard for standard MPNN unless the depth is proportional to the diameter of the graph). Us- ing VN thus makes it easy to implement for example the mean subtraction, which helps reduce over-smoothing and improves the performance of GNN (Yang et al., 2020; Zhao & Akoglu, 2019). See more connection between MPNN + VN and over-smoothing in Appendix D.6. 3.2. Assumptions We have two mild assumptions on feature spaceX ⊂Rn×d and the regularity of target function L. AS1. ∀i ∈ [n], xi ∈ Xi, ∥xi∥ < C1. This implies X is compact. AS2. ∥WQ∥ < C2, ∥WK∥ < C2, ∥WV ∥ < C2 for target layer L. Combined with AS1 on X, this means α′(xi, xj) is both upper and lower bounded, which further impliesP j eα′(xi,xj) be both upper bounded and lower bounded. 4. O(1)-depth O(1)-width MPNN + VN for unbiased approximation of attention The standard self-attention takes O(n2) computational time, therefore not scalable for large graphs. Reducing the compu- tational complexity of self-attention in Transformer is active research (Tay et al., 2020). In this section, we consider self-attention in a specific type of efficient transformers, Per- former (Choromanski et al., 2020) and Linear Transformer (Katharopoulos et al., 2020b). One full self-attention layer L is of the following form x(l+1) i = nX j=1 κ \u0010 W(l) Q x(l) i , W(l) K x(l) j \u0011 Pn k=1 κ \u0010 W(l) Q x(l) i , W(l) K x(l) k \u0011· \u0010 W(l) V x(l) j \u0011 (3) where κ : Rd × Rd → R is the softmax kernel κ(x, y) := exp( xT y). The kernel function can be ap- proximated via κ(x, y) = ⟨Φ(x), Φ(y)⟩V ≈ ϕ(x)T ϕ(y) where the first equation is by Mercer’s theorem and ϕ(·) : Rd → Rm is a low-dimensional feature map with random transformation. For Performer (Choroman- ski et al., 2020), the choice of ϕ is taken as ϕ(x) = exp \u0012 −∥x∥2 2 2 \u0013 √m \u0002 exp \u0000 wT 1 x \u0001 , ··· , exp \u0000 wT mx \u0001\u0003 where wk ∼ N (0, Id) is i.i.d sampled random variable. For Linear Trans- former (Katharopoulos et al., 2020b), ϕ(x) = elu(x) + 1. By switching κ(x, y) to be ϕ(x)T ϕ(y), and denote qi = W(l) Q x(l) i , ki = W(l) K x(l) i and vi = W(l) V x(l) i , the approx- imated version of Equation (3) by Performer and Linear Transformer becomes x(l+1) i = nX j=1 ϕ (qi)T ϕ (kj)Pn k=1 ϕ (qi)T ϕ (kk) · vj = \u0010 ϕ (qi)T Pn j=1 ϕ (kj) ⊗ vj \u0011T ϕ (qi)T Pn k=1 ϕ (kk) . (4) where we use the matrix multiplication association rule to derive the second equality. The key advantage of Equation (4) is that Pn j=1 ϕ (kj) andPn j=1 ϕ(kj) ⊗ vj can be approximated by the virtual node, and shared for all graph nodes, using only O(1) layers of MPNNs. We denote the self-attention layer of this form in Equation (4) as LPerformer. Linear Transformer differs from Performer by choosing a different form of ϕ(x) = Relu(x) + 1in its self-attention layer LLinear-Transformer. In particular, the VN will approximate Pn j=1 ϕ (kj) andPn j=1 ϕ (kj) ⊗ vj, and represent it as its feature. Both ϕ (kj) and ϕ (kj) ⊗vj can be approximated arbitrarily well by an MLP with constant width (constant in n but can be exponential in d) and depth. Note that ϕ(kj) ⊗ vj ∈ Rdm but can be reshaped to 1 dimensional feature vector. More specifically, the initial feature for the virtual node is 1(d+1)m, where d is the dimension of node features and m is the number of random projections ωi. Message function + aggregation function for virtual node τϕvn-gn : R(d+1)m × M →R(d+1)m is τj∈[n]ϕ(k) vn-gn(·, {xi}i) = [ nX j=1 ϕ (kj) , ReshapeTo1D( nX j=1 ϕ (kj) ⊗ vj)] (5) where ReshapeTo1D(·) flattens a 2D matrix to a 1D vec- tor in raster order. This function can be arbitrarily approxi- mated by MLP. Note that the virtual node’s feature dimen- sion is (d + 1)m (where recall m is the dimension of the feature map ϕ used in the linear transformer/Performer), which is larger than the dimension of the graph node d. This is consistent with the early intuition that the vir- tual node might be overloaded when passing information among nodes. The update function for virtual node γvn : R(d+1)m × R(d+1)m → R(d+1)m is just coping the second argument, which can be exactly implemented by MLP. VN then sends its message back to all other nodes, where each graph node i applies the update function γgn : 4On the Connection Between MPNN and Graph Transformer R(d+1)m × Rd → Rd of the form γgn(xi, [ nX j=1 ϕ (kj) , ReshapeTo1D( nX j=1 ϕ (kj) ⊗ vj)]) = \u0010 ϕ (qi) Pn j=1 ϕ (kj) ⊗ vj \u0011T ϕ (qi)T Pn k=1 ϕ (kk) (6) to update the graph node feature. As the update function γgn can not be computed exactly in MLP, what is left is to show that error induced by using MLP to approximate τϕvn-gn and γgn in Equation (5) and Equation (6) can be made arbitrarily small. Theorem 4.1. Under the AS1 and AS2, MPNN + VN of O(1) width and O(1) depth can approximate LPerformer and LLinear-Transformer arbitrarily well. Proof. We first prove the case of LPerformer. We can decom- pose our target function as the composition of τj∈[n]ϕ(k) vn-gn, γgn and ϕ. By the uniform continuity of the functions, it suffices to show that 1) we can approximate ϕ, 2) we can approximate operations in γgn and τϕvn-gn arbitrar- ily well on the compact domain, and 3) the denominator ϕ (qi)T Pn k=1 ϕ (kk) is uniformly lower bounded by a pos- itive number for any node features in X. For 1), each component of ϕ is continuous and all inputs kj, qj lie in the compact domain so ϕ can be approximated arbitrarily well by MLP with O(1) width and O(1) depth (Cybenko, 1989). For 2), we need to approximate the operations in γgn and τϕvn-gn, i.e., approximate multiplication, and vector-scalar division arbitrarily well. As all those operations are con- tinuous, it boils down to showing that all operands lie in a compact domain. By assumption AS1 and AS2 on WQ, WK, WV and input featureX, we know thatqi, ki, vi lies in a compact domain for all graph nodes i. As ϕ is con- tinuous, this implies that ϕ(qi), Pn j=1 ϕ(kj) ⊗ vj lies in a compact domain (n is fixed), therefore the numerator lies in a compact domain. Lastly, since all operations do not involve n, the depth and width are constant in n. For 3), it is easy to see that ϕ (qi)T Pn k=1 ϕ (kk) is always positive. We just need to show that the denominator is bound from below by a positive constant. For Performer, ϕ(x) = exp \u0012 −∥x∥2 2 2 \u0013 √m \u0002 exp \u0000 wT 1 x \u0001 , ··· , exp \u0000 wT mx \u0001\u0003 where wk ∼ N (0, Id). As all norm of input x to ϕ is upper bounded by AS1, exp(−∥x∥2 2 2 ) is lower bounded. As m is fixed, we know that ∥wT i x∥ ≤ ∥wi∥∥x∥, which implies that wT i x is lower bounded by −∥wi∥∥x∥ which further im- plies that exp(wT i x) is lower bounded. This means that MPNN (GCN, GAT, GraphSage...) + Virtual Node Graph Transformer DeepSets Invariant Graph Network (IGN) Figure 2: The link between MPNN and GT is drawn via DeepSets in Section 5 of our paper and Invariant Graph Network (IGN) in Kim et al. (2022). Interestingly, IGN is a generalization of DeepSets (Maron et al., 2018). ϕ (qi)T Pn k=1 ϕ (kk) is lower bounded. For Linear Transformer, the proof is essentially the same as above. We only need to show that ϕ(x) = elu(x) + 1is continuous and positive, which is indeed the case. Besides Performers, there are many other different ways of obtaining linear complexity. In Appendix C.2, we discuss the limitation of MPNN + VN on approximating other types of efficient transformers such as Linformer (Wang et al., 2020b) and Sparse Transformer (Child et al., 2019). 5. O(1) depth O(nd) width MPNN + VN We have shown that the MPNN + VN can approximate self- attention in Performer and Linear Transformer using only O(1) depth and O(1) width. One may naturally wonder whether MPNN + VN can approximate the self-attention layer in the full transformer. In this section, we show that MPNN + VN with O(1) depth (number of layers), but with O(nd) width, can approximate 1 self-attention layer (and full transformer) arbitrarily well. The main observation is that MPNN + VN is able to ex- actly simulate (not just approximate) equivariant DeepSets (Zaheer et al., 2017), which is proved to be universal in approximating any permutation invariant/equivariant maps (Zaheer et al., 2017; Segol & Lipman, 2019). Since the self-attention layer is permutation equivariant, this implies that MPNN + VN can approximate the self-attention layer (and full transformer) with O(1) depth and O(nd) width fol- lowing a result on DeepSets from Segol & Lipman (2019). We first introduce the permutation equivariant map, equiv- ariant DeepSets, and permutation equivariant universality. Definition 5.1 (permutation equivariant map). A map F : Rn×k → Rn×l satisfying F(σ · X) = σ · F(X) for all σ ∈ Sn and X ∈ Rn×d is called permutation equivariant. Definition 5.2 (equivariant DeepSets of Zaheer et al. (2017)). Equivariant DeepSets has the following form F(X) = Lds m◦ν◦···◦ ν◦Lds 1 (X), where Lds i is a linear per- mutation equivariant layer and ν is a nonlinear layer such as ReLU. The linear permutation equivariant layer inDeepSets has the following formLds i (X) = XA+ 1 n 11T XB+1cT , 5On the Connection Between MPNN and Graph Transformer where A, B ∈ Rdi×di+1, c ∈ Rdi+1 is the weights and bias in layer i, and ν is ReLU. Definition 5.3 (permutation equivariant universality). Given a compact domain X of Rn×din , permutation equivariant universality of a model F : Rn×din → Rn×dout means that for every permutation equivariant continuous function H : Rn×din → Rn×dout defined over X, and any ϵ > 0, there exists a choice of m (i.e., network depth), di (i.e., network width at layer i) and the trainable parameters of F so that ∥H(X) − F(X)∥∞ < ϵfor all X ∈ X. The universality of equivariantDeepSets is stated as follows. Theorem 5.4 (Segol & Lipman (2019)). DeepSets with con- stant layer is universal. Using ReLU activation the width ω := maxidi (di is the width for i-th layer of DeepSets) required for universal permutation equivariant network sat- isfies ω ≤ dout + din + \u0012 n + din din \u0013 = O(ndin ). We are now ready to state our main theorem. Theorem 5.5. MPNN + VN can simulate (not just approx- imate) equivariant DeepSets: Rn×d → Rn×d. The depth and width of MPNN + VN needed to simulateDeepSets is up to a constant factor of the depth and width ofDeepSets. This implies that MPNN + VN of O(1) depth and O(nd) width is permutation equivariant universal, and can approximate self-attention layer and transformers arbitrarily well. Proof. Equivariant DeepSets has the following form F(X) = Lds m ◦ ν ◦ ··· ◦ν ◦ Lds 1 (X), where Lds i is the linear permutation equivariant layer and ν is an entrywise nonlinear activation layer. Recall that the linear equivariant layer has the form Lds i (X) = XA + 1 n 11T XB +1cT . As one can use the same nonlinear entrywise activation layer ν in MPNN + VN, it suffices to prove that MPNN + VN can compute linear permutation equivariant layer Lds. Now we show that 2 layers of MPNN + VN can exactly simulate any given linear permutation equivariant layer Lds. Specifically, at layer 0, we initialized the node features as follows: The VN node feature is set to 0, while the node feature for the i-th graph node is set up as xi ∈ Rd. At layer 1: VN node feature is 1 n 11T X, average of node features. The collection of features over n graph node fea- ture is XA. We only need to transform graph node features by a linear transformation, and set the VN feature as the average of graph node features in the last iteration. Both can be exactly implemented in Definition 3.4 of simplified heterogeneous MPNN + VN. At layer 2: VN node feature is set to be 0, and the graph node feature is XA + 1 n 11T XB + 1cT . Here we only need to perform the matrix multiplication of the VN feature with B, as well as add a bias c. This can be done by implementing a linear function for γgn. It is easy to see the width required for MPNN + VN to simulate DeepSets is constant. Thus, one can use 2 layers of MPNN + VN to compute linear permutation equivariant layer Lds i , which implies that MPNN + VN can simulate 1 layer of DeepSets exactly with constant depth and con- stant width (independent of n). Then by the universality of DeepSets, stated in Theorem 5.4, we conclude that MPNN + VN is also permutation equivariant universal, which implies that the constant layer of MPNN + VN with O(nd) width is able to approximate any continuous equivariant maps. As the self-attention layer L and full transformer are both continuous and equivariant, they can be approximated by MPNN + VN arbitrarily well. Thanks to the connection between MPNN + VN with DeepSets, there is no extra assumption on X except for being compact. The drawback on the other hand is that the upper bound on the computational complexity needed to approximate the self-attention with wide MPNN + VN is worse than directly computing self-attention when d >2. 6. O(n) depth O(1) width MPNN + VN The previous section shows that we can approximate a full at- tention layer in Transformer using MPNN with O(1) depth but O(nd) width where n is the number of nodes andd is the dimension of node features. In practice, it is not desirable to have the width depend on the graph size. In this section, we hope to study MPNN + VNs with O(1) width and their ability to approximate a self-attention layer in the Transformer. However, this appears to be much more challenging. Our result in this section only shows that for a rather restrictive family of input graphs (see Assumption 3 below), we can approximate a full self-attention layer of transformer with an MPNN + VN of O(1) width and O(n) depth. We leave the question of MPNN + VN’s ability in approximate transformers for more general families of graphs for future investigation. We first introduce the notion of (V , δ) separable node fea- tures. This is needed to ensure that VN can approximately select one node feature to process at each iteration with attention αvn, the self-attention in the virtual node. Definition 6.1 ((V , δ) separable by α). Given a graph G of size n and a fixed V ∈ Rn×d = [v1, ...,vn] and ¯α ∈ A, we say node feature X ∈ Rn×d of G is (V , δ) separable by some ¯α if the following holds. For any node feature xi, there exist weights W ¯α K, W ¯α Q in attention score ¯α such that ¯α(xi, vi) > maxj̸=i ¯α(xj, vi) + δ. We say set X is (V , δ) separable by ¯α if every element X ∈ Xis (V , δ) separable by ¯α. 6On the Connection Between MPNN and Graph Transformer Table 2: Baselines for Peptides-func (graph classification) and Peptides-struct (graph regression). The perfor- mance metric is Average Precision (AP) for classification and MAE for regression. Bold: Best score. Model # Params. Peptides-func Peptides-struct Test AP before VN Test AP after VN↑ Test MAE before VN Test MAE after VN↓ GCN 508k 0.5930 ±0.0023 0.6623 ±0.0038 0.3496 ±0.0013 0.2488±0.0021 GINE 476k 0.5498 ±0.0079 0.6346 ±0.0071 0.3547 ±0.0045 0.2584 ±0.0011 GatedGCN 509k 0.5864 ±0.0077 0.6635 ±0.0024 0.3420 ±0.0013 0.2523 ±0.0016 GatedGCN+RWSE 506k 0.6069 ±0.0035 0.6685±0.0062 0.3357±0.0006 0.2529 ±0.0009 Transformer+LapPE 488k 0.6326 ±0.0126 - 0.2529 ±0.0016 - SAN+LapPE 493k 0.6384 ±0.0121 - 0.2683 ±0.0043 - SAN+RWSE 500k 0.6439 ±0.0075 - 0.2545 ±0.0012 - Table 3: Test performance in graph-level OGB benchmarks (Hu et al., 2020). Shown is the mean ± s.d. of 10 runs. Model ogbg-molhiv ogbg-molpcba ogbg-ppa ogbg-code2 AUROC ↑ Avg. Precision↑ Accuracy ↑ F1 score ↑ GCN 0.7606 ± 0.0097 0.2020 ± 0.0024 0.6839 ± 0.0084 0.1507 ± 0.0018 GCN+virtual node 0.7599 ± 0.0119 0.2424 ± 0.0034 0.6857 ± 0.0061 0.1595 ± 0.0018 GIN 0.7558 ± 0.0140 0.2266 ± 0.0028 0.6892 ± 0.0100 0.1495 ± 0.0023 GIN+virtual node 0.7707 ± 0.0149 0.2703 ± 0.0023 0.7037 ± 0.0107 0.1581 ± 0.0026 SAN 0.7785 ± 0.2470 0.2765 ± 0.0042 – – GraphTrans (GCN-Virtual) – 0.2761 ± 0.0029 – 0.1830 ± 0.0024 K-Subtree SAT – – 0.7522 ± 0.0056 0.1937 ± 0.0028 GPS 0.7880 ± 0.0101 0.2907 ± 0.0028 0.8015 ± 0.0033 0.1894 ± 0.0024 MPNN + VN + NoPE 0.7676 ± 0.0172 0.2823 ± 0.0026 0.8055 ± 0.0038 0.1727 ± 0.0017 MPNN + VN + PE 0.7687 ± 0.0136 0.2848 ± 0.0026 0.8027 ± 0.0026 0.1719 ± 0.0013 The use of (V , δ) separability is to approximate hard se- lection function arbitrarily well, which is stated below and proved in Appendix B.1. Lemma 6.2 (approximate hard selection) . Given X is (V , δ) separable by ¯α for some fixed V ∈ Rn×d, ¯α ∈ A and δ >0, the following holds. For any ϵ >0 and i ∈ [n], there exists a set of attention weights Wi,Q, Wi,K in i-th layer of MPNN + VN such that αvn(xi, vi) > 1 − ϵ for any xi ∈ Xi. In other words, we can approximate a hard selection function fi(x1, ...,xn) = xi arbitrarily well on X by setting αvn = ¯α. With the notation set up, We now state an extra assumption needed for deep MPNN + VN case and the main theorem. AS3. X is (V , δ) separable by ¯α for some fixed V ∈ Rn×d, ¯α ∈ Aand δ >0. Theorem 6.3. Assume AS 1-3 hold for the compact set X and L. Given any graph G of size n with node featuresX ∈ X, and a self-attention layer L on G (fix WK, WQ, WV in α), there exists a O(n) layer of heterogeneous MPNN + VN with the specific aggregate/update/message function that can approximate L on X arbitrarily well. The proof is presented in the Appendix B. On the high level, we can design an MPNN + VN where the i-th layer will select ˜xi, an approximation of xi via attention mechanism, enabled by Lemma 6.2, and send ˜xi to the virtual node. Virtual node will then pass the ˜xi to all graph nodes and computes the approximation of eα(xi,xj), ∀j ∈ [n]. Repeat such procedures n times for all graph nodes, and finally, use the last layer for attention normalization. A slight relaxation of AS3 is also provided in the appendix. 7. Experiments We benchmark MPNN + VN for three tasks, long range interaction modeling in Section 7.1, OGB regression tasks in Section 7.2, and focasting sea surface temperature in Section 7.3. The code is available https://github. com/Chen-Cai-OSU/MPNN-GT-Connection . 7.1. MPNN + VN for LRGB Datasets We experiment with MPNN + VN for Long Range Graph Benchmark (LRGB) datasets. Original paper (Dwivedi et al., 2022) observes that GT outperforms MPNN on 4 out of 5 datasets, among which GT shows signifi- cant improvement over MPNN on Peptides-func and Peptides-struct for all MPNNs. To test the effec- tiveness of the virtual node, we take the original code and modify the graph topology by adding a virtual node and keeping the hyperparameters of all models unchanged. Results are in Table 2. Interestingly, such a simple change can boost MPNN + VN by a large margin on 7On the Connection Between MPNN and Graph Transformer Table 4: Evaluation on PCQM4Mv2 (Hu et al., 2021) dataset. For GPS evaluation, we treated the validation set of the dataset as a test set, since the test-dev set labels are private. Model PCQM4Mv2 Test-dev MAE ↓ Validation MAE ↓ Training MAE # Param. GCN 0.1398 0.1379 n/a 2.0M GCN-virtual 0.1152 0.1153 n/a 4.9M GIN 0.1218 0.1195 n/a 3.8M GIN-virtual 0.1084 0.1083 n/a 6.7M GRPE (Park et al., 2022) 0.0898 0.0890 n/a 46.2M EGT (Hussain et al., 2022) 0.0872 0.0869 n/a 89.3M Graphormer (Shi et al., 2022) n/a 0.0864 0.0348 48.3M GPS-small n/a 0.0938 0.0653 6.2M GPS-medium n/a 0.0858 0.0726 19.4M MPNN + VN + PE (small) n/a 0.0942 0.0617 5.2M MPNN + VN + PE (medium) n/a 0.0867 0.0703 16.4M MPNN + VN + NoPE (small) n/a 0.0967 0.0576 5.2M MPNN + VN + NoPE (medium) n/a 0.0889 0.0693 16.4M Peptides-func and Peptides-struct. Notably, with the addition of VN, GatedGCN + RWSE (random-walk structural encoding) after augmented by VN outperforms all transformers on Peptides-func, and GCN outper- forms transformers on Peptides-struct. 7.2. Stronger MPNN + VN Implementation Next, by leveraging the modularized implementation from GraphGPS (Rampášek et al., 2022), we implemented a ver- sion of MPNN + VN with/without extra positional embed- ding. Our goal is not to achieve SOTA but instead to push the limit of MPNN + VN and better understand the source of the performance gain for GT. In particular, we replace the GlobalAttention Module in GraphGPS with DeepSets, which is equivalent to one specific version of MPNN + VN. We tested this specific version of MPNN + VN on 4 OGB datasets, both with and without the use of positional em- bedding. The results are reported in Table 3. Interestingly, even without the extra position embedding, our MPNN + VN is able to further improve over the previous GCN + VN & GIN + VN implementation. The improvement on ogbg-ppa is particularly impressive, which is from 0.7037 to 0.8055. Furthermore, it is important to note that while MPNN + VN does not necessarily outperform GraphGPS, which is a state-of-the-art architecture using both MPNN, Position/structure encoding and Transformer, the difference is quite small – this however, is achieved by a simple MPNN + VN architecture. We also test MPNN + VN on large-scale molecule datasets PCQMv2, which has 529,434 molecule graphs. We fol- lowed (Rampášek et al., 2022) and used the original vali- dation set as the test set, while we left out random 150K molecules for our validation set. As we can see from Table 4, MPNN + VN + NoPE performs significantly better than the early MPNN + VN implementation: GIN + VN and GCN + VN. The performance gap between GPS on the other hand is rather small: 0.0938 (GPS) vs. 0.0942 (MPNN + VN + PE) for the small model and 0.0858 (GPS) vs. 0.0867 (MPNN + VN + PE) for the medium model. 7.3. Forecasting Sea Surface Temperature In this experiment, we apply our MPNN + VN model to forecast sea surface temperature (SST). We are particularly interested in the empirical comparison between MPNN + VN and Linear Transformer (Katharopoulos et al., 2020a) as according to Section 4, MPNN + VN theoretically can approximate Linear Transformer. In particular, from the DOISST data proposed by (Huang et al., 2021), we construct a dataset of daily SST in the Pacific Ocean from 1982 to 2021, in the region of lon- gitudes from 180.125◦E to 269.875◦E and latitudes from −14.875◦N to 14.875◦N. Following the procedure from (de Bezenac et al., 2018; de Bézenac et al., 2019) and Wang et al. (2022), we divide the region into 11 batches of equal size with 30 longitudes and 30 latitudes at 0.5◦-degree reso- lution, that can be represented as a graph of 900 nodes. The tasks are to predict the next 4 weeks, 2 weeks and 1 week of SST at each location, given 6 weeks of historical data. We train on data from years 1982–2018, validate on data from 2019 and test on data from 2020–2021. The number of training, validation, and testing examples are roughly 150K, 3K, and 7K. See details of dataset construction, model ar- chitectures, and training scheme in Appendix D.5. We compare our model to other baselines including TF- Net (Wang et al., 2020a), a SOTA method for spatiotempo- ral forecasting, Linear Transformer (Katharopoulos et al., 2020a; Wang et al., 2020b) with Laplacian positional en- coding (LapPE), and Multilayer Perceptron (MLP). We use Mean Square Error (MSE) as the metric and report the er- rors on the test set, shown in the Table 5. We observe that the virtual node (VN) alone improves upon MPNN by3.8%, 6.6% and 4.5% in 4-, 2- and 1-week settings, respectively. 8On the Connection Between MPNN and Graph Transformer Table 5: Results of SST prediction. Model 4 weeks 2 weeks 1 week MLP 0.3302 0.2710 0.2121 TF-Net 0.2833 0.2036 0.1462 Linear Transformer + LapPE 0.2818 0.2191 0.1610 MPNN 0.2917 0.2281 0.1613 MPNN + VN 0.2806 0.2130 0.1540 Furthermore, aligned with our theory in Section 4, MPNN + VN indeed achieves comparable results with Linear Trans- former and outperforms it by a margin of 0.4%, 2.8% and 4.3% in 4-, 2- and 1-week settings, respectively. 8. Concluding Remarks In this paper, we study the expressive power of MPNN + VN under the lens of GT. If we target the self-attention layer in Performer and Linear Transformer, one only needs O(1)-depth O(1) width for arbitrary approximation error. For self-attention in full transformer, we prove that hetero- geneous MPNN + VN of either O(1) depth O(nd) width or O(n) depth O(1) width (under some assumptions) can ap- proximate 1 self-attention layer arbitrarily well. Compared to early results (Kim et al., 2022) showing GT can approx- imate MPNN, our theoretical result draws the connection from the inverse direction. On the empirical side, we demonstrate that MPNN + VN remains a surprisingly strong baseline. Despite recent ef- forts, we still lack good benchmark datasets where GT can outperform MPNN by a large margin. Understanding the inductive bias of MPNN and GT remains challenging. For example, can we mathematically characterize tasks that re- quire effective long-range interaction modeling, and provide a theoretical justification for using GT over MPNN (or vice versa) for certain classes of functions on the space of graphs? We believe making processes towards answering such ques- tions is an important future direction for the graph learning community. Acknowledgement This work was supported in part by the U.S. Department Of Energy, Office of Science, U. S. Army Research Office under Grant W911NF-20-1-0334, Google Faculty Award, Amazon Research Award, a Qualcomm gift fund, and NSF Grants #2134274, #2107256, #2134178, CCF-2217033, and CCF-2112665. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. arXiv preprint arXiv:2006.05205, 2020. Battaglia, P. W., Hamrick, J. B., Bapst, V ., Sanchez- Gonzalez, A., Zambaldi, V ., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., et al. Rela- tional inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. arXiv preprint arXiv:2006.13318, 2020. Chen, D., O’Bray, L., and Borgwardt, K. Structure-aware transformer for graph representation learning. In Interna- tional Conference on Machine Learning, pp. 3469–3489. PMLR, 2022. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Cybenko, G. Approximation by superpositions of a sig- moidal function. Mathematics of control, signals and systems, 2(4):303–314, 1989. de Bezenac, E., Pajot, A., and Gallinari, P. Deep learn- ing for physical processes: Incorporating prior scientific knowledge. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=By4HsfWAZ. de Bézenac, E., Pajot, A., and Gallinari, P. Deep learn- ing for physical processes: incorporating prior scien- tific knowledge. Journal of Statistical Mechanics: The- ory and Experiment, 2019(12):124009, dec 2019. doi: 10.1088/1742-5468/ab3195. URL https://dx.doi. org/10.1088/1742-5468/ab3195. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. Dwivedi, V . P. and Bresson, X. A generalization of transformer networks to graphs. arXiv preprint arXiv:2012.09699, 2020. 9On the Connection Between MPNN and Graph Transformer Dwivedi, V . P., Rampášek, L., Galkin, M., Parviz, A., Wolf, G., Luu, A. T., and Beaini, D. Long range graph bench- mark. arXiv preprint arXiv:2206.08164, 2022. d’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biroli, G., and Sagun, L. Convit: Improving vision trans- formers with soft convolutional inductive biases. In In- ternational Conference on Machine Learning, pp. 2286– 2296. PMLR, 2021. Geisler, S., Li, Y ., Mankowitz, D., Cemgil, A. T., Günne- mann, S., and Paduraru, C. Transformers meet directed graphs. arXiv preprint arXiv:2302.00049, 2023. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017. Han, K., Wang, Y ., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y ., Xiao, A., Xu, C., Xu, Y ., et al. A survey on vision transformer. IEEE transactions on pattern analysis and machine intelligence, 2022. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020. Hu, W., Fey, M., Ren, H., Nakata, M., Dong, Y ., and Leskovec, J. Ogb-lsc: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021. Huang, B., Liu, C., Banzon, V ., Freeman, E., Gra- ham, G., Hankins, B., Smith, T., and Zhang, H.-M. Improvements of the daily optimum inter- polation sea surface temperature (doisst) version 2.1. Journal of Climate , 34(8):2923 – 2939, 2021. doi: 10.1175/JCLI-D-20-0166.1. URL https: //journals.ametsoc.org/view/journals/ clim/34/8/JCLI-D-20-0166.1.xml . Hussain, M. S., Zaki, M. J., and Subramanian, D. Global self-attention as a replacement for graph convolution. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 655–665, 2022. Hwang, E., Thost, V ., Dasgupta, S. S., and Ma, T. An analysis of virtual nodes in graph neural networks for link prediction. In Learning on Graphs Conference, 2022. Kalyan, K. S., Rajasekharan, A., and Sangeetha, S. Am- mus: A survey of transformer-based pretrained mod- els in natural language processing. arXiv preprint arXiv:2108.05542, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the International Conference on Machine Learning (ICML), 2020a. URL https://arxiv.org/abs/2006.16236. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020b. Kim, J., Nguyen, T. D., Min, S., Cho, S., Lee, M., Lee, H., and Hong, S. Pure transformers are powerful graph learners. arXiv preprint arXiv:2207.02505, 2022. Kingma, D. and Ba, J. Adam: A method for stochastic optimization. International Conference on Learning Rep- resentations, 12 2014. Kreuzer, D., Beaini, D., Hamilton, W., Létourneau, V ., and Tossou, P. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34:21618–21629, 2021. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI conference on artificial intelligence, 2018. Lim, D., Robinson, J., Zhao, L., Smidt, T., Sra, S., Maron, H., and Jegelka, S. Sign and basis invariant networks for spectral graph representation learning. arXiv preprint arXiv:2202.13013, 2022. Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021. Maron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y . Invariant and equivariant graph networks. arXiv preprint arXiv:1812.09902, 2018. Mialon, G., Chen, D., Selosse, M., and Mairal, J. Graphit: Encoding graph structure in transformers. arXiv preprint arXiv:2106.05667, 2021. Müller, L., Galkin, M., Morris, C., and Rampášek, L. Attending to graph transformers. arXiv preprint arXiv:2302.04181, 2023. Murphy, R. L., Srinivasan, B., Rao, V ., and Ribeiro, B. Janossy pooling: Learning deep permutation- invariant functions for variable-size inputs.arXiv preprint arXiv:1811.01900, 2018. 10On the Connection Between MPNN and Graph Transformer Oono, K. and Suzuki, T. Graph neural networks exponen- tially lose expressive power for node classification. arXiv preprint arXiv:1905.10947, 2019. Park, W., Chang, W.-G., Lee, D., Kim, J., et al. Grpe: Relative positional encoding for graph transformer. In ICLR2022 Machine Learning for Drug Discovery, 2022. Qi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet: Deep learning on point sets for 3d classification and segmenta- tion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652–660, 2017. Rampášek, L., Galkin, M., Dwivedi, V . P., Luu, A. T., Wolf, G., and Beaini, D. Recipe for a general, powerful, scal- able graph transformer. arXiv preprint arXiv:2205.12454, 2022. Reynolds, R. W., Smith, T. M., Liu, C., Chelton, D. B., Casey, K. S., and Schlax, M. G. Daily high-resolution blended analyses for sea surface temperature. J. Climate, 20:5473–5496, 2007. Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap, T. A simple neural network module for relational reasoning.Advances in neural information processing systems, 30, 2017. Segol, N. and Lipman, Y . On universal equivariant set networks. arXiv preprint arXiv:1910.02421, 2019. Shi, Y ., Zheng, S., Ke, G., Shen, Y ., You, J., He, J., Luo, S., Liu, C., He, D., and Liu, T.-Y . Benchmarking graphormer on large-scale molecular modeling datasets. arXiv preprint arXiv:2203.04810, 2022. Tay, Y ., Dehghani, M., Bahri, D., and Metzler, D. Effi- cient transformers: A survey. ACM Computing Surveys (CSUR), 2020. Topping, J., Di Giovanni, F., Chamberlain, B. P., Dong, X., and Bronstein, M. M. Understanding over-squashing and bottlenecks on graphs via curvature. arXiv preprint arXiv:2111.14522, 2021. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At- tention is all you need. Advances in neural information processing systems, 30, 2017. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks.arXiv preprint arXiv:1710.10903, 2017. Wagstaff, E., Fuchs, F. B., Engelcke, M., Osborne, M. A., and Posner, I. Universal approximation of functions on sets. Journal of Machine Learning Research, 23(151): 1–56, 2022. Wang, R., Kashinath, K., Mustafa, M., Albert, A., and Yu, R. Towards physics-informed deep learning for turbulent flow prediction. pp. 1457–1466, 08 2020a. doi: 10.1145/ 3394486.3403198. Wang, R., Walters, R., and Yu, R. Meta-learning dynamics forecasting using task inference. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=BsSP7pZGFQO. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity, 2020b. Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on em- pirical methods in natural language processing: system demonstrations, pp. 38–45, 2020. Wu, Q., Zhao, W., Li, Z., Wipf, D., and Yan, J. Nodeformer: A scalable graph structure learning transformer for node classification. In Advances in Neural Information Pro- cessing Systems, 2022. Wu, Z., Jain, P., Wright, M., Mirhoseini, A., Gonzalez, J. E., and Stoica, I. Representing long-range context for graph neural networks with global attention. Advances in Neural Information Processing Systems , 34:13266– 13279, 2021. Yang, C., Wang, R., Yao, S., Liu, S., and Abdelzaher, T. Revisiting over-smoothing in deep gcns. arXiv preprint arXiv:2003.13663, 2020. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y ., and Liu, T.-Y . Do transformers really perform badly for graph representation? Advances in Neural Information Processing Systems, 34:28877–28888, 2021. Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. Deep sets. Ad- vances in neural information processing systems , 30, 2017. Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint arXiv:1909.12223, 2019. Zweig, A. and Bruna, J. Exponential separations in symmet- ric neural networks. arXiv preprint arXiv:2206.01266, 2022. 11On the Connection Between MPNN and Graph Transformer A. Notations We provide a notation table for references. Table 6: Summary of important notations. Symbol Meaning X ∈ X ⊂Rn×d graph node features xi ∈ R1×d graph nodei’s feature ˜xi ∈ R1×d approximated graph nodei’s feature via attention selection M A multiset of vectors inRd W(l) Q ,W(l) K ,W(l) V ∈ Rd×d′ attention matrix ofl-th self-attention layer in graph transformer X feature space Xi projection of feature space ontoi-th coordinate Lds i i-th linear permutation equivariant layer in DeepSets L,L′ full self attention layer; approximate self attention layer in Performer z(l) vn ,z(l) i virtual/graph node feature at layerl of heterogeneous MPNN + VN αvn attention score in MPNN + VN α(·,·) normalized attention score αGATv2(·,·) normalized attention score with GATv2 α′(·,·) unnormalized attention score.α′(u,v) =uWQ(WK)TvT α′ GATv2(·,·) unnormalized attention score with GATv2.α′ GATv2(u,v) :=aT LeakyReLU (W ·[u∥v] +b) A space of attentions, where each elementα ∈ Ais of formα(u,v) =softmax(uWQ(WK)TvT) C1 upper bound on norm of all node features∥xi∥ C2 upper bound on the norm ofWQ,WK,WV in targetL C3 upper bound on the norm of attention weights ofαvn when selectingxi γ(k)(·,·) update function θ(k)(·,·) message function τ(·) aggregation function B. O(n) Heterogeneous MPNN + VN Layer with O(1) Width Can Approximate 1 Self Attention Layer Arbitrarily Well B.1. Assumptions A special case of (V , δ) separable is when δ = 0 , i.e., ∀i, ¯α(xi, vi) > maxj̸=i ¯α(xj, vi). We provide a geometric characterization of X being (V , 0) separable. Lemma B.1. Given ¯α and V , X is (V , 0) separable by ¯α ⇐⇒xi is not in the convex hull spanned by {xj}j̸=i. ⇐⇒there are no points in the convex hull of {xi}i∈[n]. Proof. The second equivalence is trivial so we only prove the first equivalence. By definition, X is (V , 0) separable by ¯α ⇐⇒ ¯α(xi, vi) > maxj̸=i ¯α(xj, vi)∀i ∈ [n] ⇐⇒ ⟨xi, W ¯α QW ¯α,T K vi⟩ > maxj̸=i⟨xj, W ¯α QW ¯α,T K vi⟩∀i ∈ [n]. By denoting the v′ i := W ¯α QW ¯α,T K vi ∈ Rd, we know that ⟨xi, v′ i⟩ > maxj̸=i⟨xj, v′ i⟩∀i ∈ [n], which implies that ∀i ∈ [n], xi can be linearly seprated from {xj}j̸=i ⇐⇒xi is not in the convex hull spanned by {xj}j̸=i, which concludes the proof. Lemma B.2 (approximate hard selection). Given X is (V , δ) separable by ¯α for some fixed V ∈ Rn×d, ¯α ∈ Aand δ >0, the following holds. For any ϵ >0 and i ∈ [n], there exists a set of attention weights Wi,Q, Wi,K in i-th layer of MPNN + VN such that αvn(xi, vi) > 1 − ϵ for any xi ∈ Xi. In other words, we can approximate a hard selection function fi(x1, ...,xn) = xi arbitrarily well on X by setting αvn = ¯α. Proof. Denote ¯α′ as the unnormalized ¯α. As X is (V , δ) separable by ¯α, by definition we know that ¯α(xi, vi) > maxj̸=i ¯α(xj, vi) + δ holds for any i ∈ [n] and xi ∈ M. We can amplify this by multiple the weight matrix in ¯α by a 12On the Connection Between MPNN and Graph Transformer constant factor c to make ¯α′(xi, vi) > maxj̸=i ¯α′(xj, vi) + cδ. This implies that e¯α′(xi,vi) > ecδ maxj̸=i e¯α′(xj,vi). This means after softmax, the attention score ¯α(xi, vi) will be at least ecδ ecδ+n−1 . We can pick a large enough c(δ, ϵ) such that ¯α(xi, vi) > 1 − ϵ for any xi ∈ Xi and ϵ >0. Proof Intuition and Outline. On the high level, i-th MPNN + VN layer will select ˜xi, an approximation i-th node feature xi via attention mechanism, enabled by Lemma 6.2, and send ˜xi to the virtual node. Virtual node will then pass the ˜xi to all graph nodes and computes the approximation of eα(xi,xj), ∀j ∈ [n]. Repeat such procedures n times for all graph nodes, and finally, use the last layer for attention normalization. The main challenge of the proof is to 1) come up with message/update/aggregation functions for heterogeneous MPNN + VN layer, which is shown in Appendix B.2, and 2) ensure the approximation error, both from approximating Aggre- gate/Message/Update function with MLP and the noisy input, can be well controlled, which is proved in Appendix B.4. We will first instantiate the Aggregate/Message/Update function for virtual/graph nodes in Appendix B.2, and prove that each component can be either exactly computed or approximated to an arbitrary degree by MLP. Then we go through an example in Appendix B.3 of approximate self-attention layer L with O(n) MPNN + VN layers. The main proof is presented in Appendix B.4, where we show that the approximation error introduced during different steps is well controlled. Lastly, in Appendix B.5 we show assumption on node features can be relaxed if a more powerful attention mechanism GATv2 (Brody et al., 2021) is allowed in MPNN + VN. B.2. Aggregate/Message/Update Functions Let M be a multiset of vectors in Rd. The specific form of Aggregate/Message/Update for virtual and graph nodes are listed below. Note that ideal forms will be implemented as MLP, which will incur an approximation error that can be controlled to an arbitrary degree. We use z(k) vn denotes the virtual node’s feature at l-th layer, and z(k) i denotes the graph node i’s node feature. Iteration index k starts with 0 and the node index starts with 1. B.2.1. VIRTUAL NODE At k-th iteration, virtual node i’s feature z(k) i is a concatenation of three component [˜xi, vk+1, 0] where the first component is the approximately selected node features xi ∈ Rd, the second component is the vi ∈ Rd that is used to select the node feature in i-th iteration. The last component is just a placeholder to ensure the dimension of the virtual node and graph node are the same. It is introduced to simplify notation. Initial feature is z(0) vn = [0d, v1, 0]. Message function + Aggregation function τj∈[n]ϕ(k) vn-gn : R2d+1 × M →R2d+1 has two cases to discuss depending on value of k. For k = 1, 2, ..., n, τj∈[n]ϕ(k) vn-gn(z(k−1) vn , {z(k−1) i }i) = (P i αvn(z(k−1) vn , z(k−1) i )z(k−1) i k = 1, 2, ..., n 12d+1 k = n + 1, n+ 2 (7) where z(k−1) vn = [˜xk−1, vk, 0]. z(k−1) i = [ 2d+1 dimz }| {xi|{z} d dim , ..., ...] is the node i’s feature, where the first d coordinates remain fixed for different iteration k. τj∈[n]ϕ(k) vn-gn use attention αvn to approximately select k-th node feature [ 2d+1 dimz }| {xk|{z} d dim , ..., ...]. Note that the particular form of attention αvn needed for soft selection is not important as long as we can approximate hard selection arbitrarily well. As the z(k−1) vn contains vk and z(k−1) i contains xi (see definition of graph node feature in Appendix B.2.2), this step can be made as close to hard selection as possible, according to Lemma B.6. In the case of k = n + 1, τj∈[n]ϕ(k) vn-gn : R2d+1 | {z } vn × M|{z} set of gn → Rd simply returns 12d+1. This can be exactly implemented by an MLP. 13On the Connection Between MPNN and Graph Transformer Update function γ(k) vn : R2d+1 | {z } vn ×R2d+1 | {z } gn → R2d+1: Given the virtual node’s feature in the last iteration, and the selected feature in virtual node y = [xk, ..., ...] with αvn, γ(k) vn (·, y) =    [y0:d, vk+1, 0] k = 1, ..., n− 1 [y0:d, 0d, 0] k = n 12d+1 k = n + 1, n+ 2 (8) where y0:d denotes the first d channels of y ∈ R2d+1. y denotes the selected node zi’s feature in Message/Aggregation function. γ(k) vn can be exactly implemented by an MLP for any k = 1, ..., n+ 2. B.2.2. G RAPH NODE Graph node i’s feature vi ∈ R2d+1 can be thought of as a concatenation of three components [ xi|{z} d dim , tmp|{z} d dim , partialsum| {z } 1 dim ], where xi, ∈ Rd, tmp ∈ Rd 3, and partialsum ∈ R. In particular, xi is the initial node feature. The first d channel will stay the same until the layer n + 2. tmp =P j∈subset of[n] eα′ ij xj stands for the unnormalized attention contribution up to the current iteration. partialsum ∈ R is a partial sum of the unnormalized attention score, which will be used for normalization in the n + 2-th iteration. Initial feature z(0) gn = [xi, 0d, 0]. Message function + Aggregate function: τj∈[n]ϕ(k) gn-vn : R2d+1 × R2d+1 → R2d+1 is just “copying the second argument” since there is just one incoming message from the virtual node, i.e., τj∈[n]ϕ(k) gn-vn(x, {y}) = y. This function can be exactly implemented by an MLP. Update function γ(k) gn : R2d+1 | {z } gn ×R2d+1 | {z } vn → R2d+1 is of the following form. γ(k) gn ([x, tmp, partialsum], y) =    [x, tmp, partialsum] k = 1 [x, tmp + eα′(x,y0:d)WV y0:d, partialsum + eα′(x,y0:d)] k = 2, ..., n+ 1 [ tmp partialsum , 0d, 0] k = n + 2 (9) where α′(x, y0:d) is the usual unnormalized attention score. Update function γ(k) gn can be arbitrarily approximated by an MLP, which is proved below. Lemma B.3. Update function γ(k) gn can be arbitrarily approximated by an MLP from R2d+1 × R2d+1 to R2d+1 for all k = 1, ..., n+ 2. Proof. We will show that for anyk = 1, ..., n+ 2, the target function γ(k) gn : R2d+1 × R2d+1 → R2d+1 is continuous and the domain is compact. By the universality of MLP in approximating continuous function on the compact domain, we know γ(k) gn can be approximated to arbitrary precision by an MLP. Recall that γ(k) gn ([x, tmp, partialsum], y) =    [x, tmp, partialsum] k = 1 [x, tmp + eα′(x,y0:d)WV y0:d, partialsum + eα′(x,y0:d)] k = 2, ..., n+ 1 [ tmp partialsum , 0d, 0] k = n + 2 3tmp technicially denotes the dimension of projected feature by WV and does not has to be in Rd. We use Rd here to reduce the notation clutter. 14On the Connection Between MPNN and Graph Transformer it is easy to see that k = 1, γ(1) gn is continuous. We next show for k = 2, ..., n+ 2, γ(1) gn is also continuous and all arguments lie in a compact domain. γ(k) gn is continuous because to a) α′(x, y) is continuous b) scalar-vector multiplication, sum, and exponential are all continuous. Next, we show that four component x, tmp, partialsum, y0:d all lies in a compact domain. x is the initial node features, and by AS1 their norm is bounded so x is in a compact domain. tmp is an approximation of eα′ i,1WV x1 + eα′ i,2WV x2 + .... As α′(xi, xj) is both upper and lower bounded by AS2 for all i, j∈ [n] and xi is bounded by AS1, eα′ i,1WV x1 + eα′ i,2WV x2 + ... is also bounded from below and above. tmp will also be bounded as we can control the error to any precision. partialsum is an approximation of eα′ i,1 + eα′ i,2 + .... For the same reason as the case above, partialsum is also bounded both below and above. y0:d will be ˜xi at i-th iteration so it will also be bounded by AS1. Therefore we conclude the proof. B.3. A Running Example We provide an example to illustrate how node features are updated in each iteration. Time 0: All nodes are initialized as indicated in Appendix B.2. Virtual node feature z(0) vn = [0d, v1, 0]. Graph node feature z(0) i = [xi, 0d, 0] for all i ∈ [n]. Time 1: For virtual node, according to the definition of τj∈[n]ϕ(1) vn-gn in Equation (7), it will pick an approximation of x1, i.e. ˜x1. Note that the approximation error can be made arbitrarily small. VN’s node feature z(1) vn = [˜x1, v2, 0]. For i-th graph node feature, z(0) vn = 1d, and z(0) i = [xi, 0d, 0]. According to γ(k) gn in Equation (9), z(1) i = [xi, 0d, 0]. Time 2: For the virtual node feature: similar to the analysis in time 1, VN’s feature z(2) vn = [˜x2, v3, 0] now. Note that the weights and bias in τj∈[n]ϕ(2) vn-gn will be different from those in τj∈[n]ϕ(1) vn-gn. For i-th graph node feature, as z(1) vn = [ ˜x1, v2, 0] and z(1) i = [ xi, 0d, 0], according to γ(k) gn in Equation (9), z(2) i = [xi, e gα′ i,1WV ˜x1, e gα′ i,1]. Here gα′ i,1 := α′(xi, ˜x1). We will use similar notations in later iterations. 4 Time 3: Similar to the analysis above, z(3) vn = [fx3, v4, 0]. z(3) i = [xi, e gα′ i,1WV ˜x1 + e gα′ i,2WV ˜x2, e gα′ i,1 + e gα′ i,2]. Time n: z(n) vn = [˜xn, 0d, 0]. z(n) i = xi, e gα′ i,1WV ˜x1 + ... + e ^α′ i,n−1WV ]xn−1| {z } n−1 terms , e gα′ i,1 + e gα′ i,2 + ... + e ^α′ i,n−1]| {z } n−1 terms . Time n + 1: 4To reduce the notation clutter and provide an intuition of the proof, we omit the approximation error introduced by using MLP to approximate aggregation/message/update function, and assume the aggregation/message/update can be exactly implemented by neural networks. In the proofs, approximation error by MLP is handled rigorously. 15On the Connection Between MPNN and Graph Transformer According to Appendix B.2.1, in n + 1 iteration, the virtual node’s feature will be 1d. z(n+1) i = [xi, P k∈[n] e gα′ ik WV ˜xk, P k∈[n] e gα′ ik ] Time n + 2 (final layer): For the virtual node, its node feature will stay the same. For the graph node feature, the last layer will serve as a normalization of the attention score (use MLP to approximate vector- scalar multiplication), and set the last channel to be 0 (projection), resulting in an approximation of[xi, P k∈[n] e gα′ ik WV ˜xk P k∈[n] e gα′ ik , 0]. Finally, we need one more linear transformation to make the node feature become [ P k∈[n] e gα′ ik WV ˜xk P k∈[n] e gα′ ik , 0d, 0]. The first d channel is an approximation of the output of the self-attention layer for node i where the approximation error can be made as small as possible. This is proved in Appendix B, and we conclude that heterogeneous MPNN + VN can approximate the self-attention layer L to arbitrary precision with O(n) MPNN layers. B.4. Controlling Error On the high level, there are three major sources of approximation error: 1) approximate hard selection with self-attention and 2) approximate equation γ(k) gn with MLPs, and 3) attention normalization in the last layer. In all cases, we aim to approximate the output of a continuous map Lc(x). However, our input is usually not exact x but an approximation of ˜x. We also cannot access the original map Lc but instead, an MLP approximation of Lc, denoted as LMLP. The following lemma allows to control the difference between Lc(x) and LMLP(˜x). Lemma B.4. Let Lc be a continuous map from compact set to compact set in Euclidean space. Let LMLP be the approximation of Lc by MLP . If we can control∥x − ˜x∥ to an arbitrarily small degree, we can then control the error ∥Lc(x) − LMLP(˜x)∥ arbitrarily small. Proof. By triangle inequality ∥Lc(x) − LMLP(˜x)∥ ≤ ∥Lc(x) − LMLP(x))∥ + ∥LMLP(x) − LMLP(˜x)∥. For the first term ∥Lc(˜x)−LMLP(˜x)∥, by the universality of MLP, we can control the error∥Lc(˜x)−LMLP(˜x)∥ in arbitrary degree. For the second term ∥LMLP(x) − LMLP(˜x)∥, as LMLP is continuous on a compact domain, it is uniformly continuous by Heine-Cantor theorem. This means that we can control the ∥LMLP(x) − LMLP(˜x)∥ as long as we can control ∥x − ˜x∥, independent from different x. By assumption, this is indeed the case so we conclude the proof. Remark B.5. The implication is that when we are trying to approximate the output of a continuous map Lc on the compact domain by an MLP LMLP, it suffices to show the input is 1) ∥Lc − LMLP∥∞ and 2) ∥˜x − x∥ can be made arbitrarily small. The first point is usually done by the universality of MLP on the compact domain (Cybenko, 1989). The second point needs to be shown case by case. In the Appendix B.3, to simplify the notations we omit the error introduced by using MLP to approximate aggrega- tion/message/update functions (continuous functions on the compact domain of Rd.) in MPNN + VN. Lemma B.4 justify such reasoning. Lemma B.6 (˜xi approximates xi. gα′ i,j approximates α′ i,j.). For any ϵ >0 and x ∈ X, there exist a set of weights for message/aggregate functions of the virtual node such that||xi − ˜xi|| < ϵand |α′ i,j − gα′ i,j| < ϵ. Proof. By Lemma 6.2 We know that gαi,j := eα(xi, xj) → δ(i − j) as C3(ϵ) goes to infinity. Therefore we have ||˜xi − xi|| = || X j gαi,jxj − xi|| = || X (eαi,j − δ(i − j))xj|| < ϵ X ||xj|| < nC1ϵ (10) As n and C1 are fixed, we can make the upper bound as small as we want by increasing C3. |α′ i,j−gα′ i,j| = |α′(xi, xj)−α′ MLP(˜xi, xj)| = |α′(xi, xj)−α′(˜xi, xj)|+|α′(˜xi, xj)−α′ MLP(˜xi, xj)| = |α′(xi−˜xi, xj)| = (xi − ˜xi)T xjC2 2 + ϵ < nC1ϵC1C2 2 + ϵ = (nC2 1 C2 2 + 1)ϵ. As α′ i,j, gα′ i,j is bounded from above and below, it’s easy to see 16On the Connection Between MPNN and Graph Transformer that |eα′ i,j − e gα′ i,j | = |eα′ i,j (1 − eα′ i,j−gα′ i,j )| < C(1 − eα′ i,j−gα′ i,j ) can be controlled to arbitrarily degree. Theorem 6.3. Assume AS 1-3 hold for the compact set X and L. Given any graph G of size n with node features X ∈ X, and a self-attention layer L on G (fix WK, WQ, WV in α), there exists a O(n) layer of heterogeneous MPNN + VN with the specific aggregate/update/message function that can approximateL on X arbitrarily well. Proof. i-th MPNN + VN layer will select ˜xi, an arbitrary approximation i-th node feature xi via attention mechanism. This is detailed in the message/aggregation function of the virtual node in Appendix B.2.1. Assuming the regularity condition on feature space X, detailed in AS3, the approximation error can be made as small as needed, as shown in Lemmas 6.2 and B.6. Virtual node will then pass the ˜xi to all graph nodes, which computes an approximation of eα′(˜xi,xj), ∀j ∈ [n]. This step is detailed in the update function γ(k) gn of graph nodes, which can also be approximated arbitrarily well by MLP, proved in Lemma B.3. By Lemma B.4, we have an arbitrary approximation of eα′(˜xi,xj), ∀j ∈ [n], which itself is an arbitrary approximation of eα′(xi,xj), ∀j ∈ [n]. Repeat such procedures n times for all graph nodes, we have an arbitrary approximation of P k∈[n] eα′ ik WV xk ∈ Rd andP k∈[n] eα′ ik ∈ R. Finally, we use the last layer to approximate attention normalizationLc(x, y) = x y , where x ∈ Rd, y∈ R. As inputs for attention normalization are arbitrary approximation of P k∈[n] eα′ ik WV xk and P k∈[n] eα′ ik , both of them are lower/upper bounded according to AS1 and AS2. Since the denominator is upper bounded by a positive number, this implies that the target function Lc is continuous in both arguments. By evoking Lemma B.4 again, we conclude that we can approximate its output P k∈[n] eα′ ik WV xk P k∈[n] eα′ ik arbitrarily well. This concludes the proof. B.5. Relaxing Assumptions with More Powerful Attention One limitation of Theorem 6.3 are assumptions on node features space X: we need to 1) restrict the variability of node feature so that we can select one node feature to process each iteration. 2) The space of the node feature also need to satisfy certain configuration in order for VN to select it. For 2), we now consider a different attention function for αvn in MPNN + VN that can relax the assumptions AS3 on X. More powerful attention mechanism. From proof of Theorem 6.3, we just need α(·, ·) uniformly select every node in X ∈ X. The unnormalized bilinear attention α′ is weak in the sense that f(·) = ⟨xiWQWT K, ·⟩ has a linear level set. Such a constraint can be relaxed via an improved attention module GATv2. Observing the ranking of the attention scores given by GAT (Veliˇckovi´c et al., 2017) is unconditioned on the query node, Brody et al. (2021) proposed GATv2, a more expressive attention mechanism. In particular, the unnormalized attention score α′ GATv2(u, v) := aT LeakyReLU (W · [u∥v] + b), where [·||·] is concatenation. We will let αvn = αGATv2 to select features in τj∈[n]ϕ(k) vn-gn. (a) (b) Figure 3: In the left figure, we have one example of X being (V , δ) separable, for which α can uniformly select any point (marked as red) xi ∈ Xi. In the right figure, we change αvn in MPNN + VN to αGATv2, which allows us to select more diverse feature configurations. The cluster in the middle cannot be selected by any α ∈ Abut can be selected by αGATv2 according to Proposition B.9. Lemma B.7. α′ GATv2(·, ·) can approximate any continuous function from Rd × Rd → R. For any v ∈ Rd, a restriction of α′ GATv2(·, v) can approximate any continuous function from Rd → R. Proof. Any function continuous in both arguments of α′ GATv2 is also continuous in the concatenation of both arguments. As 17On the Connection Between MPNN and Graph Transformer any continuous functions in R2d can be approximated by α′ GATv2 on a compact domain according to the universality of MLP (Cybenko, 1989), we finish the proof for the first statement. For the second statement, we can write W as 2 × 2 block matrix and restrict it to cases where only W11 is non-zero. Then we have α′ GATv2(u, v) = aT LeakyReLU \u0012\u0014 W11 W12 W21 W22 \u0015 · \u0014 u v \u0015 + b \u0013 = aT LeakyReLU (W11u + b) (11) which gives us an MLP on the first argument u. By the universality of MLP, we conclude the proof for the second statement. Definition B.8. Given δ >0, We call X is δ nonlinearly separable if and only if mini̸=j d(Xi, Xj) > δ. AS 3’. X is δ nonlinearly separable for some δ >0. Proposition B.9. If X ⊂Rn×d satisfies that Xi is δ-separated from Xj for any i, j∈ [n], the following holds. For any X ∈ Xand i ∈ [n], there exist a αGATv2 to select any xi ∈ Xi. This implies that we can arbitrarily approximate the self-attention layer L after relaxing AS3 to AS3’. Proof. For any i ∈ [n], as Xi is δ-separated from other Xj, ∀j ̸= i, we can draw a region Ωi ⊂ Rd that contains Xi and separate Xi from other Xj(j ̸= i), where the distance from Xi from other Xj is at least δ according to the definition of Definition B.8. Next, we show how to construct a continuous function f whose value in Xi is at least 1 larger than its values in any other Xj ∀j ̸= i. We set the values of f in Xi to be 1.5 and values of f in Xj, ∀j ̸= i to be 0. We can then interpolate f in areas outside of ∪Xi (one way is to set the values of f(x) based on d(x, Xi), which results in a continuous function that satisfies our requirement. By the universality of αGATv2, we can approximate f to arbitrary precision, and this will let us select any Xi. C. On the Limitation of MPNN + VN Although we showed that in the main paper, MPNN + VN of varying depth/width can approximate the self-attention of full/linear transformers, this does not imply that there is no difference in practice between MPNN + VN and GT. Our theoretical analysis mainly focuses on approximating self-attention without considering computational efficiency. In this section, we mention a few limitations of MPNN + VN compared to GT. C.1. Representation Gap The main limitation of deep MPNN + VN approximating full self-attention is that we require a quite strong assumption: we restrict the variability of node features in order to select one node feature to process each iteration. Such assumption is relaxed by employing stronger attention in MPNN + VN but is still quite strong. For the large width case, the main limitation is the computational complexity: even though the self-attention layer requires O(n2) complexity, to approximate it in wide MPNN + VN framework, the complexity will becomeO(nd) where d is the dimension of node features. We think such limitation shares a similarity with research in universal permutational invariant functions. Both DeepSets (Zaheer et al., 2017) and Relational Network (Santoro et al., 2017) are universal permutational invariant architecture but there is still a representation gap between the two (Zweig & Bruna, 2022). Under the restriction to analytic activation functions, one can construct a symmetric function acting on sets of size n with elements in dimension d, which can be efficiently approximated by the Relational Network, but provably requires width exponential inn and d for the DeepSets. We believe a similar representation gap also exists between GT and MPNN + VN and leave the characterization of functions lying in such gap as the future work. C.2. On The Difficulty of Approximating Other Linear Transformers In Section 4, we showed MPNN + VN of O(1) width and depth can approximate the self-attention layer of one type of linear transformer, Performer. The literature on efficient transformers is vast (Tay et al., 2020) and we do not expect MPNN 18On the Connection Between MPNN and Graph Transformer + VN can approximate many other efficient transformers. Here we sketch a few other linear transformers that are hard to approximate by MPNN + VN of constant depth and width. Linformer (Wang et al., 2020b) projects then×d dimension keys and values tok×d suing additional projection layers, which in graph setting is equivalent to graph coarsening. As MPNN + VN still operates on the original graph, it fundamentally lacks the key component to approximate Linformer. We consider various types of efficient transformers effectively generalize the virtual node trick. By first switching to a more expansive model and reducing the computational complexity later on, efficient transformers effectively explore a larger model design space than MPNN + VN, which always sticks to the linear complexity. C.3. Difficulty of Representing SAN Type Attention In SAN (Kreuzer et al., 2021), different attentions are used conditional on whether an edge is presented in the graph or not, detailed below. One may wonder whether we can approximate such a framework in MPNN + VN. In our proof of using MPNN + VN to approximate regular GT, we mainly work with Definition 3.4 where we do not use any gn-gn edges and therefore not leverage the graph topology. It is straightforward to use gn-gn edges and obtain the different message/update/aggregate functions for gn-gn edges non-gn-gn edges. Although we still achieve the similar goal of SAN to condition on the edge types, it turns out that we can not arbitrarily approximate SAN. Without loss of generality, SAN uses two types of attention depending on whether two nodes are connected by the edge. Specifically, ˆwk,l ij =    Q1,k,lhl i◦K1,k,lhl j◦E1,k,leij√dk if i and j are connected in sparse graph Q2,k,lhl i◦K2,k,lhl j◦E2,k,leij√dk otherwise    wk,l ij =    1 1+γ · softmax \u0010P dk ˆwk,l ij \u0011 if i and j are connected in sparse graph γ 1+γ · softmax \u0010P dk ˆwk,l ij \u0011 otherwise    (12) where ◦ denotes element-wise multiplication and Q1,k,l, Q2,k,l, K1,k,l, K2,k,l, E1,k,l, E2,k,l ∈ Rdk×d. γ ∈ R+is a hyperparameter that tunes the amount of bias towards full-graph attention, allowing flexibility of the model to different datasets and tasks where the necessity to capture long-range dependencies may vary. To reduce the notation clutter, we remove the layer indexl, and edge features, and also consider only one-attention head case (remove attention index k). The equation is then simplified to ˆwij =    Q1hl i◦K1hl j√dk if i and j are connected in sparse graph Q2hl i◦K2hl j√dk otherwise    wij = \u001a 1 1+γ · softmax (P d ˆwij) if i and j are connected in sparse graph γ 1+γ · softmax (P d ˆwij) otherwise \u001b (13) We will then show that Equation (13) can not be expressed (up to an arbitrary approximation error) in MPNN + VN framework. To simulate SAN type attention, our MPNN + VN framework will have to first simulate one type of attention for all edges, as we did in the main paper, and then simulate the second type of attention between gn-gn edges by properly offset the contribution from the first attention. This seems impossible (although we do not have rigorous proof) as we cannot express the difference between two attention in the new attention mechanism. D. Experimental Details D.1. Dataset Description ogbg-molhiv and ogbg-molpcba (Hu et al., 2020) are molecular property prediction datasets adopted by OGB from MoleculeNet. These datasets use a common node (atom) and edge (bond) featurization that represent chemophysical properties. The prediction task of ogbg-molhiv is a binary classification of molecule’s fitness to inhibit HIV replication. The ogbg-molpcba, derived from PubChem BioAssay, targets to predict the results of 128 bioassays in the multi-task binary classification setting. 19On the Connection Between MPNN and Graph Transformer ogbg-ppa (Wu et al., 2021) consists of protein-protein association (PPA) networks derived from 1581 species categorized into 37 taxonomic groups. Nodes represent proteins and edges encode the normalized level of 7 different associations between two proteins. The task is to classify which of the 37 groups does a PPA network originate from. ogbg-code2 (Wu et al., 2021) consists of abstract syntax trees (ASTs) derived from the source code of functions written in Python. The task is to predict the first 5 subtokens of the original function’s name. OGB-LSC PCQM4Mv2 (Hu et al., 2021) is a large-scale molecular dataset that shares the same featurization as ogbg-mol* datasets. It consists of 529,434 molecule graphs. The task is to predict the HOMO-LUMO gap, a quantum physical property originally calculated using Density Functional Theory. True labels for original test-dev and test-challange dataset splits are kept private by the OGB-LSC challenge organizers. Therefore for the purpose of this paper, we used the original validation set as the test set, while we left out random 150K molecules for our validation set. D.2. Reproducibility For LRGB results in Section 7.1, we reproduce the original results up to very small differences. Table 7: Reproduce the original results up to small differences. No VN is used. Model # Params. Peptides-func Peptides-struct Test AP (reproduce) Test AP ↑ Test MAE (reproduce) Test MAE↓ GCN 508k 0.5918 ±0.0065 0.5930 ±0.0023 0.3468 ±0.0009 0.3496 ±0.0013 GINE 476k 0.5595 ±0.0126 0.5498 ±0.0079 0.3532 ±0.0024 0.3547 ±0.0045 GatedGCN 509k 0.5886 ±0.0027 0.5864 ±0.0077 0.3409 ±0.0011 0.3420 ±0.0013 GatedGCN+RWSE 506k 0.6083 ±0.0032 0.6069 ±0.0035 0.3377 ±0.0025 0.3357 ±0.0006 D.3. The Role of Graph Topology In our experiments, we considered graph topology in experiments (i.e., message passing operates on both GN-VN (graph node-virtual node) and GN-GN edges). To understand the role of GN-VN and GN-GN edges, we carried out a set of new experiments where we discard the original graph topology, and only do message passing on GN-VN edges, for Peptides-func & Peptides-struct datasets. The results are shown in Appendix D.3. We observe that in general, MPNN + VN using GN-VN edges only perform slightly worse than MPNN + VN using both GN-VN and GN-GN edges. However, it still performs better than the standard MPNN without VN. We believe adding VN as a simple way of long-range modeling is the main reason we see good results on Peptides-func & Peptides-struct datasets. Utilizing local graph topology in MPNN will further improve the performance. In general, combining local (message passing) and global modeling (such as GT and VN) in GNN is an active research direction, with novel applications in macromolecule (DNA, RNA, Protein) modeling. In the recent SOTA model GraphGPS (Rampášek et al., 2022), MPNN is interleaved with GT. Consistent with our findings, Rampášek et al. (2022) also showed both the local component (MPNN) and global component (GT) contribute to the final performance. Table 8: Utilizing local graph topology in MPNN will further improve the performance on Peptides-func and Peptides-struct. Peptides-funcAP↑ Peptides-structMAE↓w/o VN (only graph topology) w/ VN + graph topology Only VNw/o VN (only graph topology) w/ VN + graph topology Only VN GCN 0.5930±0.0023 0 .6623±0.0038 0 .6488±0.0056 0.3496±0.0013 0 .2488±0.0021 0 .2511±0.0025GINE 0.5498±0.0079 0 .6346±0.0071 0 .6022±0.0072 0.3547±0.0045 0 .2584±0.0011 0 .2608±0.0021GatedGCN 0.5864±0.0077 0 .6635±0.0024 0 .6493±0.0044 0.3420±0.0013 0 .2523±0.0016 0 .2684±0.0039GatedGCN+RWSE0.6069±0.0035 0 .6685±0.0062 0 .6432±0.0072 0.3357±0.0006 0 .2529±0.0009 0 .2645±0.0023 D.4. Additional Experiments We tested MPNN + VN on PascalVOC-SP datasets and also observe improvement, shown in Table 9, although the improvement is not as large as that ofPeptides-func and Peptides-struct datasets. The best MPNN + VN model 20On the Connection Between MPNN and Graph Transformer is GatedGCN + LapPE where the performance gap to the best GT model is rather small. Table 9: Baseline experiments for PascalVOC-SP and COCO-SP with rag-boundary graph on SLIC compactness 30 for the node classification task. The performance metric is macro F1 on the respective splits (Higher is better). All experiments are run 4 times with 4 different seeds. The MP-GNN models are 8 layers deep, while the transformer-based models have 4 layers in order to maintain comparable hidden representation size at the fixed parameter budget of 500k. Bold: Best score. Model # Params PascalVOC-SP Before VN + Test F1 After VN + Test F1 ↑ GCN 496k 0.1268 ±0.0060 0.1901 ±0.0040 GINE 505k 0.1265 ±0.0076 0.1198 ±0.0073 GatedGCN 502k 0.2873 ±0.0219 0.2874 ±0.0178 GatedGCN+LapPE 502k 0.2860 ±0.0085 0.3103±0.0068 Transformer+LapPE 501k 0.2694 ±0.0098 - SAN+LapPE 531k 0.3230±0.0039 - SAN+RWSE 468k 0.3216 ±0.0027 - D.5. Predicting Sea Surface Temperature In this experiment, we consider a specific physical modeling problem: forecasting sea surface temperature (SST), that is the water temperature close to the ocean’s surface. SST is an essential climate indicator and plays a significant role in analyzing and monitoring the dynamics of weather, climate, and other biological systems for several applications in environmental protection, agriculture, and industry. We use the NOAA/NESDIS/NCEI Daily Optimum Interpolation Sea Surface Temperature (DOISST) version 2.1 proposed by (Huang et al., 2021) as an improvement upon version 2.0 from (Reynolds et al., 2007). We consider the daily SST data of the Pacific Ocean from 1982 to 2021, in the region of longitudes from 180.125◦E to 269.875◦E and latitudes from −14.875◦N to 14.875◦N. We reduce the resolution of the original data from 0.25◦-degree to 0.5◦-degree. Following the procedure from (de Bezenac et al., 2018), (de Bézenac et al., 2019) and (Wang et al., 2022), we divide the region into 11 square batches of equal size (see Table 11), each contains exactly 30 longitudes and 30 latitudes that can be represented as a grid graph of 900 nodes in which we connect each node to its nearest 8 neighbors. We take time series from 1982 to 2018 as our training set, data in 2019 as our validation set, and data from 2020 to 2021 as our testing set. In our experiments, we set the history window wh as 6 weeks (i.e. 42 days) and the prediction window wp as 4 weeks (i.e. 28 days), 2 weeks (i.e. 14 days) or 1 week (i.e. 7 days). For each example, each node of the graph is associated with an input time series capturing the temperatures at the corresponding (longitude, latitude) for the last wh days, and the task is to predict the output time series of temperatures for the next wp days. We represent each time series as a long vector and the learning task is fundamentally a node-level regression task. We make sure that there is no overlapping among training, validation and testing sets (e.g., the output of a training example will not appear in any input of another validation example). The number of training, validation, and testing examples are roughly 150K, 3K and 7K, respectively for each setting (see Table 10). We compare our MPNN + VN model with: • Multilayer Perceptron (MLP) which treats both the input and output as long vectors and has 512 hidden neurons. • TF-Net (Wang et al., 2020a) with the setting as in the original paper. • Linear Transformer (Katharopoulos et al., 2020a) (Wang et al., 2020b)5 with Laplacian positional encoding (LapPE). We compute the first 16 eigenvectors as positions for LapPE. Both MPNN and MPNN + VN have 3 layers of message passing with 256 hidden dimensions. We apply an MLP with one hidden layer of 512 neurons on top of the network to make the final prediction. 5The Linear Transformer implementation is publicly available at https://github.com/lucidrains/ linear-attention-transformer 21On the Connection Between MPNN and Graph Transformer Table 10: Number of training, validation and testing examples for each setting in the task of SST prediction. History window Prediction window Train size Validation size Test size 6 weeks 4 weeks 147, 884 3 , 245 7 , 271 2 weeks 148, 038 3 , 399 7 , 425 1 week 148, 115 3 , 476 7 , 502 Table 11: These are 11 regions of the Pacific in our experiment. Index Longitudes Latitues 1 [180.125 ◦E, 194.875◦E] [-14.875 ◦N, -0.125◦N] 2 [195.125 ◦E, 209.875◦E] [-14.875 ◦N, -0.125◦N] 3 [210.125 ◦E, 224.875◦E] [-14.875 ◦N, -0.125◦N] 4 [225.125 ◦E, 239.875◦E] [-14.875 ◦N, -0.125◦N] 5 [240.125 ◦E, 254.875◦E] [-14.875 ◦N, -0.125◦N] 6 [255.125 ◦E, 269.875◦E] [-14.875 ◦N, -0.125◦N] 7 [180.125 ◦E, 194.875◦E] [0.125 ◦N, 14.875◦N] 8 [195.125 ◦E, 209.875◦E] [0.125 ◦N, 14.875◦N] 9 [210.125 ◦E, 224.875◦E] [0.125 ◦N, 14.875◦N] 10 [225.125 ◦E, 239.875◦E] [0.125 ◦N, 14.875◦N] 11 [240.125 ◦E, 254.875◦E] [0.125 ◦N, 14.875◦N] We train all our models with 100 epochs with batch size 20, initial learning rate 10−3, and Adam optimizer (Kingma & Ba, 2014). D.6. Connection to Over-Smoothing Phenomenon Over-smoothing refers to the phenomenon that deep GNN will produce same features at different nodes after too many convolution layers. Here we draw some connection between VN and common ways of reducing over-smoothing. We think that using VN can potentially help alleviate the over-smoothing problem. In particular, we note that the use of VN can simulate some strategies people use in practice to address over-smoothing. We give two examples below. Example 1: In (Zhao & Akoglu, 2019), the two-step method (center & scale) PairNorm is proposed to reduce the over- smoothing issues. In particular, PairNorm consists of 1) Center and 2) Scale ˜xc i = ˜xi − 1 n X i ˜xi ˙xi = s · ˜xc iq 1 n P i ||˜xc i ||2 2 Where ˜x is the node features after graph convolution and s is a hyperparameter. The main component for implementing PairNorm is to compute the mean and standard deviation of node features. For the mean of node features, this can be exactly computed in VN. For standard deviation, VN can arbitrarily approximate it using the standard universality result of MLP [5]. If we further assume that the standard deviation is lower bounded by a constant, then MPNN + VN can arbitrarily approximate the PairNorm on the compact set. Example 2: In (Yang et al., 2020) mean subtraction (same as the first step of PairNorm) is also introduced to reduce over-smoothing. As mean subtraction can be trivially implemented in MPNN + VN, arguments in (Yang et al., 2020) (with mean subtraction the revised power Iteration in GCN will lead to the Fiedler vector) can be carried over to MPNN + VN setting. 22On the Connection Between MPNN and Graph Transformer In summary, introducing VN allows MPNN to implement key components of (Yang et al., 2020; Zhao & Akoglu, 2019), we think this is one reason why we observe encouraging empirical performance gain of MPNN + VN. 23",
      "meta_data": {
        "arxiv_id": "2301.11956v4",
        "authors": [
          "Chen Cai",
          "Truong Son Hy",
          "Rose Yu",
          "Yusu Wang"
        ],
        "published_date": "2023-01-27T19:15:31Z",
        "pdf_url": "https://arxiv.org/pdf/2301.11956v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the theoretical connection between Message Passing Neural Networks (MPNNs) and Graph Transformers (GTs), specifically focusing on the expressive power of MPNNs augmented with a virtual node (MPNN+VN). It demonstrates that MPNN+VN is powerful enough to arbitrarily approximate the self-attention layer of GTs. Key findings include: 1) MPNN+VN with O(1) depth and O(1) width can approximate self-attention layers in linear transformers like Performer/Linear Transformer. 2) MPNN+VN with O(1) depth and O(nd) width (where n is nodes, d is feature dimension) can approximate full self-attention layers by simulating equivariant DeepSets, implying permutation equivariant universality. 3) Under certain strong assumptions on node features, MPNN+VN with O(n) depth and O(1) width can approximate a self-attention layer. Empirically, MPNN+VN is shown to be a surprisingly strong baseline, outperforming GT on the Long Range Graph Benchmark (LRGB) dataset, improving over early implementations on OGB datasets, and excelling on a climate modeling task compared to Linear Transformer and MPNN.",
        "methodology": "The methodology involves both theoretical proofs and empirical evaluations. Theoretically, the paper employs approximation theorems for continuous functions by MLPs to show MPNN+VN's capacity. For linear transformers (Performer/Linear Transformer), specific message, aggregation, and update functions are designed for the virtual node to collect global sums and products, enabling O(1) depth and width approximation. For full self-attention, the universality of equivariant DeepSets is leveraged, demonstrating that MPNN+VN can simulate DeepSets layers exactly, thus achieving O(1) depth and O(nd) width universal approximation. For O(n) depth and O(1) width approximation, an explicit construction relies on a 'hard selection' mechanism for node features, enabled by specific attention weights and controlled approximation errors across layers. Empirically, existing MPNN architectures (GCN, GINE, GatedGCN) are augmented with a virtual node, and their performance is evaluated on various graph learning tasks. A stronger MPNN+VN implementation based on GraphGPS's modularity, replacing GlobalAttention with DeepSets, is also developed.",
        "experimental_setup": "The experimental evaluation covered three main tasks and several datasets: 1) Long Range Graph Benchmark (LRGB) datasets, specifically Peptides-func (graph classification with Average Precision) and Peptides-struct (graph regression with Mean Absolute Error), comparing MPNN+VN against various MPNNs and Graph Transformers. 2) OGB graph-level benchmarks (ogbg-molhiv, ogbg-molpcba, ogbg-ppa, ogbg-code2) using metrics like AUROC, Average Precision, Accuracy, and F1 score, where MPNN+VN (with/without positional encoding) was compared against GCN, GIN, SAN, GraphTrans, K-Subtree SAT, and GPS. Additionally, the large-scale PCQM4Mv2 molecular dataset was used, measuring MAE. 3) Forecasting Sea Surface Temperature (SST) in the Pacific Ocean from 1982-2021, where the region was divided into 11 grid graphs of 900 nodes each. The task was to predict 1, 2, or 4 weeks of SST given 6 weeks of historical data, measured by Mean Square Error (MSE). Baselines included MLP, TF-Net, Linear Transformer + LapPE, and standard MPNN. Training involved 100 epochs, batch size 20, initial learning rate 10^-3, and Adam optimizer.",
        "limitations": "The paper highlights several limitations. Theoretically, the O(n) depth O(1) width MPNN+VN approximation of full self-attention requires strong assumptions on node features (specifically, (V, δ) separable features), which limits its applicability to a restrictive family of input graphs. While a more powerful attention mechanism (GATv2) can relax this, the assumption remains substantial. For the O(1) depth O(nd) width case, the computational complexity is worse than directly computing self-attention when the feature dimension d is greater than 2. More generally, the paper acknowledges a potential representation gap between GT and MPNN+VN, similar to that between DeepSets and Relational Networks, which is not fully characterized. Furthermore, MPNN+VN struggles to approximate certain other efficient transformers like Linformer (which relies on graph coarsening, a mechanism not inherent to MPNN+VN) or Sparse Transformer. It also cannot arbitrarily approximate SAN-type attention, which conditions on edge presence.",
        "future_research_directions": "Future research directions include: 1) Alleviating the strong assumptions on node features required for the O(n) depth O(1) width MPNN+VN to approximate full self-attention, to make it applicable to more general families of graphs. 2) Characterizing the representation gap between Graph Transformers and MPNN+VN, similar to how the gap between DeepSets and Relational Networks has been studied. This would involve identifying specific graph functions that one architecture can efficiently compute while the other cannot. 3) Mathematically characterizing tasks that fundamentally require effective long-range interaction modeling, and providing theoretical justifications for when to use GT over MPNN (or vice versa) for certain classes of functions on graphs. 4) A deeper understanding of the inductive biases inherent in both MPNN and GT architectures."
      }
    },
    {
      "title": "Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More",
      "abstract": "The current best practice for computing optimal transport (OT) is via entropy\nregularization and Sinkhorn iterations. This algorithm runs in quadratic time\nas it requires the full pairwise cost matrix, which is prohibitively expensive\nfor large sets of objects. In this work we propose two effective log-linear\ntime approximations of the cost matrix: First, a sparse approximation based on\nlocality-sensitive hashing (LSH) and, second, a Nystr\\\"om approximation with\nLSH-based sparse corrections, which we call locally corrected Nystr\\\"om (LCN).\nThese approximations enable general log-linear time algorithms for\nentropy-regularized OT that perform well even for the complex, high-dimensional\nspaces common in deep learning. We analyse these approximations theoretically\nand evaluate them experimentally both directly and end-to-end as a component\nfor real-world applications. Using our approximations for unsupervised word\nembedding alignment enables us to speed up a state-of-the-art method by a\nfactor of 3 while also improving the accuracy by 3.1 percentage points without\nany additional model changes. For graph distance regression we propose the\ngraph transport network (GTN), which combines graph neural networks (GNNs) with\nenhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales\nlog-linearly in the number of nodes.",
      "full_text": "Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More Johannes Gasteiger 1 Marten Lienen 1 Stephan Günnemann 1 Abstract The current best practice for computing opti- mal transport (OT) is via entropy regularization and Sinkhorn iterations. This algorithm runs in quadratic time as it requires the full pairwise cost matrix, which is prohibitively expensive for large sets of objects. In this work we propose two effective log-linear time approximations of the cost matrix: First, a sparse approximation based on locality-sensitive hashing (LSH) and, sec- ond, a Nyström approximation with LSH-based sparse corrections, which we call locally corrected Nyström (LCN). These approximations enable general log-linear time algorithms for entropy- regularized OT that perform well even for the complex, high-dimensional spaces common in deep learning. We analyse these approximations theoretically and evaluate them experimentally both directly and end-to-end as a component for real-world applications. Using our approxima- tions for unsupervised word embedding alignment enables us to speed up a state-of-the-art method by a factor of 3 while also improving the accuracy by 3.1 percentage points without any additional model changes. For graph distance regression we propose the graph transport network (GTN), which combines graph neural networks (GNNs) with enhanced Sinkhorn. GTN outcompetes pre- vious models by 48 % and still scales log-linearly in the number of nodes. 1. Introduction Measuring the distance between two distributions or sets of objects is a central problem in machine learning. One common method of solving this is optimal transport (OT). OT is concerned with the problem of ﬁnding the transport plan for moving a source distribution (e.g. a pile of earth) to 1Technical University of Munich, Germany. Correspondence to: Johannes Gasteiger <j.gasteiger@in.tum.de>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). a sink distribution (e.g. a construction pit) with the cheapest cost w.r.t. some pointwise cost function (e.g. the Euclidean distance). The advantages of this method have been shown numerous times, e.g. in generative modelling (Arjovsky et al., 2017; Bousquet et al., 2017; Genevay et al., 2018), loss functions (Frogner et al., 2015), set matching (Wang et al., 2019), or domain adaptation (Courty et al., 2017). Motivated by this, many different methods for accelerating OT have been proposed in recent years (Indyk & Thaper, 2003; Papadakis et al., 2014; Backurs et al., 2020). However, most of these approaches are specialized methods that do not generalize to modern deep learning models, which rely on dynamically changing high-dimensional embeddings. In this work we make OT computation for high-dimensional point sets more scalable by introducing two fast and accurate approximations of entropy-regularized optimal transport: Sparse Sinkhorn and LCN-Sinkhorn, the latter relying on our novel locally corrected Nyström (LCN) method. Sparse Sinkhorn uses a sparse cost matrix to leverage the fact that in entropy-regularized OT (also known as the Sinkhorn distance) (Cuturi, 2013) often only each point’s nearest neighbors inﬂuence the result. LCN-Sinkhorn extends this approach by leveraging LCN, a general similarity matrix approximation that fuses local (sparse) and global (low- rank) approximations, allowing us to simultaneously cap- ture interactions between both close and far points. LCN- Sinkhorn thus fuses sparse Sinkhorn and Nyström-Sinkhorn (Altschuler et al., 2019). Both sparse Sinkhorn and LCN- Sinkhorn run in log-linear time. We theoretically analyze these approximations and show that sparse corrections can lead to signiﬁcant improvements over the Nyström approximation. We furthermore validate these approximations by showing that they are able to re- produce both the Sinkhorn distance and transport plan sig- niﬁcantly better than previous methods across a wide range of regularization parameters and computational budgets (as e.g. demonstrated in Fig. 1). We then show the impact of these improvements by employing Sinkhorn approximations end-to-end in two high-impact machine learning tasks. First, we incorporate them into Wasserstein Procrustes for word embedding alignment (Grave et al., 2019). Without any further model changes LCN-Sinkhorn improves upon the arXiv:2107.06876v2  [cs.LG]  5 Apr 2022Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More 0 1¯Pij 0 1¯PMS ij Multiscale OT 0 1¯Pij 0 1¯PNys,ij Diagonal (perfect correlation) Nyström 0 1¯Pij 0 1¯Psp ij Sparse Sinkhorn 0 1¯Pij 0 1¯PLCN,ij LCN-Sinkhorn Figure 1: The proposed methods (sparse and LCN- Sinkhorn) show a clear correlation with the full Sinkhorn transport plan, as opposed to previous methods. Entries of approximations (y-axis) and full Sinkhorn (x-axis) for pre- aligned word embeddings (EN-DE). Color denotes density. original method’s accuracy by 3.1 percentage points using a third of the training time. Second, we develop the graph transport network (GTN), which combines graph neural net- works (GNNs) with optimal transport for graph distance regression, and further improve it via learnable unbalanced OT and multi-head OT. GTN with LCN-Sinkhorn is the ﬁrst model that both overcomes the bottleneck of using a single embedding per graph and scales log-linearly in the number of nodes. Our implementation is available online.1 In summary, our paper’s main contributions are: • Locally Corrected Nyström (LCN), a ﬂexible log-linear time approximation for similarity matrices, merging local (sparse) and global (low-rank) approximations. • Entropy-regularized optimal transport (a.k.a. Sinkhorn dis- tance) with log-linear runtime via sparse Sinkhorn and LCN-Sinkhorn. These are the ﬁrst log-linear approxi- mations that are stable enough to substitute full entropy- regularized OT in models using high-dimensional spaces. • The graph transport network (GTN), a siamese GNN using multi-head unbalanced LCN-Sinkhorn. GTN both sets the state of the art on graph distance regression and still scales log-linearly in the number of nodes. 2. Entropy-regularized optimal transport This work focuses on optimal transport between two discrete sets of points. We use entropy regularization, which enables fast computation and often performs better than regular OT 1https://www.daml.in.tum.de/lcn (Cuturi, 2013). Formally, given two categorical distributions modelled via the vectors p∈Rn and q ∈Rm supported on two sets of points Xp = {xp1,..., xpn}and Xq = {xq1,..., xqm}in Rd and the cost function c: Rd×Rd → R (e.g. the L2 distance) giving rise to the cost matrix Cij = c(xpi,xqi) we aim to ﬁnd the Sinkhorn distance dλ c and the associated optimal transport plan ¯P (Cuturi, 2013) ¯P = arg min P ⟨P,C⟩F −λH(P), dλ c = ⟨P,C⟩F −λH(P), s.t. P1m = p, PT1n = q, (1) with the Frobenius inner product ⟨.,.⟩F and the entropy H(P) = −∑n i=1 ∑m j=1 Pijlog Pij. Note that dλ c in- cludes the entropy and can thus be negative, while Cu- turi (2013) originally used d1/λ Cuturi,c = ⟨¯P,C⟩F. This op- timization problem can be solved by ﬁnding the vectors ¯sand ¯tthat normalize the columns and rows of the ma- trix ¯P = diag( ¯s)Kdiag(¯t) with the similarity matrix Kij = e− Cij λ , so that ¯P1m = pand ¯PT1n = q. We can achieve this via the Sinkhorn algorithm, which initializes the normalization vectors as s(1) = 1n and t(1) = 1m and updates them alternatingly via (Sinkhorn & Knopp, 1967) s(i) = p⊘(Kt(i−1)), t(i) = q⊘(KTs(i)) (2) until convergence, where ⊘denotes elementwise division. 3. Sparse Sinkhorn The Sinkhorn algorithm is faster than non-regularized EMD algorithms, which run in O(n2mlog nlog(nmax(C))) (Tarjan, 1997). However, its computational cost is still quadratic in time O(nm), which is prohibitively expensive for large nand m. We overcome this by observing that the matrix K, and hence also ¯P, is negligibly small everywhere except at each point’s closest neighbors because of the ex- ponential used in K’s computation. We propose to leverage this by approximating Cvia the sparse matrix Csp, where Csp ij = { Cij if xpi and xqj are “near”, ∞ otherwise. (3) Ksp and ¯Psp follow from the deﬁnitions of K and ¯P. Finding “near” neighbors can be approximately solved via locality-sensitive hashing (LSH) on Xp ∪Xq. Locality-sensitive hashing. LSH tries to ﬁlter “near” from “far” data points by putting them into different hash buckets. Points closer than a certain distance r1 are put into the same bucket with probability at leastp1, while those beyond some distance r2 = c·r1 with c> 1 are put into the same bucket with probability at most p2 ≪p1. There is a plethora of LSH methods for different metric spaces and their associ- ated cost (similarity/distance) functions (Wang et al., 2014;Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More Shrivastava & Li, 2014), and we can use any of them. In this work we focus on cross-polytope LSH (Andoni et al., 2015) and k-means LSH (Paulevé et al., 2010) (see App. H). We can control the (average) number of neighbors via the number of hash buckets. This allows sparse Sinkhorn with LSH to scale log-linearly with the number of points, i.e. O(nlog n) for n≈m(see App. A and App. K). Unfortu- nately, Sinkhorn with LSH can fail when e.g. the cost is evenly distributed or the matrix Ksp does not have support (see App. B). However, we can alleviate these limitations by fusing Ksp with the Nyström method. 4. Locally corrected Nyström and LCN-Sinkhorn Nyström method. The Nyström method is a popular way of approximating similarity matrices that provides per- formance guarantees for many important tasks (Williams & Seeger, 2001; Musco & Musco, 2017). It approx- imates a positive semi-deﬁnite (PSD) similarity matrix K via its low-rank decomposition KNys = UA−1V. Since the optimal decomposition via SVD is too expen- sive to compute, Nyström instead chooses a set of l landmarks L = {xl1,..., xll}and obtains the matrices via Uij = k(xpi,xlj), Aij = k(xli,xlj), and Vij = k(xli,xqj), where k(x1,x2) is an arbitrary PSD kernel, e.g. k(x1,x2) = e−c(x1,x2) λ for Sinkhorn. Common meth- ods of choosing landmarks from Xp ∪Xq are uniform and ridge leverage score (RLS) sampling. We instead focus on k-means Nyström and sampling via k-means++, which we found to be signiﬁcantly faster than recursive RLS sampling (Zhang et al., 2008) and perform better than both uniform and RLS sampling (see App. H). Sparse vs. Nyström. Exponential kernels like the one used for K (e.g. the Gaussian kernel) typically correspond to a reproducing kernel Hilbert space that is inﬁnitely dimen- sional. The resulting Gram matrix Kthus usually has full rank. A low-rank approximation like the Nyström method can therefore only account for its global structure and not the local structure around each point x. As such, it is ill-suited for any moderately low entropy regularization parameter, where the transport matrix ¯P resembles a permutation ma- trix. Sparse Sinkhorn, on the other hand, cannot account for global structure and instead approximates all non-selected distances as inﬁnity. It will hence fail if more than a handful of neighbors are required per point. These approximations are thus opposites of each other, and as such not competing but rather complementary approaches. Locally corrected Nyström. Since the entries in our sparse approximation are exact, we can directly fuse it with the Nyström approximation. For the indices of all non-zero values in the sparse approximationKsp we calculate the cor- responding entries in the Nyström approximation, obtaining the sparse matrix Ksp Nys. To obtain the locally corrected Nys- tröm (LCN) approximation2 we subtract these entries from KNys and replace them with their exact values, i.e. KLCN = KNys −Ksp Nys + Ksp = KNys + Ksp ∆. (4) LCN-Sinkhorn. To obtain the approximate transport plan ¯PLCN we run the Sinkhorn algorithm with KLCN instead of K. However, we never fully instantiate KLCN. Instead, we directly use the decomposition and calculate the matrix- vector product in Eq. (2) as KLCNt= U(A−1Vt) +Ksp ∆t, similarly to Altschuler et al. (2019). As a result we ob- tain the decomposed approximate OT plan ¯PLCN = ¯PNys + ¯Psp ∆ = ¯PU ¯PW + ¯Psp −¯Psp Nys and the approximate distance (using Lemma A from Altschuler et al. (2019)) dλ LCN,c = λ(sT ¯PU ¯PW1m + 1T n ¯PU ¯PWt + sT ¯Psp ∆ 1m + 1T n ¯Psp ∆ t). (5) This approximation scales log-linearly with dataset size (see App. A and App. K for details). It allows us to smoothly move from Nyström-Sinkhorn to sparse Sinkhorn by vary- ing the number of landmarks and neighbors. We can thus freely choose the optimal “operating point” based on the un- derlying problem and regularization parameter. We discuss the limitations of LCN-Sinkhorn in App. B. 5. Theoretical analysis Approximation error.The main question we aim to answer in our theoretical analysis is what improvements to expect from adding sparse corrections to Nyström Sinkhorn. To do so, we ﬁrst analyse approximations of Kin a uniform and a clustered data model. In these we use Nyström and LSH schemes that largely resemble k-means, as used in most of our experiments. Relevant proofs and notes for this section can be found in App. C to G. Theorem 1. Let Xp and Xq have nsamples that are uni- formly distributed on a d-dimensional closed, locally Eu- clidean manifold. Let Cij = ∥xpi −xqj∥2 and Kij = e−Cij/λ. Let the landmarks be arranged a priori, with a minimum distance 2R between each other. Then the ex- pected error of the Nyström approximation KNys between a point xpi and its kth-nearest neighbor xqik is E[Ki,ik −KNys,i,ik] = E[e−δk/λ] −E[KNys,i,ik], (6) with δk denoting the kth-nearest neighbor distance. With Γ(.,.) denoting the upper incomplete Gamma function the 2LCN has an unrelated namesake on integrals, which uses high-order term to correct quadrature methods around singularities (Canino et al., 1998).Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More second term is bounded by E[KNys,i,ik] ≤d(Γ(d) −Γ(d,2R/λ)) (2R/λ)d + O(e−2R/λ). (7) Eq. (6) is therefore dominated by E[e−δk/λ] if δk ≪R, which is a reasonable assumption given that R only de- creases slowly with the number of landmarks lsince R≥ ((d/2)! l )1/d 1 2√π (Cohn, 2017). In this case the approxima- tion’s largest error is the one associated with the point’s nearest neighbor. LCN uses the exact result for these near- est neighbors and therefore removes the largest errors, pro- viding signiﬁcant beneﬁts even for uniform data. For ex- ample, just removing the ﬁrst neighbor’s error we obtain a 68 % decrease in the dominant ﬁrst term (d=32, λ=0.05, n=1000). This is even more pronounced in clustered data. Theorem 2. Let Xp,Xq ⊆Rd be inside c(shared) clusters. Let r be the maximum L2 distance of a point to its clus- ter center and Dthe minimum distance between different cluster centers, with r≪D. Let Cij = ∥xpi −xqj∥2 and Kij = e−Cij/λ. Let each LSH bucket used for the sparse approximation Ksp cover at least one cluster. Let KNys and KLCN both use one landmark at each cluster center. Then the maximum possible error is max xpi,xqj Kij −KNys,i,j = 1 −e−2r/λ −O(e−2(D−r)/λ), (8) max xpi,xqj Kij −Ksp ij = e−(D−2r)/λ, (9) max xpi,xqj Kij −KLCN,i,j = e−(D−2r)/λ(1 −e−2r/λ(2 −e−2r/λ) + O(e−2D/λ)). (10) This shows that the error in KNys is close to 1 for any rea- sonably large r λ (which is the maximum error possible). The errors in Ksp and KLCN on the other hand are vanishingly small in this case, since r≪D. The reduced maximum error directly translates to an im- proved Sinkhorn approximation. We can show this by adapting the Sinkhorn approximation error bounds due to Altschuler et al. (2019). Deﬁnition 1. A generalized diagonal is the set of elements Miσ(i) ∀i∈{1,...,n }with matrix M ∈Rn×n and per- mutation σ. A non-negative matrix has support if it has a strictly positive generalized diagonal. It has total support if M ̸= 0 and all non-zero elements lie on a strictly positive generalized diagonal. Theorem 3. Let Xp,Xq ⊆Rd have n samples. Denote ρ as the maximum distance between two samples. Let ˜Kbe a non-negative matrix with support, which approxi- mates the similarity matrix Kwith Kij = e−∥xpi−xqj∥2/λ and maxi,j|˜Kij − Kij| ≤ ε′ 2 e−ρ/λ, where ε′ = min(1, ε 50(ρ+λlog λn ε ) ). When performing the Sinkhorn al- gorithm until ∥˜P1N −p∥1 + ∥˜PT1N −q∥1 ≤ε′/2, the resulting approximate transport plan ˜P and distance ˜dλ c are bounded by |dλ c −˜dλ ˜c|≤ ε, D KL( ¯P∥˜P) ≤ε/λ. (11) Convergence rate. We next show that sparse and LCN- Sinkhorn converge as fast as regular Sinkhorn by adapting the convergence bound by Dvurechensky et al. (2018) to account for sparsity. Theorem 4. Given a non-negative matrix ˜K∈Rn×n with support and p∈Rn, q∈Rn. The Sinkhorn algorithm gives a transport plan satisfying∥˜P1N−p∥1 +∥˜PT1N−q∥1 ≤ εin iterations k≤2 + −4 ln(mini,j{˜Kij|˜Kij >0}mini,j{pi,qj}) ε . (12) Backpropagation. Efﬁcient gradient computation is al- most as important for modern deep learning models as the algorithm itself. These models usually aim at learning the embeddings in Xp and Xq and therefore need gradients w.r.t. the cost matrix C. We can estimate these either via auto- matic differentiation of the unrolled Sinkhorn iterations or via the analytic solution that assumes exact convergence. Depending on the problem at hand, either the automatic or the analytic estimator will lead to faster overall convergence (Ablin et al., 2020). LCN-Sinkhorn works ﬂawlessly with automatic backpropagation since it only relies on basic lin- ear algebra (except for choosing Nyström landmarks and LSH neighbors, for which we use a simple straight-through estimator (Bengio et al., 2013)). To enable fast analytic backpropagation we provide analytic gradients in Propo- sition 1. Note that both backpropagation methods have runtime linear in the number of points nand m. Proposition 1. In entropy-regularized OT and LCN- Sinkhorn the derivatives of the distances dλ c and dλ LCN,c (Eqs. (1) and (5)) and the optimal transport plan ¯P ∈ Rn×m w.r.t. the (decomposed) cost matrixC∈Rn×m with total support are ∂dλ c ∂C = ¯P, (13) ∂dλ LCN,c ∂U = −λ¯s(W¯t)T, ∂dλ LCN,c ∂W = −λ(¯sTU)T¯tT, ∂dλ LCN,c ∂log Ksp = −λ¯Psp, ∂dλ LCN,c ∂log Ksp Nys = −λ¯Psp Nys. (14) This allows backpropagation in time O((n+ m)l2).Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More 6. Graph transport network Graph distance learning. Predicting similarities or dis- tances between graph-structured objects is useful across a wide range of applications. It can be used to predict the reac- tion rate between molecules (Houston et al., 2019), or search for similar images (Johnson et al., 2015), similar molecules for drug discovery (Birchall et al., 2006), or similar code for vulnerability detection (Li et al., 2019). We propose the graph transport network (GTN) to evaluate approximate Sinkhorn on a full deep learning model and advance the state of the art on this task. Graph transport network. GTN uses a Siamese graph neural network (GNN) to embed two graphs independently as sets of node embeddings. These sets are then matched us- ing multi-head unbalanced OT. Node embeddings represent the nodes’ local environments, so similar neighborhoods will be close in embedding space and matched accordingly. Since Sinkhorn is symmetric and permutation invariant, any identical pair of graphs will thus by construction have a predicted distance of 0 (ignoring the entropy offset). More precisely, given an undirected graph G= (V,E), with node set Vand edge set E, node attributes xi ∈RHx and (optional) edge attributes ei,j ∈RHe , with i,j ∈V , we update the node embeddings in each GNN layer via h(l) self,i = σ(W(l) nodeh(l−1) i + b(l)), (15) h(l) i = h(l) self,i + ∑ j∈Ni η(l) i,jh(l) self,jWedgeei,j, (16) with Ni denoting the neighborhood of node i, h(0) i = xi, h(l) i ∈ RHN for l ≥ 1, the bilinear layer Wedge ∈ RHN×HN×He , and the degree normalization η(1) i,j = 1 and η(l) i,j = 1/√degidegj for l> 1. This choice of ηi,j allows our model to handle highly skewed degree distributions while still being able to represent node degrees. We found the choice of non-linearity σnot to be critical and chose a LeakyReLU. We do not use the bilinear layer Wedgeei,j if there are no edge attributes. We aggregate each layer’s node embeddings to obtain the overall embedding of node i hGNN i = [h(1) self,i∥h(1) i ∥h(2) i ∥... ∥h(L) i ]. (17) We then compute the embeddings for matching via hﬁnal i = MLP(hGNN i ). Having obtained the embedding sets Hﬁnal 1 and Hﬁnal 2 of both graphs we use the L2 distance as a cost function for the Sinkhorn distance. Finally, we calculate the prediction from the Sinkhorn distance viad= dλ cwout +bout, with learnable wout and bout. GTN is trained end-to-end via backpropagation. For small graphs we use the full Sinkhorn distance and scale to large graphs with LCN-Sinkhorn. GTN is more expressive than models that aggegrate node em- beddings to a single ﬁxed-size embedding but still scales log-linearly in the number of nodes, as opposed to previous approaches which scale quadratically. Learnable unbalanced OT.Since GTN regularly encoun- ters graphs with disagreeing numbers of nodes it needs to be able to handle cases where ∥p∥1 ̸= ∥q∥1 or where not all nodes in one graph have a corresponding node in the other, i.e. P1m <por PT1n <q. Unbalanced OT allows han- dling both of these cases (Peyré & Cuturi, 2019), usually by swapping the strict balancing requirements with a uniform divergence loss term onpand q(Frogner et al., 2015; Chizat et al., 2018). However, this uniformly penalizes deviations from balanced OT and therefore cannot adaptively ignore parts of the distribution. We propose to improve on this by swapping the cost matrix Cwith the bipartite matching (BP) matrix (Riesen & Bunke, 2009) CBP = [ C C (p,ε) C(ε,q) C(ε,ε) ] , C(p,ε) ij = { ci,ε i= j ∞ i̸= j , C(ε,q) ij = { cε,j i= j ∞ i̸= j , C(ε,ε) ij = 0, (18) and obtain the deletion cost ci,ε and cε,j from the input sets Xp and Xq. Using the BP matrix only adds minor compu- tational overhead since we just need to save the diagonals cp,ε and cε,q of Cp,ε and Cε,q. We can then use CBP in the Sinkhorn algorithm (Eq. (2)) via KBPt= [Kˆt+ cp,ε ⊙ˇt cε,q ⊙ˆt+ 1T nˇt ] , KT BPs= [KTˆs+ cε,q ⊙ˇs cp,ε ⊙ˆs+ 1T mˇs ] , (19) where ˆtdenotes the upper and ˇtthe lower part of the vector t. To calculate dλ c we can decompose the transport plan PBP in the same way as CBP, with a single scalar for Pε,ε. For GTN we obtain the deletion cost via ci,ε = ∥α⊙xpi∥2, with a learnable vector α∈Rd. Multi-head OT. Inspired by attention models (Vaswani et al., 2017) and multiscale kernels (Bermanis et al., 2013) we further improve GTN by using multiple OT heads. Us- ing K heads means that we calculate K separate sets of embeddings representing the same pair of objects by using separate linear layers, i.e. hﬁnal k,i = W(k)hGNN i for head k. We then calculate OT in parallel for these sets using a series of regularization parameters λk = 2k−K/2λ. This yields a set of distances dλ c ∈RK. We obtain the ﬁnal prediction via d = MLP( dλ c). Both learnable unbalanced OT and multi-head OT might be of independent interest. 7. Related work Hierarchical kernel approximation. These methods usu- ally hierarchically decompose the kernel matrix into sepa-Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More rate blocks and use low-rank or core-diagonal approxima- tions for each block (Si et al., 2017; Ding et al., 2017). This idea is similar in spirit to LCN, but LCN boils it down to its essence by using one purely global part and a ﬁne-grained LSH method to obtain one exact and purely local part. Log-linear optimal transport. For an overview of optimal transport and its foundations see Peyré & Cuturi (2019). On low-dimensional grids and surfaces OT can be solved using dynamical OT (Papadakis et al., 2014; Solomon et al., 2014), convolutions (Solomon et al., 2015), or embedding/hashing schemes (Indyk & Thaper, 2003; Andoni et al., 2008). In higher dimensions we can use tree-based algorithms (Back- urs et al., 2020) or hashing schemes (Charikar, 2002), which are however limited to a previously ﬁxed set of points Xp, Xq, on which only the distributions pand qchange. Another approach are sliced Wasserstein distances (Rabin et al., 2011). However, they do not provide a transport plan, require the L2 distance as a cost function, and are either unstable in convergence or prohibitively expensive for high dimensions ( O(nd3)) (Meng et al., 2019). For high-dimensional sets that change dynamically (e.g. dur- ing training) one method of achieving log-linear runtime is a multiscale approximation of entropy-regularized OT (Schmitzer, 2019; Gerber & Maggioni, 2017). Tenetov et al. (2018) recently proposed using a low-rank approximation of the Sinkhorn similarity matrix obtained via a semidiscrete approximation of the Euclidean distance. Altschuler et al. (2019) improved upon this approach by using the Nyström method for the approximation. However, these approaches still struggle with high-dimensional real-world problems, as we will show in Sec. 8. Accelerating Sinkhorn. Another line of work has been pur- suing accelerating entropy-regularized OT without chang- ing its computational complexity w.r.t. the number of points. Original Sinkhorn requires O(1/ε2) iterations, but Dvurechensky et al. (2018) and Jambulapati et al. (2019) recently proposed algorithms that reduce the computational complexity to O(min(n9/4/ε,n2/ε2)) and O(n2/ε), re- spectively. Mensch & Peyré (2020) proposed an online Sinkhorn algorithm to signiﬁcantly reduce its memory cost. Alaya et al. (2019) proposed reducing the size of the Sinkhorn problem by screening out neglectable components, which allows for approximation guarantees. Genevay et al. (2016) proposed using a stochastic optimization scheme instead of Sinkhorn iterations. Essid & Solomon (2018) and Blondel et al. (2018) proposed alternative regularizations to obtain OT problems with similar runtimes as the Sinkhorn algorithm. This work is largely orthogonal to ours. Embedding alignment. For an overview of cross-lingual word embedding models see Ruder et al. (2019). Unsuper- vised word embedding alignment was proposed by Conneau et al. (2018), with subsequent advances by Alvarez-Melis & Jaakkola (2018); Grave et al. (2019); Joulin et al. (2018). Graph matching and distance learning.Graph neural net- works (GNNs) have recently been successful on a wide va- riety of graph-based tasks (Kipf & Welling, 2017; Gasteiger et al., 2019; 2020; Zambaldi et al., 2019). GNN-based ap- proaches for graph matching and graph distance learning either rely on a single ﬁxed-dimensional graph embedding (Bai et al., 2019; Li et al., 2019), or only use attention or some other strongly simpliﬁed variant of optimal transport (Bai et al., 2019; Riba et al., 2018; Li et al., 2019). Oth- ers break permutation invariance and are thus ill-suited for this task (Ktena et al., 2017; Bai et al., 2018). So far only approaches using a single graph embedding allow faster than quadratic scaling in the number of nodes. Compared to the Sinkhorn-based image model proposed by Wang et al. (2019) GTN uses no CNN or cross-graph attention, but an enhanced GNN and embedding aggregation scheme. OT has recently been proposed for graph kernels (Maretic et al., 2019; Vayer et al., 2019), which can (to some extent) be used for graph matching, but not for distance learning. 8. Experiments Approximating Sinkhorn. We start by directly investigat- ing different Sinkhorn approximations. To do so we com- pute entropy-regularized OT on (i) pairs of104 word embed- dings from Conneau et al. (2018), which we preprocess with Wasserstein Procrustes alignment in order to obtain both close and distant neighbors, (ii) the armadillo and dragon point clouds from the Stanford 3D Scanning Repository (Stanford, 2014) (with 104 randomly subsampled points), and (iii) pairs of 104 data points that are uniformly dis- tributed in the d-ball (d= 16). We let every method use the same total number of 40 average neighbors and landmarks (LCN uses 20 each) and set λ = 0 .05 (as e.g. in Grave et al. (2019)). Besides the Sinkhorn distance we measure transport plan approximation quality by (a) calculating the Pearson correlation coefﬁcient (PCC) between all entries in the approximated plan and the true ¯P and (b) comparing the sets of 0.1 % largest entries in the approximated and true ¯P using the Jaccard similarity (intersection over union, IoU). Note that usually the OT plan is more important than the distance, since it determines the training gradient and tasks like embedding alignment are exclusively based on the OT plan. In all ﬁgures the error bars denote standard deviation across 5 runs, which is often too small to be visible. Table 1 shows that for word embeddings both sparse Sinkhorn, LCN-Sinkhorn and factored OT (Forrow et al., 2019) obtain distances that are signiﬁcantly closer to the true dλ c than Multiscale OT and Nyström-Sinkhorn. Furthermore, the transport plan computed by sparse Sinkhorn and LCN- Sinkhorn show both a PCC and IoU that are around twice as high as Multiscale OT, while Nyström-Sinkhorn andScalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More Table 1: Mean and standard deviation of relative Sinkhorn distance error, IoU of top 0.1 % and correlation coefﬁcient (PCC) of OT plan entries across 5 runs. Sparse Sinkhorn and LCN-Sinkhorn achieve the best approximation in all 3 measures. EN-DE EN-ES 3D point cloud Uniform in d-ball (d=16) Rel. err.dλc PCC IoU Rel. err. dλc PCC IoU Rel. err. dλc PCC IoU Rel. err. dλc PCC IoU Factored OT 0.318 0.044 0.019 0.332 0.037 0.026 6.309 0.352 0.004 1.796 0.096 0.029 ±0.001 ±0.001 ±0.002 ±0.001 ±0.002 ±0.005 ±0.004 ±0.001 ±0.001 ±0.001 ±0.001 0.000 Multiscale OT 0.634 0.308 0.123 0.645 0.321 0.125 0.24 0.427 0.172 0.03 0.091 0.021 ±0.011 ±0.014 ±0.005 ±0.014 ±0.006 ±0.012 ±0.07 ±0.008 ±0.011 ±0.02 ±0.005 ±0.001 Nyström Skh. 1.183 0.077 0.045 1.175 0.068 0.048 1.89 0.559 0.126 1.837 0.073 0.018 ±0.005 ±0.001 ±0.005 ±0.018 ±0.001 ±0.006 ±0.07 ±0.009 ±0.014 ±0.006 0.000 0.000 Sparse Skh. 0.233 0.552 0.102 0.217 0.623 0.102 0.593 0.44 0.187 0.241 0.341 0.090 ±0.002 ±0.004 ±0.001 ±0.001 ±0.004 ±0.001 ±0.015 ±0.03 ±0.014 ±0.002 ±0.004±0.001 LCN-Sinkhorn 0.406 0.673 0.197 0.368 0.736 0.201 1.91 0.564 0.195 0.435 0.328 0.079 ±0.015 ±0.012±0.007 ±0.012 ±0.003±0.003 ±0.28 ±0.008±0.013 ±0.009 ±0.006 ±0.001 0 100 200 Runtime (ms) 0.0 0.2 0.4 0.6 0.8 1.0 PCC 0 100 200 Neighbors + landmarks 0.0 0.2 0.4 0.6 0.8 1.0 PCC 10−3 10−2 10−1 100 λ 0.0 0.2 0.4 0.6 0.8 1.0 PCC Fact. OT Multsc. OT Nys. Skh. Sp. Skh. LCN-Skh. Figure 2: OT plan approximation quality for EN-DE, via PCC. Left: Sparse Sinkhorn offers the best tradeoff with runtime, with LCN-Sinkhorn closely behind. Center: LCN-Sinkhorn achieves the best approximation for low and sparse Sinkhorn for high numbers of neighbors/landmarks. Right: Sparse Sinkhorn performs best for low, LCN-Sinkhorn for moderate and factored OT for very high entropy regularization λ. The arrow indicates factored OT results far outside the range. factored OT exhibit almost no correlation. LCN-Sinkhorn performs especially well in this regard. This is also evident in Fig. 1, which shows how the 104 ×104 approximated OT plan entries compared to the true Sinkhorn values. Mul- tiscale OT shows the best distance approximation on 3D point clouds and random high-dimensional data. However, sparse Sinkhorn and LCN-Sinkhorn remain the best OT plan approximations, especially in high dimensions. Fig. 2 shows that sparse Sinkhorn offers the best trade-off between runtime and OT plan quality. Factored OT ex- hibits a runtime 2 to 10 times longer than the competition due to its iterative reﬁnement scheme. LCN-Sinkhorn per- forms best for use cases with constrained memory (few neighbors/landmarks). The number of neighbors and land- marks directly determines memory usage and is linearly proportional to the runtime (see App. K). Fig. 2 furthermore shows that sparse Sinkhorn performs best for low regular- izations, where LCN-Sinkhorn fails due to the Nyström part going out of bounds. Nyström Sinkhorn performs best at high values and LCN-Sinkhorn always performs better than both (as long as it can be calculated). Interestingly, all approximations except factored OT seem to fail at high λ. We defer analogously discussing the distance approxi- mation to App. L. All approximations scale linearly both in the number of neighbors/landmarks and dataset size, as shown in App. K. Overall, we see that sparse Sinkhorn and LCN-Sinkhorn yield signiﬁcant improvements over previ- ous approximations. However, do these improvements also translate to better performance on downstream tasks? Embedding alignment. Embedding alignment is the task of ﬁnding the orthogonal matrix R∈Rd×d that best aligns the vectors from two different embedding spaces, which is e.g. useful for unsupervised word translation. We use the experimental setup established by Conneau et al. (2018) by migrating Grave et al. (2019)’s implementation to PyTorch. The only change we make is using the full set of20 000word embeddings and training for 300 steps, while reducing the learning rate by half every 100 steps. We do not change any other hyperparameters and do not use unbalanced OT. After training we match pairs via cross-domain similarity local scaling (CSLS) (Conneau et al., 2018). We use 10 Sinkhorn iterations, 40 neighbors on average for sparse Sinkhorn, andScalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More Table 2: Accuracy and standard deviation across 5 runs for unsupervised word embedding alignment with Wasserstein Procrustes. LCN-Sinkhorn improves upon the original by 3.1 pp. before and 2.0 pp. after iterative CSLS reﬁnement. *Migrated and re-run on GPU via PyTorch Time (s ) EN-ES ES-EN EN-FR FR-EN EN-DE DE-EN EN-RU RU-EN Avg. Original* 268 79.2 ±0.2 78.8 ±2.8 81.0 ±0.3 79.4 ±0.9 71.7 ±0.2 65.7 ±3.4 36.3 ±1.1 51.1 ±1.1 67.9 Full Sinkhorn 402 81.1 ±0.0 82.0 ±0.0 81.2±0.0 81.3 ±0.0 74.1 ±0.0 70.7±0.0 37.3 ±0.0 53.5 ±0.0 70.1 Multiscale OT 88.2 24 ±31 74.7 ±3.3 27 ±32 6.3 ±4.4 36 ±10 47 ±21 0.0 ±0.0 0.2 ±0.1 26.8 Nyström Skh. 102 64.4 ±1.0 59.3 ±1.2 64.1 ±1.6 56.8 ±4.0 54.1 ±0.6 47.1 ±3.5 14.1 ±1.2 22.5 ±2.4 47.8 Sparse Skh. 49.2 80.2 ±0.2 81.7 ±0.4 80.9 ±0.3 80.1 ±0.2 72.1 ±0.6 65.1 ±1.7 35.5 ±0.6 51.5 ±0.4 68.4 LCN-Sinkhorn 86.8 81.8 ±0.2 81.3±1.8 82.0±0.4 82.1±0.3 73.6±0.2 71.3 ±0.9 41.0±0.8 55.1±1.4 71.0 Original* + ref. 268+81 83.0 ±0.3 82.0 ±2.5 83.8±0.1 83.0±0.4 77.3 ±0.3 69.7±4.3 46.2 ±1.0 54.0 ±1.1 72.4 LCN-Skh. + ref.86.8+81 83.5 ±0.2 83.1±1.3 83.8±0.2 83.6±0.1 77.2±0.3 72.8±0.7 51.8±2.6 59.2±1.9 74.4 20 neighbors and landmarks for LCN-Sinkhorn (for details see App. H). We allow both multiscale OT and Nyström Sinkhorn to use as many landmarks and neighbors as can ﬁt into GPU memory and ﬁnetune both methods. Table 2 shows that using full Sinkhorn yields a signiﬁcant improvement in accuracy on this task compared to the origi- nal approach of performing Sinkhorn on randomly sampled subsets of embeddings (Grave et al., 2019). LCN-Sinkhorn even outperforms the full version in most cases, which is likely due to regularization effects from the approximation. It also runs 4.6x faster than full Sinkhorn and 3.1x faster than the original scheme, while using 88 % and 44 % less memory, respectively. Sparse Sinkhorn runs 1.8x faster than LCN-Sinkhorn but cannot match its accuracy. LCN- Sinkhorn still outcompetes the original method after reﬁning the embeddings with iterative local CSLS (Conneau et al., 2018). Both multiscale OT and Nyström Sinkhorn fail at this task, despite their larger computational budget. This shows that the improvements achieved by sparse Sinkhorn and LCN-Sinkhorn have an even larger impact in practice. Graph distance regression. The graph edit distance (GED) is useful for various tasks such as image retrieval (Xiao et al., 2008) or ﬁngerprint matching (Neuhaus & Bunke, 2004), but its computation is NP-complete (Bunke & Shearer, 1998). For large graphs we therefore need an effective ap- proximation. We use the Linux dataset by Bai et al. (2019) and generate 2 new datasets by computing the exact GED using the method by Lerouge et al. (2017) on small graphs (≤ 30 nodes) from the AIDS dataset (Riesen & Bunke, 2008) and a set of preferential attachment graphs. We com- pare GTN to 3 state-of-the-art baselines: SiameseMPNN (Riba et al., 2018), SimGNN (Bai et al., 2019), and the Graph Matching Network (GMN) (Li et al., 2019). We tune the hyperparameters of all baselines and GTN via grid search. For more details see App. H to J. We ﬁrst test GTN and the proposed OT enhancements. Ta- ble 3 shows that GTN improves upon other models by 20 % with a single head and by 48 % with 8 OT heads. Its per- Table 3: RMSE for GED regression across 3 runs and the targets’ standard deviationσ. GTN outperforms previous models by 48 %. Linux AIDS30 Pref. att. σ 0.184 16.2 48.3 SiamMPNN 0.090 ±0.007 13.8 ±0.3 12.1 ±0.6 SimGNN 0.039 4.5 ±0.3 8.3 ±1.4 GMN 0.015 ±0.000 10.3 ±0.6 7.8 ±0.3 GTN, 1 head 0.022 ±0.001 3.7 ±0.1 4.5 ±0.3 8 OT heads 0.012 ± 0.001 3.2 ± 0.1 3.5 ± 0.2 Unbalanced OT 0.033 ±0.002 15.7 ±0.5 9.7 ±0.9 Balanced OT 0.034 ±0.001 15.3 ±0.1 27.4 ±0.9 Table 4: RMSE for graph distance regression across 3 runs and the targets’ standard deviationσ. Using LCN-Sinkhorn with GTN increases the error by only 10 % and allows log- linear scaling. GED PM [ 10−2] AIDS30 Pref. att. Pref. att. 200 σ 16.2 48.3 10.2 Full Sinkhorn 3.7 ±0.1 4.5 ±0.3 1.27 ±0.06 Nyström Skh. 3.6 ± 0.3 6.2 ±0.6 2.43 ±0.07 Multiscale OT 11.2 ±0.3 27.4 ±5.4 6.71 ±0.44 Sparse Skh. 44 ±30 40.7 ±8.1 7.57 ±1.09 LCN-Skh. 4.0 ±0.1 5.1 ± 0.4 1.41 ± 0.15 formance breaks down with regular unbalanced (using KL- divergence loss for the marginals) and balanced OT, showing the importance of learnable unbalanced OT. Having established GTN as a state-of-the-art model we next ask whether we can sustain its performance when using approximate OT. For this we additionally generate a set of larger graphs with around 200 nodes and use the Pyramid matching (PM) kernel (Nikolentzos et al., 2017) as the pre- diction target, since these graphs are too large to compute the GED. See App. J for hyperparameter details. Table 4 shows that both sparse Sinkhorn and the multiscale method usingScalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More 4 (expected) neighbors fail at this task, demonstrating that the low-rank approximation in LCN has a crucial stabilizing effect during training. Nyström Sinkhorn with 4 landmarks performs surprisingly well on the AIDS30 dataset, suggest- ing an overall low-rank structure with Nyström acting as regularization. However, it does not perform as well on the other two datasets. Using LCN-Sinkhorn with 2 neigh- bors and landmarks works well on all three datasets, with an RMSE increased by only 10 % compared to full GTN. App. K furthermore shows that GTN with LCN-Sinkhorn in- deed scales linearly in the number of nodes across multiple orders of magnitude. This model thus enables graph match- ing and distance learning on graphs that are considered large even for simple node-level tasks (20 000 nodes). 9. Conclusion Locality-sensitive hashing (LSH) and the novel locally cor- rected Nyström (LCN) method enable fast and accurate approximations of entropy-regularized OT with log-linear runtime: Sparse Sinkhorn and LCN-Sinkhorn. The graph transport network (GTN) is one example for such a model, which can be substantially improved with learnable unbal- anced OT and multi-head OT. It sets the new state of the art for graph distance learning while still scaling log-linearly with graph size. These contributions enable new applica- tions and models that are both faster and more accurate, since they can sidestep workarounds such as pooling. ACKNOWLEDGMENTS We would like to thank Johannes Pitz, Oleksandr Shchur, Aleksandar Bojchevski, and Daniel Zügner for their support, suggestions, and feedback during the process of creating this paper. This research was supported by the Deutsche Forschungsgemeinschaft (DFG) through the Emmy Noether grant GU 1409/2-1 and the TUM International Graduate School of Science and Engineering (IGSSE), GSC 81. References Pierre Ablin, Gabriel Peyré, and Thomas Moreau. Super- efﬁciency of automatic differentiation for functions de- ﬁned as a minimum. In ICML, 2020. Mokhtar Z. Alaya, Maxime Berar, Gilles Gasso, and Alain Rakotomamonjy. Screening Sinkhorn Algorithm for Reg- ularized Optimal Transport. In NeurIPS, 2019. Jason Altschuler, Francis Bach, Alessandro Rudi, and Jonathan Niles-Weed. Massively scalable Sinkhorn dis- tances via the Nyström method. In NeurIPS, 2019. David Alvarez-Melis and Tommi S. Jaakkola. Gromov- Wasserstein Alignment of Word Embedding Spaces. In EMNLP, 2018. Alexandr Andoni, Piotr Indyk, and Robert Krauthgamer. Earth mover distance over high-dimensional spaces. In ACM-SIAM symposium on Discrete algorithms (SODA), 2008. Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya P. Razenshteyn, and Ludwig Schmidt. Practical and Op- timal LSH for Angular Distance. In NeurIPS, 2015. Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein Generative Adversarial Networks. In ICML, 2017. Arturs Backurs, Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Scalable Nearest Neighbor Search for Optimal Transport. In ICML, 2020. Yunsheng Bai, Hao Ding, Yizhou Sun, and Wei Wang. Con- volutional Set Matching for Graph Similarity. In Re- lational Representation Learning Workshop, NeurIPS , 2018. Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. SimGNN: A Neural Network Ap- proach to Fast Graph Similarity Computation. In WSDM, 2019. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. arXiv, 1308.3432, 2013. Christian Berg, Jens Peter Reus Christensen, and Paul Res- sel. Harmonic Analysis on Semigroups. Number 100 in Graduate Texts in Mathematics. 1984. Amit Bermanis, Amir Averbuch, and Ronald R. Coifman. Multiscale data sampling and function extension. Ap- plied and Computational Harmonic Analysis, 34(1):15– 29, 2013. Kristian Birchall, Valerie J. Gillet, Gavin Harper, and Stephen D. Pickett. Training Similarity Measures for Spe- ciﬁc Activities: Application to Reduced Graphs. Journal of Chemical Information and Modeling, 46(2):577–586, 2006. Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth and Sparse Optimal Transport. In AISTATS, 2018. Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl- Johann Simon-Gabriel, and Bernhard Schoelkopf. From optimal transport to generative modeling: the VEGAN cookbook. arXiv, 1705.07642, 2017. Horst Bunke and Kim Shearer. A graph distance metric based on the maximal common subgraph. Pattern Recog- nition Letters, 19(3):255–259, 1998.Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More Lawrence F. Canino, John J. Ottusch, Mark A. Stalzer, John L. Visher, and Stephen M. Wandzura. Numeri- cal Solution of the Helmholtz Equation in 2D and 3D Using a High-Order Nyström Discretization. Journal of Computational Physics, 146(2):627–663, 1998. Moses Charikar. Similarity estimation techniques from rounding algorithms. In ACM symposium on Theory of computing (STOC), 2002. Lénaïc Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. Scaling algorithms for unbal- anced optimal transport problems. Mathematics of Com- putation, 87(314):2563–2609, 2018. Henry Cohn. A Conceptual Breakthrough in Sphere Packing. Notices of the American Mathematical Society , 64(02): 102–115, 2017. Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Word translation without parallel data. In ICLR, 2018. Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In NeurIPS, 2017. Marco Cuturi. Sinkhorn Distances: Lightspeed Computa- tion of Optimal Transport. In NeurIPS, 2013. Yi Ding, Risi Kondor, and Jonathan Eskreis-Winkler. Mul- tiresolution Kernel Approximation for Gaussian Process Regression. In NeurIPS, 2017. Pavel E. Dvurechensky, Alexander Gasnikov, and Alexey Kroshnin. Computational Optimal Transport: Complex- ity by Accelerated Gradient Descent Is Better Than by Sinkhorn’s Algorithm. InICML, 2018. Montacer Essid and Justin Solomon. Quadratically Regu- larized Optimal Transport on Graphs. SIAM Journal on Scientiﬁc Computing, 40(4):A1961–A1986, 2018. Matthias Fey and Jan E. Lenssen. Fast Graph Represen- tation Learning with PyTorch Geometric. In Workshop on Representation Learning on Graphs and Manifolds, ICLR, 2019. Aden Forrow, Jan-Christian Hütter, Mor Nitzan, Philippe Rigollet, Geoffrey Schiebinger, and Jonathan Weed. Sta- tistical Optimal Transport via Factored Couplings. In AISTATS, 2019. Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso A. Poggio. Learning with a Wasserstein Loss. In NeurIPS, 2015. Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Günnemann. Predict then Propagate: Graph Neural Net- works Meet Personalized PageRank. In ICLR, 2019. Johannes Gasteiger, Janek Groß, and Stephan Günnemann. Directional Message Passing for Molecular Graphs. In ICLR, 2020. Aude Genevay, Marco Cuturi, Gabriel Peyré, and Francis R. Bach. Stochastic Optimization for Large-scale Optimal Transport. In NeurIPS, 2016. Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learn- ing Generative Models with Sinkhorn Divergences. In AISTATS, 2018. Samuel Gerber and Mauro Maggioni. Multiscale Strategies for Computing Optimal Transport. J. Mach. Learn. Res., 18:72:1–72:32, 2017. Edouard Grave, Armand Joulin, and Quentin Berthet. Un- supervised Alignment of Embeddings with Wasserstein Procrustes. In AISTATS, 2019. Paul L. Houston, Apurba Nandi, and Joel M. Bowman. A Machine Learning Approach for Prediction of Rate Constants. The Journal of Physical Chemistry Letters, 10 (17):5250–5258, 2019. Piotr Indyk and Nitin Thaper. Fast image retrieval via em- beddings. In International Workshop on Statistical and Computational Theories of Vision, ICCV, 2003. Arun Jambulapati, Aaron Sidford, and Kevin Tian. A Di- rect tilde{O}(1/epsilon) Iteration Parallel Algorithm for Optimal Transport. In NeurIPS, 2019. Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. Image retrieval using scene graphs. In CVPR, 2015. Armand Joulin, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou, and Edouard Grave. Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion. In EMNLP, 2018. Thomas N. Kipf and Max Welling. Semi-Supervised Clas- siﬁcation with Graph Convolutional Networks. In ICLR, 2017. Soﬁa Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Ra- jchl, Matthew Lee, Ben Glocker, and Daniel Rueckert. Distance Metric Learning Using Graph Convolutional Networks: Application to Functional Brain Networks. In MICCAI, 2017. Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre Héroux, and Sébastien Adam. New binary linear pro- gramming formulation to compute the graph edit distance. Pattern Recognit., 72:254–265, 2017.Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph Matching Networks for Learning the Similarity of Graph Structured Objects. In ICML, 2019. Hermina Petric Maretic, Mireille El Gheche, Giovanni Chierchia, and Pascal Frossard. GOT: An Optimal Trans- port framework for Graph comparison. In NeurIPS, 2019. Cheng Meng, Yuan Ke, Jingyi Zhang, Mengrui Zhang, Wenxuan Zhong, and Ping Ma. Large-scale optimal trans- port map estimation using projection pursuit. In NeurIPS, 2019. Arthur Mensch and Gabriel Peyré. Online Sinkhorn: Opti- mal Transport distances from sample streams. InNeurIPS, 2020. Cameron Musco and Christopher Musco. Recursive Sam- pling for the Nystrom Method. In NeurIPS, 2017. Michel Neuhaus and Horst Bunke. An Error-Tolerant Ap- proximate Matching Algorithm for Attributed Planar Graphs and Its Application to Fingerprint Classiﬁcation. In Structural, Syntactic, and Statistical Pattern Recogni- tion, 2004. Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis. Matching Node Embeddings for Graph Similarity. In AAAI, 2017. David Nistér and Henrik Stewénius. Scalable Recognition with a V ocabulary Tree. InCVPR, 2006. Nicolas Papadakis, Gabriel Peyré, and Édouard Oudet. Opti- mal Transport with Proximal Splitting. SIAM J. Imaging Sciences, 7(1):212–238, 2014. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zem- ing Lin, Natalia Gimelshein, Luca Antiga, Alban Des- maison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin- tala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS, 2019. Loïc Paulevé, Hervé Jégou, and Laurent Amsaleg. Locality sensitive hashing: A comparison of hash function types and querying mechanisms. Pattern Recognit. Lett., 31 (11):1348–1358, 2010. Allon G Percus and Olivier C Martin. Scaling Universalities ofkth-Nearest Neighbor Distances on Closed Manifolds. Advances in Applied Mathematics, 21(3):424–436, 1998. Gabriel Peyré and Marco Cuturi. Computational Optimal Transport. Foundations and Trends in Machine Learning, 11(5-6):355–607, 2019. Julien Rabin, Gabriel Peyré, Julie Delon, and Marc Bernot. Wasserstein Barycenter and Its Application to Texture Mixing. In Scale Space and Variational Methods in Com- puter Vision (SSVM), 2011. Pau Riba, Andreas Fischer, Josep Lladós, and Alicia Fornés. Learning Graph Distances with Message Passing Neural Networks. In ICPR, 2018. Kaspar Riesen and Horst Bunke. IAM Graph Database Repository for Graph Based Pattern Recognition and Ma- chine Learning. In Structural, Syntactic, and Statistical Pattern Recognition, 2008. Kaspar Riesen and Horst Bunke. Approximate graph edit distance computation by means of bipartite graph match- ing. Image Vis. Comput., 27(7):950–959, 2009. Sebastian Ruder, Ivan Vulic, and Anders Søgaard. A Survey of Cross-lingual Word Embedding Models. J. Artif. Intell. Res., 65:569–631, 2019. Bernhard Schmitzer. Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems. SIAM Jour- nal on Scientiﬁc Computing, 41(3):A1443–A1481, 2019. Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS). In NeurIPS, 2014. Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. Memory Efﬁcient Kernel Approximation. Journal of Machine Learning Research, 18(20):1–32, 2017. Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Paciﬁc Journal of Mathematics, 21(2):343–348, 1967. Justin Solomon, Raif M. Rustamov, Leonidas J. Guibas, and Adrian Butscher. Earth mover’s distances on discrete surfaces. ACM Trans. Graph., 33(4):67:1–67:12, 2014. Justin Solomon, Fernando de Goes, Gabriel Peyré, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas J. Guibas. Convolutional wasserstein distances: efﬁcient optimal transportation on geometric domains. ACM Trans. Graph., 34(4):66:1–66:11, 2015. Computer Graphics Laboratory Stanford. The Stanford 3D Scanning Repository, 2014. URL http://graphics. stanford.edu/data/3Dscanrep/. Robert E. Tarjan. Dynamic trees as search trees via euler tours, applied to the network simplex algorithm. Mathe- matical Programming, 78(2):169–177, 1997. Evgeny Tenetov, Gershon Wolansky, and Ron Kimmel. Fast Entropic Regularized Optimal Transport Using Semidis- crete Cost Approximation. SIAM J. Sci. Comput., 40(5): A3400–A3422, 2018.Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In NeurIPS, 2017. Titouan Vayer, Nicolas Courty, Romain Tavenard, Laetitia Chapel, and Rémi Flamary. Optimal Transport for struc- tured data with application on graphs. In ICML, 2019. Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for Similarity Search: A Survey. arXiv, 1408.2927, 2014. Runzhong Wang, Junchi Yan, and Xiaokang Yang. Learn- ing Combinatorial Embedding Networks for Deep Graph Matching. In ICCV, 2019. Christopher K. I. Williams and Matthias Seeger. Using the Nyström Method to Speed Up Kernel Machines. In NeurIPS, 2001. Bing Xiao, Xinbo Gao, Dacheng Tao, and Xuelong Li. HMM-based graph edit distance for image indexing. Int. J. Imaging Systems and Technology , 18(2-3):209–218, 2008. Vinícius Flores Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David P. Reichert, Timothy P. Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, and Peter W. Battaglia. Deep reinforcement learning with relational inductive bi- ases. In ICLR, 2019. Kai Zhang, Ivor W. Tsang, and James T. Kwok. Improved Nyström low-rank approximation and error analysis. In ICML, 2008.Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More A. Complexity analysis Sparse Sinkhorn. A common way of achieving a high p1 and low p2 in LSH is via the AND-OR construction. In this scheme we calculate B ·r hash functions, di- vided into B sets (hash bands) of r hash functions each. A pair of points is considered as neighbors if any hash band matches completely. Calculating the hash buckets for all points with b hash buckets per function scales as O((n+ m)dBbr) for the hash functions we consider. As expected, for the tasks and hash functions we investigated we obtain approximately m/br and n/br neighbors, with br hash buckets per band. Using this we can ﬁx the num- ber of neighbors to a small, constant βin expectation with br = min(n,m)/β. We thus obtain a sparse cost matrix Csp with O(max(n,m)β) non-inﬁnite values and can cal- culate sand tin linear time O(Nsink max(n,m)β), where Nsink ≤2 + −4 ln(mini,j{˜Kij|˜Kij>0}mini,j{pi,qj}) ε (see The- orem 4) denotes the number of Sinkhorn iterations. Calcu- lating the hash buckets with r = log min(n,m)−log β log b takes O((n+ m)dBb(log min(n,m) −log β)/log b). Since B, b, and βare small, we obtain roughly log-linear scaling with the number of points overall, i.e. O(nlog n) for n≈m. LCN-Sinkhorn. Both choosing landmarks via k-means++ sampling and via k-means with a ﬁxed number of itera- tions have the same runtime complexity of O((n+ m)ld). Precomputing Wcan be done in timeO(nl2 +l3). The low- rank part of updating the vectorssand tcan be computed in O(nl+ l2 + lm), with lchosen constant, i.e. independently of n and m. Since sparse Sinkhorn with LSH has a log- linear runtime we again obtain log-linear overall runtime for LCN-Sinkhorn. B. Limitations Sparse Sinkhorn. Using a sparse approximation for K works well in the common case when the regularization parameter λ is low and the cost function varies enough between data pairs, such that the transport planP resembles a sparse matrix. However, it can fail if the cost between pairs is very similar or the regularization is very high, if the dataset contains many hubs, i.e. points with a large number of neighbors, or if the distributions por qare spread very unevenly. Furthermore, sparse Sinkhorn can be too unstable to train a model from scratch, since randomly initialized embeddings often have no close neighbors (see Sec. 8). Note also that LSH requires the cost function to be associated with a metric space, while regular Sinkhorn can be used with arbitrary costs. Note that we are only interested in an approximate solution with ﬁnite error ε. We therefore do not need the kernel matrix to be fully indecomposable or have total support, which would be necessary and sufﬁcient for a unique (up to a scalar factor) and exact solution, respectively (Sinkhorn & Knopp, 1967). However, the sparse approximation is not guaranteed to have support (Def. 1), which is necessary and sufﬁcient for the Sinkhorn algorithm to converge. The ap- proximated matrix is actually very likely not to have support if we use one LSH bucket per sample. This is due to the non- quadratic block structure resulting from every point only having non-zero entries for points in the other data set that fall in the same bucket. We can alleviate this problem by using unbalanced OT, as proposed in Sec. 6, or (empirically) the AND-OR construction. We can also simply choose to ignore this as long as we limit the maximum number of Sinkhorn iterations. On the 3D point cloud and random data experiments we indeed ignored this issue and actually observed good performance. Experiments with other LSH schemes and the AND-OR construction showed no perfor- mance improvement despite the associated cost matrices having support. Not having support therefore seems not to be an issue in practice, at least for the data we investigated. LCN-Sinkhorn. The LCN approximation is guaranteed to have support due to the Nyström part. Other weak spots of sparse Sinkhorn, such as very similar cost between pairs, high regularization, or data containing many hubs, are also usually handled well by the Nyström part of LCN. Highly concentrated distributions pand qcan still have adverse effects on LCN-Sinkhorn. We can compensate for these by sampling landmarks or neighbors proportional to each point’s probability mass. The Nyström part of LCN also has its limits, though. If the regularization parameter is low or the cost function varies greatly, we observed stability issues (over- and un- derﬂows) of the Nyström approximation because of the inverse A−1, which cannot be calculated in log-space. The Nyström approximation furthermore is not guaranteed to be non-negative, which can lead to catastrophic failures if the matrix product in Eq. (2) becomes negative. In these extreme cases we also observed catastrophic elimination with the correction Ksp ∆. Since a low entropy regularization essentially means that optimal transport is very local, we recommend using sparse Sinkhorn in these scenarios. This again demonstrates the complementarity of the sparse ap- proximation and Nyström: In cases where one fails we can often resort to the other. C. Proof of Theorem 1 By linearity of expectation we obtain E[Ki,ik −KNys,i,ik] = E[Ki,ik] −E[KNys,i,ik] = E[e−δk/λ] −E[KNys,i,ik] (20) with the distance to the kth-nearest neighbor δk. Note that without loss of generality we can assume unit manifoldScalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More volume and obtain the integral resulting from the ﬁrst expec- tation as (ignoring boundary effects that are exponentially small in n, see Percus & Martin (1998)) E[e−δk/λ]≈ n! (n−k)!(k−1)! ∫((d/2)!)1/d√π 0 e−r/λVd(r)k−1(1−Vd(r))n−k∂Vd(r) ∂r dr, (21) with the volume of the d-ball Vd(r) = πd/2rd (d/2)! . (22) Since this integral does not have an analytical solution we can either calculate it numerically or lower bound it us- ing Jensen’s inequality (again ignoring exponentially small boundary effects) E[e−δk/λ] ≥e−E[δk]/λ≈exp ( −((d/2)!)1/d √πλ (k−1 + 1/d)! (k−1)! n! (n+ 1/d)! ) . (23) To upper bound the second expectationE[KNys,i,ik] we now denote the distance between two points by ria = ∥xpi − xa∥2, the kernel by kia = e−ria/λ and the inter-landmark kernel matrix by KL. We ﬁrst consider p(xj |xj is kth-nearest neighbor) = = ∫ p(δk = rij |xi,xj)p(xi)p(xj) dxi = ∫ p(δk = rij |rij)p(rij |xj) drijp(xj) = ∫ p(δk = rij |rij)p(rij) drijp(xj) = ∫ p(δk = rij) drijp(xj) = p(xj) = p(xi), (24) where the third step is due to the uniform distribution. Since landmarks are more than 2Rapart we can approximate K−1 L = (Il + 1l×lO(e−2R/λ))−1 = In−1l×lO(e−2R/λ), (25) where 1l×l denotes the constant 1 matrix, with the number of landmarks l. We can now use (1) the fact that landmarks are arranged apriori, (2) Hölder’s inequality, (3) Eq. (24), and (4) Eq. (25) to obtain E[KNys,i,ik] = E [ l∑ a=1 l∑ b=1 kia(K−1 L )abkikb ] (1) = l∑ a=1 l∑ b=1 (K−1 L )abE[kiakikb] (2) ≤ l∑ a=1 l∑ b=1 (K−1 L )abE[k2 ia]1/2E[k2 ikb]1/2 (3) = l∑ a=1 l∑ b=1 (K−1 L )abE[k2 ia]1/2E[k2 ib]1/2 (4) = l∑ a=1 E[k2 ia] −O(e−2R/λ). (26) Since landmarks are more than 2Rapart we have VM ≥ lVd(R), where VM denotes the volume of the manifold. Assuming Euclideanness in Vd(R) we can thus use the fact that data points are uniformly distributed to obtain E[k2 ia] = E[e−2ria/λ] = 1 VM ∫ e−2r/λ∂Vd(r) ∂r dr ≤ 1 lVd(R) ∫ e−2r/λ∂Vd(r) ∂r dr = 1 lVd(R) ∫ R 0 e−2r/λ∂Vd(r) ∂r dr+ O(e−2R/λ) = d lRd ∫ R 0 e−2r/λrd−1 dr+ O(e−2R/λ) = d(Γ(d) −Γ(d,2R/λ)) l(2R/λ)d + O(e−2R/λ) (27) and ﬁnally E[KNys,i,ik] ≤ l∑ a=1 E[k2 ia] −O(e−2R/λ) ≤d(Γ(d) −Γ(d,2R/λ)) (2R/λ)d + O(e−2R/λ). (28) D. Proof of Theorem 2 We ﬁrst prove two lemmas that will be useful later on. Lemma A. Let ˜K be the Nyström approximation of the similarity matrix Kij = e−∥xi−xj∥2/λ, with all Nyström landmarks being at least Dapart and data samples beingScalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More no more than raway from its closest landmark. Then ˜Kij = ˜K2L ij + O(e−2 max(D−r,D/2)/λ), (29) where ˜K2L denotes the Nyström approximation using only the two landmarks closest to the points xi and xj. Proof. We denote the landmarks closest to the two points iand j with the indices aand b, or jointly with A, and all other landmarks with C. We furthermore denote the kernel between the point iand the point aas kia = e−∥xa−xj∥2/λ and the vector of kernels between a set of points A and a point ias kAi. We can split up A−1 used in the Nyström approximation ˜K= UA−1V, (30) where Acd = kcd, Uic = kic, and Vdj = kdj, into relevant blocks via A−1 = (A2L B BT Aother )−1 = (A−1 2L +A−1 2LB(A/A2L)−1BTA−1 2L −A−1 2LB(A/A2L)−1 −(A/A2L)−1BTA−1 2L (A/A2L)−1 ) , (31) where A/A2L = Aother −BTA−1 2L B denotes the Schur complement. We can thus write the entries of the Nyström approximation as ˜Kij = kT AiA−1 2L kAj + kT AiA−1 2L B(A/A2L)−1BTA−1 2L kAj −kT AiA−1 2L B(A/A2L)−1kCj −kT Ci(A/A2L)−1BTA−1 2L kAj + kT Ci(A/A2L)−1kCj = ˜K2L ij + (kT Ci −kT AiA−1 2L B) (Aother −BTA−1 2L B)−1 (kCj −BTA−1 2L kAj). (32) Interestingly, the difference to ˜K2L ij is again a Nyström ap- proximation where each factor is the difference between the correct kernel (e.g. kCj) and the previous Nyström approxi- mation of this kernel (e.g. BTA−1 2L kAj). We next bound the inverse, starting with BTA−1 2L B=(kCa kCb ) 1 1−k2ab ( 1 −kab −kab 1 )(kTCa kTCb ) = 1 1−k2ab (kCakTCa−kabkCakTCb−kabkCbkTCa+kCbkTCb ) =1l−2×l−2(1 +O(e−2D/λ))·4O(e−2D/λ) =1l−2×l−2O(e−2D/λ), (33) where 1l−2×l−2 denotes the constant 1 matrix, with the num- ber of landmarks l. The last steps use the fact that landmarks are more than D apart and 0 ≤k ≤1 for all k. For this reason we also have Aother = Il−2 + 1l−2×l−2O(e−D/λ) and can thus use the Neumann series to obtain (Aother −BTA−1 2L B)−1 = (Il−2 + 1l−2×l−2O(e−D/λ))−1 = Il−2 −1l−2×l−2O(e−D/λ). (34) We can analogously bound the other terms in Eq. (32) to obtain ˜Kij = ˜K2L ij + (kT Ci −11×l−2O(e−D/λ)) (Il−2 −1l−2×l−2O(e−D/λ)) (kCj −1l−2×1O(e−D/λ)) (1) = ˜K2L ij + kT CikCj + O(e−(D+max(D−r,D/2))/λ) = ˜K2L ij + ∑ 1≤k≤l k̸=a,b e−(∥xi−xk∥2+∥xk−xj∥2)/λ + O(e−(D+max(D−r,D/2))/λ) (2) ≤ ˜K2L ij + de−2 max(D−r,D/2)/λ + O(e−max(2(D−r),(1+ √ 3)D/2)/λ) = ˜K2L ij + O(e−2 max(D−r,D/2)/λ), (35) where ddenotes the dimension of x. Step (1) follows from the fact that any points’ second closest landmarks must be at least max(D−r,D/2) away (since landmarks are at least Dapart). This furthermore means that any point can have at most dsecond closest landmarks at this distance, which we used in step (2). Lemma B. Let ˜K be the Nyström approximation of the similarity matrix Kij = e−∥xi−xj∥2/λ. Let xi and xj be data points with equal L2 distance ri and rj to all l landmarks, which have the same distance ∆ > 0 to each other. Then ˜Kij = le−(ri+rj)/λ 1 + (l−1)e−∆/λ (36) Proof. The inter-landmark distance matrix is A= e−∆/λ1l×l + (1 −e−∆/λ)Il, (37) where 1l×l denotes the constant 1 matrix. Using the identity (b1n×n+ (a−b)In)−1 = −b (a−b)(a+ (n−1)b)1n×n+ 1 a−bIn (38)Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More we can compute ˜Kij=Ui,:A−1V:,j =(e−ri/λe−ri/λ···)( −e−∆/λ (1−e−∆/λ)(1 + (l−1)e−∆/λ)1l×l+ 11−e−∆/λIl )  e−rj/λ e−rj/λ ...   =e−(ri+rj)/λ 1−e−∆/λ ( −l2e−∆/λ 1 + (l−1)e−∆/λ+l ) =e−(ri+rj)/λ 1−e−∆/λ l−le−∆/λ 1 + (l−1)e−∆/λ = le−(ri+rj)/λ 1 + (l−1)e−∆/λ. (39) Moving on to the theorem, ﬁrst note that it analyzes the maximum error realizable under the given constraints, not an expected error. Ksp is correct for all pairs inside a cluster and 0 otherwise. We therefore obtain the maximum error by considering the closest possible pair between clusters. By deﬁnition, this pair has distance D−2rand thus max xpi,xqj K−Ksp = e−(D−2r)/λ (40) LCN is also correct for all pairs inside a cluster, so we again consider the closest possible pair xi, xj between clusters. We furthermore use Lemma A to only consider the landmarks of the two concerned clusters, adding an error of O(e−2(D−r)/λ), since r≪D. Hence, K2LLCN,ij=(e−r/λ e−(D−r)/λ)( 1 e−D/λ e−D/λ 1 )−1(e−(D−r)/λ e−r/λ ) = 1 1−e−2D/λ (e−r/λ e−(D−r)/λ)( 1 −e−D/λ −e−D/λ 1 )(e−(D−r)/λ e−r/λ ) = 1 1−e−2D/λ (e−r/λ e−(D−r)/λ)(e−(D−r)/λ−e−(D+r)/λ) e−r/λ−e−(2D−r)/λ ) = 1 1−e−2D/λ(e−D/λ−e−(D+2r)/λ+e−D/λ−e−(3D−2r)/λ) = e−D/λ 1−e−2D/λ(2−e−2r/λ−e−(2D−2r)/λ) =e−D/λ(2−e−2r/λ)−O(e−2(D−r)/λ) (41) and thus max xpi,xqj K−KLCN = e−(D−2r)/λ(1 −e−2r/λ(2 −e−2r/λ) + O(e−2D/λ)). (42) For pure Nyström we need to consider the distances inside a cluster. In the worst case two points overlap, i.e. Kij = 1, and lie at the boundary of the cluster. Since r ≪D we again use Lemma A to only consider the landmark in the concerned cluster, adding an error of O(e−2(D−r)/λ). KNys,ij = e−2r/λ + O(e−2(D−r)/λ) (43) Note that when ignoring the effect from other clusters we can generalize the Nyström error to l ≤dlandmarks per cluster. In this case, because of symmetry we can opti- mize the worst-case distance from all cluster landmarks by putting them on an (l−1)-simplex centered on the cluster center. Since there are at most dlandmarks in each clus- ter there is always one direction in which the worst-case points are r away from all landmarks. The circumradius of an (l−1)-simplex with side length ∆ is √ l−1 2l ∆. Thus, the maximum distance to all landmarks is √ r2 + l−1 2l ∆2. Using Lemma B we therefore obtain the Nyström approxi- mation Kmulti Nys,ij = le−2 √ r2+ l−1 2l ∆2/λ 1 + (l−1)e−∆/λ + O(e−2(D−r)/λ) (44) E. Notes on Theorem 3 Lemmas C-F and and thus Theorem 1 by Altschuler et al. (2019) are also valid for Qoutside the simplex so long as ∥Q∥1 = ∑ i,j|Qij|= nand it only has non-negative en- tries. Any ˜P returned by Sinkhorn fulﬁlls these conditions if the kernel matrix is non-negative and has support. There- fore the rounding procedure given by their Algorithm 4 is not necessary for this result. Furthermore, to be more consistent with Theorems 1 and 2 we use the L2 distance instead of L2 2 in this theorem, which only changes the dependence on ρ. F. Notes on Theorem 4 To adapt Theorem 1 by Dvurechensky et al. (2018) to sparse matrices (i.e. matrices with some Kij = 0 ) we need to redeﬁne ν := min i,j {Kij|Kij >0}, (45) i.e. take the minimum only w.r.t. non-zero elements in their Lemma 1. We furthermore need to consider sums exclu- sively over these non-zero elements instead of the full 1 vector in their Lemma 1. The Sinkhorn algorithm converges since the matrix has support (Sinkhorn & Knopp, 1967). However, the point it converges to might not exist because we only require support, not total support. Therefore, we need to consider slightly perturbed optimal vectors for the proof, i.e. deﬁne a negligibly small ˜ε≪ε,ε′for which |B(u∗,v∗)1 −r|≤ ˜ε, |B(u∗,v∗)T1 −c|≤ ˜ε. Support furthermore guarantees that no row or column is completely zero, thus preventing any unconstrained uk or vk, and any non-converging row or column sum of B(uk,vk). With these changes in place all proofs work the same as in the dense case.Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More G. Proof of Proposition 1 Theorem A (Danskin’s theorem). Consider a continuous function φ: Rk ×Z →R, with the compact set Z ⊂Rj. If φ(x,z) is convex in xfor every z∈Z and φ(x,z) has a unique maximizer ¯z, the derivative of f(x) = max z∈Z φ(x,z) (46) is given by the derivative at the maximizer, i.e. ∂f ∂x = ∂φ(x,¯z) ∂x . (47) We start by deriving the derivatives of the distances. To show that the Sinkhorn distance fulﬁlls the conditions for Danskin’s theorem we ﬁrst identify x= C, z = P, and φ(C,P) = −⟨P,C⟩F + λH(P). We next observe that the restrictions P1m = pand PT1n = qdeﬁne a compact, convex set for P. Furthermore, φis a continuous function and linear in C, i.e. both convex and concave for any ﬁnite P. Finally, φ(C,P) is concave in P since ⟨P,C⟩F is linear and λH(P) is concave. Therefore the maximizer ¯P is unique and Danskin’s theorem applies to the Sinkhorn distance. Using ∂CNys,ij ∂Ukl = ∂ ∂Ukl ( −λlog( ∑ a UiaWaj) ) = −λδik Wlj∑ aUiaWaj = −λδik Wlj KNys,ij , (48) ∂CNys,ij ∂Wkl = ∂ ∂Wkl ( −λlog( ∑ a UiaWaj) ) = −λδjl Uik∑ aUiaWaj = −λδjl Uik KNys,ij , (49) ¯PNys,ij KNys,ij = ∑ b ¯PU,ib¯PW,bj∑ aUiaWaj = ¯si¯tj ∑ bUibWbj∑ aUiaWaj = ¯si¯tj ∑ bUibWbj∑ aUiaWaj = ¯si¯tj (50) and the chain rule we can calculate the derivative w.r.t. the cost matrix as ∂dλ c ∂C = − ∂ ∂C ( −⟨¯P,C⟩F + λH( ¯P) ) = ¯P, (51) ∂dλ LCN,c ∂Ukl = ∑ i,j ∂CNys,ij ∂Ukl ∂dλ LCN,c ∂CNys,ij = −λ ∑ i,j δikWlj ¯PNys,ij KNys,ij = −λ ∑ i,j δikWlj¯si¯tj = −λ¯sk ∑ j Wlj¯tj = (−λ¯s(W¯t)T) kl, (52) ∂dλ LCN,c ∂Wkl = ∑ i,j ∂CNys,ij ∂Wkl ∂dλ LCN,c ∂CNys,ij = −λ ∑ i,j δjlUik ¯PNys,ij KNys,ij = −λ ∑ i,j δjlUik¯si¯tj = −λ (∑ i ¯siUik ) ¯tl = (−λ(¯sTU)T¯tT) kl, (53) and ∂dλ LCN,c ∂log Ksp and ∂dλ LCN,c ∂log Ksp Nys follow directly from ∂dλ c ∂C . We can then backpropagate in timeO((n+m)l2) by computing the matrix-vector multiplications in the right order. H. Choosing LSH neighbors and Nyström landmarks We focus on two LSH methods for obtaining near neighbors. Cross-polytope LSH (Andoni et al., 2015) uses a random projection matrix R ∈Rd×b/2 with the number of hash buckets b, and then decides on the hash bucket via h(x) = arg max([xTR∥−xTR]), where ∥denotes concatenation. k-means LSH computes k-means and uses the clusters as hash buckets. We further improve the sampling probabilities of cross- polytope LSH via the AND-OR construction. In this scheme we calculate B·rhash functions, divided into Bsets (hash bands) of r hash functions each. A pair of points is con- sidered as neighbors if any hash band matches completely. k-means LSH does not work well with the AND-OR con- struction since its samples are highly correlated. For large datasets we use hierarchical k-means instead (Paulevé et al., 2010; Nistér & Stewénius, 2006). The 3D point clouds, uniform data and the graph transport network (GTN) use the L2 distance between embeddings as a cost function. For these we use (hierarchical) k-means LSH and k-means Nyström in both sparse Sinkhorn and LCN-Sinkhorn. Word embedding similarities are measured via a dot product. In this case we use cross-polytope LSH for sparse Sinkhorn in this case. For LCN-Sinkhorn we found that using k- means LSH works better with Nyström using k-means++ sampling than cross-polytope LSH. This is most likely due to a better alignment between LSH samples and Nyström. We convert the cosine similarity to a distance via dcos =√ 1 − xTp xq ∥xp∥2∥xq∥2 (Berg et al., 1984) to use k-means with dot product similarity. Note that this is actually based on cosine similarity, not the dot product. Due to the balanced nature of OT we found this more sensible than maximum inner product search (MIPS). For both experiments we also experimented with uniform and recursive RLS sampling but found that the above mentioned methods work better.Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More I. Implementational details Our implementation runs in batches on a GPU via Py- Torch (Paszke et al., 2019) and PyTorch Scatter (Fey & Lenssen, 2019). To avoid over- and underﬂows we use log-stabilization throughout, i.e. we save all values in log- space and compute all matrix-vector products and addi- tions via the log-sum-exp trick log ∑ iexi = max jxj + log(∑ iexi−maxjxj). Since the matrix Ais small we com- pute its inverse using double precision to improve stability. Surprisingly, we did not observe any beneﬁt from using the Cholesky decomposition or not calculating A−1 and instead solving the equation B = AX for X. We fur- thermore precompute W = A−1V to avoid unnecessary operations. We use 3 layers and an embedding size HN = 32 for GTN. The MLPs use a single hidden layer, biases and LeakyReLU non-linearities. The single-head MLP uses an output size of HN, match = HN and a hidden embedding size of 4HN, i.e. the same as the concatenated node embedding, and the multi- head MLP uses a hidden embedding size ofHN. To stabilize initial training we scale the node embeddings by ¯d ¯n √ HN, match directly before calculating OT. ¯ddenotes the average graph distance in the training set, ¯nthe average number of nodes per graph, and HN, match the matching embedding size, i.e. 32 for single-head and 128 for multi-head OT. For the graph datasets, the 3D point clouds and random data we use the L2 distance for the cost function. For word embedding alignment we use the dot product, since this best resembles their generation procedure. J. Graph dataset generation and experimental details The dataset statistics are summarized in Table 5. Each dataset contains the distances between all graph pairs in each split, i.e. 10 296 and 1128 distances for preferential attachment. The AIDS dataset was generated by randomly sampling graphs with at most 30 nodes from the original AIDS dataset (Riesen & Bunke, 2008). Since not all node types are present in the training set and our choice of GED is permutation-invariant w.r.t. types, we permuted the node types so that there are no previously unseen types in the vali- dation and test sets. For the preferential attachment datasets we ﬁrst generated 12, 4, and 4 undirected “seed” graphs (for train, val, and test) via the initial attractiveness model with randomly chosen parameters: 1 to 5 initial nodes, ini- tial attractiveness of 0 to 4 and 1/2¯nand 3/2¯ntotal nodes, where ¯nis the average number of nodes (20, 200, 2000, and 20 000). We then randomly label every node (and edge) in these graphs uniformly. To obtain the remaining graphs we edit the “seed” graphs between¯n/40 and ¯n/20 times by ran- domly adding, type editing, or removing nodes and edges. Editing nodes and edges is 4x and adding/deleting edges 3x as likely as adding/deleting nodes. Most of these numbers were chosen arbitrarily, aiming to achieve a somewhat rea- sonable dataset and process. We found that the process of ﬁrst generating seed graphs and subsequently editing these is crucial for obtaining meaningfully structured data to learn from. For the GED we choose an edit cost of 1 for changing a node or edge type and 2 for adding or deleting a node or an edge. We represent node and edge types as one-hot vectors. We train all models except SiamMPNN (which uses SGD) and GTN on Linux with the Adam optimizer and mean squared error (MSE) loss for up to 300 epochs and reduce the learn- ing rate by a factor of 10 every 100 steps. On Linux we train for up to 1000 epochs and reduce the learning rate by a factor of 2 every 100 steps. We use the parameters from the best epoch based on the validation set. We choose hyperparameters for all models using multiple steps of grid search on the validation set, see Tables 6 to 8 for the ﬁnal values. We use the originally published result of SimGNN on Linux and thus don’t provide its hyperparameters. GTN uses 500 Sinkhorn iterations. We obtain the ﬁnal entropy regularization parameter from λbase via λ = λbase ¯d ¯n 1 log n, where ¯ddenotes the average graph distance and ¯nthe av- erage number of nodes per graph in the training set. The factor ¯d/¯nserves to estimate the embedding distance scale and 1/log ncounteracts the entropy scaling with nlog n. Note that the entropy regularization parameter was small, but always far from 0, which shows that entropy regulariza- tion actually has a positive effect on learning. On the pref. att. 200 dataset we use no L2 regularization, λbase = 0.5, and a batch size of 200. For pref. att. 2k we use λbase = 2 and a batch size of 20 for full Sinkhorn and 100 for LCN- Sinkhorn. For pref. att. 20k we use λbase = 50 and a batch size of 4. λbase scales with graph size due to normalization of the PM kernel. For LCN-Sinkhorn we use roughly 10 neighbors for LSH (20 k-means clusters) and 10 k-means landmarks for Nys- tröm on pref. att. 200. We double these numbers for pure Nyström Sinkhorn, sparse Sinkhorn, and multiscale OT. For pref. att. 2k we use around 15 neighbors ( 10 ·20 hierar- chical clusters) and 15 landmarks and for pref. att. 20k we use roughly 30 neighbors (10 ·10 ·10 hierarchical clusters) and 20 landmarks. The number of neighbors for the 20k dataset is higher and strongly varies per iteration due to the unbalanced nature of hierarchical k-means. This increase in neighbors and landmarks and PyTorch’s missing support for ragged tensors largely explains LCN-Sinkhorn’s deviation from perfectly linear runtime scaling. We perform all runtime measurements on a compute node using one Nvidia GeForce GTX 1080 Ti, two Intel Xeon E5-2630 v4, and 256GB RAM.Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More Table 5: Graph dataset statistics. Distance (test set) Graphs Avg. nodes Avg. edges Node Edge Graph type Distance Mean Std. dev. train/val/test per graph per graph types types AIDS30 Molecules GED 50.5 16.2 144/48/48 20.6 44.6 53 4 Linux Program dependence GED 0.567 0.181 600/200/200 7.6 6.9 7 - Pref. att. Initial attractiveness GED 106.7 48.3 144/48/48 20.6 75.4 6 4 Pref. att. 200 Initial attractiveness PM 0.400 0.102 144/48/48 199.3 938.8 6 - Pref. att. 2k Initial attractiveness PM 0.359 0.163 144/48/48 2045.6 11330 6 - Pref. att. 20k Initial attractiveness PM 0.363 0.151 144/48/48 20441 90412 6 - Table 6: Hyperparameters for the Linux dataset. lr batchsize layers emb. size L2 reg. λbase SiamMPNN 1 ×10−4 256 3 32 5 ×10−4 - GMN 1 ×10−4 20 3 64 0 - GTN, 1 head 0.01 1000 3 32 1 ×10−6 1.0 8 OT heads 0.01 1000 3 32 1 ×10−6 1.0 Balanced OT 0.01 1000 3 32 1 ×10−6 2.0 Table 7: Hyperparameters for the AIDS dataset. lr batchsize layers emb. size L2 reg. λbase SiamMPNN 1 ×10−4 256 3 32 5 ×10−4 - SimGNN 1 ×10−3 1 3 32 0.01 - GMN 1 ×10−2 128 3 32 0 - GTN, 1 head 0.01 100 3 32 5 ×10−3 0.1 8 OT heads 0.01 100 3 32 5 ×10−3 0.075 Balanced OT 0.01 100 3 32 5 ×10−3 0.1 Nyström 0.015 100 3 32 5 ×10−3 0.2 Multiscale 0.015 100 3 32 5 ×10−3 0.2 Sparse OT 0.015 100 3 32 5 ×10−3 0.2 LCN-OT 0.015 100 3 32 5 ×10−3 0.2 Table 8: Hyperparameters for the preferential attachment GED dataset. lr batchsize layers emb. size L2 reg. λbase SiamMPNN 1 ×10−4 256 3 64 1 ×10−3 - SimGNN 1 ×10−3 4 3 32 0 - GMN 1 ×10−4 20 3 64 0 - GTN, 1 head 0.01 100 3 32 5 ×10−4 0.2 8 OT heads 0.01 100 3 32 5 ×10−3 0.075 Balanced OT 0.01 100 3 32 5 ×10−4 0.2 Nyström 0.02 100 3 32 5 ×10−5 0.2 Multiscale 0.02 100 3 32 5 ×10−5 0.2 Sparse OT 0.02 100 3 32 5 ×10−5 0.2 LCN-OT 0.02 100 3 32 5 ×10−5 0.2Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More 0 100 200 Neighbors + landmarks 0 100 200 300Runtime (ms) Multsc. OT Nys. Skh. Sparse Skh. LCN-Skh. Figure 3: Runtime scales linearly with the number of neigh- bors/landmarks for all relevant Sinkhorn approximation methods. K. Runtimes Table 9 compares the runtime of the full Sinkhorn dis- tance with different approximation methods using 40 neigh- bors/landmarks. We separate the computation of approxi- mate Kfrom the optimal transport computation (Sinkhorn iterations), since the former primarily depends on the LSH and Nyström methods we choose. We observe a 2-4x speed difference between sparse (multiscale OT and sparse Sinkhorn) and low-rank approximations (Nyström Sinkhorn and LCN-Sinkhorn), while factored OT is multiple times slower due to its iterative reﬁnement scheme. In Fig. 3 we observe that this runtime gap stays constant indepen- dent of the number of neighbors/landmarks, i.e. the relative difference decreases as we increase the number of neigh- bors/landmarks. This gap could either be due to details in low-level CUDA implementations and hardware or the fact that low-rank approximations require 2x as many multiplica- tions for the same number of neighbors/landmarks. In either case, both Table 9 and Fig. 3 show that the runtimes of all approximations scale linearly both in the dataset size and the number of neighbors and landmarks, while full Sinkhorn scales quadratically. We furthermore investigate whether GTN with approximate Sinkhorn indeed scales log-linearly with the graph size by generating preferential attachment graphs with 200, 2000, and 20 000 nodes (±50 %). We use the Pyramid match- ing (PM) kernel (Nikolentzos et al., 2017) as prediction target. Fig. 4 shows that the runtime of LCN-Sinkhorn scales almost linearly (dashed line) and regular full Sinkhorn quadraticly (dash-dotted line) with the number of nodes, despite both achieving similar accuracy and LCN using slightly more neighbors and landmarks on larger graphs to sustain good accuracy. Full Sinkhorn went out of memory for the largest graphs. 100 1000 10000 Avg. graph size 10 100 1000 10000Time per epoch (s) Full LCN Figure 4: Log-log runtime per epoch for GTN with full Sinkhorn and LCN-Sinkhorn. LCN-Sinkhorn scales almost linearly with graph size while sustaining similar accuracy. L. Distance approximation Fig. 5 shows that for the chosen λ= 0.05 sparse Sinkhorn offers the best trade-off between computational budget and distance approximation, with LCN-Sinkhorn and multiscale OT coming in second. Factored OT is again multiple times slower than the other methods. Note that dλ c can be neg- ative due to the entropy offset. This picture changes as we increase the regularization. For higher regularizations LCN-Sinkhorn is the most precise at constant computa- tional budget (number of neighbors/landmarks). Note that the crossover points in this ﬁgure roughly coincide with those in Fig. 2. Keep in mind that usually the OT plan is more important than the raw distance approximation, since it determines the training gradient and tasks like embed- ding alignment don’t use the distance at all. This becomes evident in the fact that sparse Sinkhorn achieves a better distance approximation than LCN-Sinkhorn but performs worse in both downstream tasks investigated in Sec. 8.Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More Table 9: Runtimes (ms) of Sinkhorn approximations for EN-DE embeddings at different dataset sizes. Full Sinkhorn scales quadratically, while all approximationes scale at most linearly with the size. Sparse approximations are 2-4x faster than low-rank approximations, and factored OT is multiple times slower due to its iterative reﬁnement scheme. Note that similarity matrix computation time (K) primarily depends on the LSH/Nyström method, not the OT approximation. N = 10000 N = 20000 N = 50000 K OT K OT K OT Full Sinkhorn 8 2950 29 11 760 OOM OOM Factored OT 29 809 32 1016 55 3673 Multiscale OT 90 48 193 61 521 126 Nyström Skh. 29 135 41 281 79 683 Sparse Skh. 42 46 84 68 220 137 LCN-Sinkhorn 101 116 242 205 642 624 0 100 200 Runtime (ms) −5.0 −2.5 0.0 2.5 dλ c/103 0 100 200 Neighbors + landmarks −5 0 5 dλ c/103 Fact. OT Multsc. OT Nys. Skh. Sparse Skh. LCN-Skh. 10−3 10−2 10−1 100 λ 0.0 0.5 1.0 1.5 2.0 Rel. err. dλ c Figure 5: Sinkhorn distance approximation for different runtimes and computational budgets (both varied via the number of neighbors/landmarks), and entropy regularization parameters λ. The dashed line denotes the true Sinkhorn distance. The arrow indicates factored OT results far outside the depicted range. Left: Sparse Sinkhorn consistently performs best across all runtimes. Center: Sparse Sinkhorn mostly performs best, with LCN-Sinkhorn coming in second, and factored OT being seemingly independent from the number of neighbors. Right: Sparse Sinkhorn performs best for low λ, LCN-Sinkhorn for moderate and high λand factored OT for very high λ.",
      "meta_data": {
        "arxiv_id": "2107.06876v2",
        "authors": [
          "Johannes Gasteiger",
          "Marten Lienen",
          "Stephan Günnemann"
        ],
        "published_date": "2021-07-14T17:40:08Z",
        "pdf_url": "https://arxiv.org/pdf/2107.06876v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the scalability limitations of optimal transport (OT) in high-dimensional spaces by proposing two log-linear time approximations: Sparse Sinkhorn, based on locality-sensitive hashing (LSH), and Locally Corrected Nyström (LCN) Sinkhorn, which fuses sparse and Nyström approximations. These methods enable stable log-linear time computation of entropy-regularized OT. Furthermore, the authors introduce the Graph Transport Network (GTN), a siamese Graph Neural Network (GNN) that leverages multi-head unbalanced LCN-Sinkhorn to achieve state-of-the-art performance in graph distance regression, scaling log-linearly with the number of nodes. The approximations improve speed and accuracy in applications like unsupervised word embedding alignment.",
        "methodology": "The core methodology involves approximating the full pairwise cost matrix, which is prohibitively expensive for large datasets. Sparse Sinkhorn approximates the cost matrix (C) as sparse (Csp) by identifying 'near' neighbors using Locality-Sensitive Hashing (LSH) techniques (e.g., cross-polytope LSH, k-means LSH). LCN-Sinkhorn extends this by fusing the sparse approximation with the Nyström method, resulting in KLCN = KNys - Ksp_Nys + Ksp, where Ksp_Nys are Nyström approximation entries corresponding to the sparse indices. The Sinkhorn algorithm then runs with this approximate matrix, computing matrix-vector products efficiently without full instantiation. The Graph Transport Network (GTN) utilizes a Siamese GNN to embed graphs as sets of node embeddings. These sets are matched using an enhanced Sinkhorn distance calculation that incorporates learnable unbalanced OT (replacing the cost matrix with a bipartite matching matrix to handle differing node counts and learn deletion costs) and multi-head OT (using multiple parallel Sinkhorn heads with varying regularization parameters). Backpropagation is supported via automatic differentiation or analytic gradients.",
        "experimental_setup": "The methods are evaluated in two main phases: direct Sinkhorn approximation quality and end-to-end application performance. For direct approximation, experiments are conducted on 10,000 word embeddings (EN-DE, EN-ES), 10,000 subsampled points from 3D point clouds (armadillo and dragon), and 10,000 uniformly distributed points in a 16-dimensional d-ball. Metrics include relative Sinkhorn distance error, Pearson correlation coefficient (PCC), and Jaccard similarity (IoU) of the top 0.1% transport plan entries. For end-to-end applications, unsupervised word embedding alignment uses 20,000 word embeddings across multiple language pairs, evaluated by accuracy using CSLS. Graph distance regression experiments utilize the Linux dataset, AIDS30 dataset (small graphs with exact Graph Edit Distance, GED), and generated preferential attachment graphs (small, 200, 2k, 20k nodes, targeting GED or Pyramid Matching kernel for larger graphs). GTN is compared against SiameseMPNN, SimGNN, and GMN using RMSE. Experiments are performed on a compute node with an Nvidia GeForce GTX 1080 Ti GPU and Intel Xeon CPUs.",
        "limitations": "Sparse Sinkhorn can fail when the cost is very similar between pairs, regularization is high, datasets contain many hubs, or distributions are unevenly spread. It may also be too unstable for training models from scratch with randomly initialized embeddings. LSH necessitates a metric space for the cost function. The sparse approximation is not guaranteed to have 'support,' which is technically required for Sinkhorn convergence, though empirically not always an issue. LCN-Sinkhorn struggles with highly concentrated distributions (P and Q), and its Nyström part can face stability issues (over/underflows, non-negativity) if the regularization parameter is low or the cost function varies greatly, especially due to the inverse A^-1. These issues can lead to catastrophic failures. The authors note that PyTorch's missing support for ragged tensors influences LCN-Sinkhorn's deviation from perfectly linear runtime scaling on larger graphs.",
        "future_research_directions": "The paper explicitly states that both the proposed learnable unbalanced optimal transport and multi-head optimal transport components \"might be of independent interest,\" suggesting these as promising avenues for future research."
      }
    },
    {
      "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
      "abstract": "Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.",
      "full_text": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort Qualcomm AI Research∗ Amsterdam, The Netherlands {ybond, markusn, tijmen}@qti.qualcomm.com Abstract Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI signif- icantly. Due to their size, the capability of these networks has increased tremen- dously, but this has come at the cost of a significant increase in necessary com- pute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a “no-op” or just a partial update of the residual. To achieve the exact zeros needed in the attention matrix for a no-update, the input to the softmax is pushed to be larger and larger during training, causing outliers in other parts of the network. Based on these observations, we propose two sim- ple (independent) modifications to the attention mechanism - clipped softmax and gated attention . We empirically show that models pre-trained using our methods learn significantly smaller outliers while maintaining and sometimes even improving the floating-point task performance. This enables us to quantize transformers to full INT8 quantization of the activations without any additional effort. We demonstrate the effectiveness of our methods on both language models (BERT, OPT) and vision transformers. Our source code is available at https: //github.com/qualcomm-ai-research/outlier-free-transformers . 1 Introduction Quantization has been one of the most impactful ways to reduce the computational complexity of transformer networks. Previous work has shown that quantizing networks to 4-bit weights is possible without losing too much accuracy [66, 69]. Some research even shows 4-bit weights might be optimal when trading off model size and bit-width [12]. However, quantizing transformers is not always trivial. When quantizing the activations of a trans- former, significant problems arise with outliers in specific layers. This has been noted by several researchers that suggest fixes to transformers after training to ameliorate their effect [13, 67]. These methods are frequently tedious and either require retraining the network, require implementing specific hardware for input-channel quantization [13] or require parts of the activations to still be in higher bit-widths, reducing the effectiveness of the activation quantization [67]. In this paper, we set out to solve the transformer outlier problem entirely by changing the architecture of the network itself. We hope to make transformers easy to quantize from the get-go without needing ∗Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.12929v2  [cs.LG]  9 Nov 2023any post-processing. To do so, we thoroughly analyze why these outliers appear. Previous work has found the existence of these outliers [4, 13], but in our work, we come to a fuller understanding of these outlying values. We find that the outliers occur because attention heads are trying not to update the hidden state, and in the process, strong outliers appear due to the softmax function. This happens for language and vision transformers and different specific transformer architectures. This understanding is the foundation for two new tweaks we suggest to transformer architectures that can remove the problem of the outliers entirely. 2 Background and related work In this section, we briefly cover the basics of neural network quantization and discuss why modern transformers are difficult to quantize. Quantization One of the most powerful ways to decrease the computational time and memory consumption of neural networks is quantization, which uses low-bit representations for the weights and activation tensors. On top of that, using low-bit fixed-point representations, such as INT8, one can further reduce energy consumption since the fixed-point operations are more efficient than their floating-point counterparts [23, 59]. We simulate the quantization process in floating-point according to Jacob et al. [26]. We use the following definition of the quantization function: bx := q (x; s, z, b) =s · \u0010 clip \u0010jx s m + z; 0, 2b − 1 \u0011 − z \u0011 , (1) where x denotes the quantizer input (i.e., network weights or activations), s ∈ R+ the scale factor or the step-size, z ∈ Z the zero point, and b ∈ N the bitwidth. ⌊·⌉ denotes the round-to-nearest-integer operator. This quantization scheme is called uniform affine or asymmetric quantization [24, 32, 76] and it is one of the most commonly used quantization schemes because it allows for efficient implementation of fixed-point arithmetic. In the case of symmetric quantization, we restrict the quantization grid to be symmetric around z = 0. In this work, we focus on post-training quantization (PTQ) methods, which take a pre-trained FP32 network and convert it directly into a fixed-point network without the need for the original training pipeline [2, 5, 7, 25, 32, 35, 41, 43, 44, 75]. These methods require either no data or only a small calibration dataset and are easier to use compared to quantization-aware training (QAT, Bhalgat et al. 3, Esser et al. 16, Gupta et al. 21, Jacob et al. 26, Krishnamoorthi 32) methods that have you train the entire network for more epochs. For more details on neural network quantization, we refer the reader to [19, 46]. Outliers in Transformers Multiple studies have shown that modern transformer-based language models tend to learn outliers in weights and activations [4, 13, 31]. These outliers are present only in a small fixed set of embedding dimensions, but they appear regularly and consistently across multiple layers and data sequences. It was also shown that those outliers play a crucial role in the model predictions and clipping them or by setting to zero the corresponding parameters significantly degrades the model task performance [31, 49]. The strongest in magnitude outliers typically appear at the output of the feed-forward network, FFN, although Dettmers et al.[13] showed that for big enough transformer-based language models they start appearing after every linear layer, including query, key, and value projection layers. This phenomenon holds for many tasks, training objectives and models (both encoder and decoder transformers), including BERT [14], RoBERTa [37], DistilBERT [53], MobileBERT [55], ELECTRA [9], BART [33], XLNet [68], GPT-2 [50], and OPT [74]. Because of these strong outliers, applying per-tensor PTQ for the FFN’s output and the residual sum will likely cause a notable error because of the following trade-off between the range and the precision. On the one hand, using a large quantization range for small-ranged values leads to a loss in representation (high rounding error). On the other hand, a small quantization range for large values leads to a very high clipping error. For the case of significant transformer outliers, frequently, no good trade-off can be found between the rounding and clipping error, resulting in an overall high error. There have been numerous attempts to fix the issue of transformer quantization [4, 12, 13, 17, 27, 28, 51, 54, 62, 63, 69, 71]. Most of these approaches resort to finer quantization granularity (row-wise, 2(a) FFN output in layer #10  (b) FFN output in layer #11 Figure 1: Histograms of outlier counts vs. token positions (blue) and hidden dimensions (green), recorded from the MNLI-m validation set on BERT-base. We use zero-based indexing for dimensions. channel-wise, group-wise weight and activation quantization), use higher bitwidth and/or different numeric format to represent those outliers better or require extra fine-tuning (in the form of QAT and/or knowledge distillation). In other words, they adapt quantization to work with outliers, which often comes at the expense of general applicability or extra inference overhead. In contrast, in this work, we want to address the root cause of the problem and understand why outliers are learned in the first place and suggest a new pre-training protocol that significantly reduces the magnitude of outliers yielding way more quantization-friendly models that can be effortlessly quantized using PTQ without strong degradation of performance. 3 Outlier analysis Outliers in BERT models In Section 2 we discussed that outliers are present only in a few designated embedding dimensions but they appear regularly and consistently across multiple layers and data sequences. We also discussed that the strongest magnitude outliers in BERT typically appear at the output of FFN in the last encoder layers. We start by taking the pre-trained BERT-base-uncased checkpoint from HuggingFace [65] and fine- tune it on MNLI dataset from the well-known GLUE benchmark [ 61] (see experimental details in C.1). To identify the outlier dimensions, we pass the MNLI-m validation set through the network and record all outliers1 at the FFN output in layers #10 and #112. As we can see in Figure 1, there are indeed only a few hidden dimensions where outliers ever occur. We also notice that the majority of outliers (> 97%) correlate with the position of delimiter tokens – [SEP], “.”, and “,”. To better understand the role of those outliers, we analyze the attention patterns of the corresponding attention heads. BERT-base uses multi-head attention with nheads = 12and each head operating on a consecutive subset of dhead = 64features. Therefore, the hidden dimension #180, which happens to have the highest outlier count in both layers #10 and #11, corresponds to attention head #3. In Figure 2 (and more examples in Appendix A.1) we show examples of the attention matrices, values and their product for that head. A common pattern we found is that the attention head assigns almost all of its probability mass to [SEP] tokens, and other less informative tokens like dots/commas, while these tokens also have small values in V associated with those tokens. This results in a small magnitude product between the two (see Figure 2a). This effectively corresponds to a (soft) no-update of the hidden representation, where only small noise is added after the residual. In other cases (Figure 2b and 2c), we observe that a significant portion of attention probability is still spent on delimiter tokens. However, by allocating some of the probability mass on other tokens (together with the small values for the delimiter tokens), this results in a (soft) selective update of the hidden representation. These patterns in self-attention seem to be a learned “workaround” for the limitations of having the softmax and the residual connections in cases where the attention head does not want to update the representation of some or all of the tokens. These observations are in line with Clark et al. [8], Kovaleva et al. [30] that also argued that attending exclusively or almost exclusively to delimiter tokens such as [SEP], periods/commas acts as a “no-op” when the attention head’s function is not applicable. 1We follow Bondarenko et al. [4] and consider outliers as values that exceed 6 standard deviations from the mean of the corresponding activation tensor. 2We use 1-based indexing for encoder layers and attention heads throughout the paper. 3(a) Attention layer #11, data sequence #1 (b) Attention layer #11, data sequence #5 (c) Attention layer #10, data sequence #5 Figure 2: Visualization of the patterns in the self-attention, specifically the attention probabilities, values, and their product (left, middle and right columns, respectively), in attention head #3 for BERT-base, computed on several data sequences from MNLI-m validation set. (a)  (b)  (c)  (d)  (e) Figure 3: A summary of our outlier analysis for ViT demonstrated on a random image from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, layer #12. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values for outlier and non-outlier patches. Outliers in ViT We conduct a similar analysis for Vision transformer [15] trained on ImageNet [52]. For this study, we use a pre-trained checkpoint following our experimental setup from Section 5. We highlight our findings in Figure 3 and provide more examples in Appendix A.2. Our analysis shows many similarities to the BERT case. Instead of delimiter tokens, the majority of outliers seem to correlate with some random uninformative patches (e.g., in the background). We also see that the corresponding attention head in the next layer allocates the majority of attention probabilities to the same patches. Finally, those outlier patches on average have a distinctly smaller magnitude of values compared to non-outlier ones, leading to similar no-update behavior. The fact that those values are not as close to zero as it was in the BERT case might be related to the smaller model capacity3, or a relatively shorter training procedure. Hypothesis Based on these observations, we pose the following hypothesis on how this behavior of attention heads is related to outliers: 1. In order for an attention block to not update a representation of a token on the residual, some attention heads want to allocate most of their attention probability mass to some fixed and common set of tokens that have a low information content (e.g., delimiter tokens or background patches) that can be learned to have a small value function output. 3We use ViT/S-16 configuration that has only 22M parameters. 42. From the definition of the softmax function4, it is easy to see that this would require an input of the softmax to have a relatively big dynamic range (Figure 4, 1 ). In fact, in the limit case where softmax is exactly zero, this would require an infinite dynamic range: softmax (x)i = 0 ⇔ ∃ j ̸= i, xj − xi = +∞ (2) 3. Since Layer Normalization ([1], 2 ) normalizes the outliers, the magnitude of the FFN output in the previous layer ( 3 ) has to be very high to still produce a sufficiently big dynamic range after the LayerNorm. Note, that this is also applicable for the transformer models with LayerNorm applied prior to the self-attention or linear transformations instead, a variant adopted by GPT, OPT, and many vision transformers [15, 38, 57, 58]. 4. Finally, as softmax will never output exact zeros, it will always back-propagate a gradient signal to grow bigger outliers5. The outliers will thus tend to become stronger in magnitude, the longer the network is trained. 4 Method Figure 4: A schematic illus- tration of the attention layer in BERT. Hidden activation tensor is denoted by x. ⊕ is an element-wise addition. A problematic output of the FFN that generates largest in magni- tude outliers is highlighted in red. Notice how those outliers in the previous layer influence the behavior in the attention mechanism in the next layer. In this section, we introduce our proposed modifications for the softmax attention mechanism. Based on our insights from Section 3, the core idea of these modifications is to grant the model the ability to produce very small the magnitude (or even exact zeros) output of attention function, without producing outliers. Recall that the self-attention [60] is defined as follows: Attention(x) := softmax \u0012Q(x)K(x)T √dhead \u0013 V (x) (3) where Q, K and V are learnable linear projections of the input x. Most modern transformer models employ a multi-headed variant of self-attention, where dmodel features are partitioned into nheads groups of dhead features, and the final output is the concatenation of the outputs of (3) applied to each group. 4.1 Clipped softmax First, we propose to replace softmax function in (3) with the follow- ing clipped softmax: clipped_softmax(x; ζ, γ) := clip ((ζ − γ) · softmax(x) +γ, 0, 1) . (4) Here x is the input and ζ ≥ 1, γ ≤ 0 are the stretch factors which are hyper-parameters of the method. Similar formulation was used before for sigmoid function [40, 45]. We can view (4) as stretching the output of the softmax from(0, 1) to (γ, ζ) and then clipping back to (0, 1) so that we can represent exact zeros if γ <0 and exact ones if ζ > 1. Specifically, the values of the softmax larger than 1−γ ζ−γ are rounded to one whereas values smaller than −γ ζ−γ are rounded to zero. With this drop-in replacement, we can achieve exact zeros (and ones) with a finite range for the softmax input. In addition to that, whenever values are clipped they will not give a gradient, preventing the outliers to grow further. 4softmax (x)i = exp (xi) / Pd j=1 exp (xj) 5Let y = softmax (x). Then ∂yi ∂xj ̸= 0∀i, j. 54.2 Gated attention An alternative way of architecting the model to have a small attention output without outliers is to equip it with an explicit conditional gating mechanism, as shown in Figure 5. The idea is that the model can use the gating to either keep or nullify the update to the representation of certain tokens and not rely on the attention probabilities and values to achieve the same outcome. Specifically, we propose the following modification to the attention function: Gated_attention(x) := sigmoid (G(x)) ⊙ softmax \u0012Q(x)K(x)T √dhead \u0013 V (x). (5) Here G is the gating function,⊙ is an element-wise multiplication across the token axis and everything else remains the same as in (3). The gating function G is parameterized by a small neural network that is learned jointly with the rest of the model. We replace the attention formulation with the proposed variant in every layer on the transformer network. Figure 5: A schematic il- lustration of our proposed gated attention. Gating module design Recall that the input to the attention layer x has shape (T, dmodel) that is reshaped into (nheads, T, dhead) for the multi-headed self-attention, where T is the sequence length. We chose to define the gating function on a per-head basis. For each head i ∈ {1, . . . , nheads}, we specify Gi : Rdhead → R and the output of the gating module is πi ∈ RT that is computed as follows: bπi,t = Gi(xi,t,:) ∀t ∈ {1, . . . , T} (6) πi,: = sigmoid(bπi,:), (7) note that gating modules are shared between different token positions but not shared across attention heads. We want our gating module to be as lightweight as possible. To start with, we experiment with Gi’s parameterized by a single linear layer. This gives us a gating module that is computationally inexpensive and has a memory overhead of just nheads · (dhead + 1)∼ dmodel extra parameters (which is equivalent to 1 extra token) per attention layer6. We also investigate the effect of using several other gating functions in Appendix B.1. 5 Experiments In this section, we evaluate the proposed modifications to self-attention on several language models (BERT, OPT) and the vision transformers (ViT). We first test the different hyperparameters for the methods and provide insight into how they work. Then we set out to test our method in terms of accuracy, and the difference in quantization improvement after training. All detailed hyperparameters of our experiments are in Appendix C. BERT We experiment with BERT-base-uncased (109M parameters) pre-training using the masked language modeling (MLM) objective. Following [ 14], we use the concatenation of the training sets of BookCorpus [77] and English Wikipedia7. We implement our methods in PyTorch [48] and use training and evaluation pipelines from HuggingFace libraries [ 20, 34, 65]. We follow closely the pre-training procedure from [ 14]. To speed up training and experimentation, we train with a maximum sequence length of 128 for the whole duration of the training. We evaluate on Wikipedia validation set and report the MLM perplexity. OPT We experiment with a 125M sized variant of OPT [74] pre-training using the causal language modeling (CLM) objective. Due to compute constraints, we train the model on the same dataset that was used for BERT pre-training (BookCorpus + Wikipedia) with a maximum sequence length of 512 6For instance, in case of BERT-base, this amounts to less than 0.009% of the total model size. 7Specifically, we use the English subset of Wiki-40b,https://huggingface.co/datasets/wiki40b, that contains cleaned-up text of English Wikipedia and training/validation splits. 6γ ζ FP16 ppl.↓ Max inf. norm Avg. kurtosis W8A8 ppl. ↓ 0 1 4.49±0.01 735±55 3076±262 1294±1046 (= Vanilla) 0 1 .003 4.48±0.01 715±335 2159±238 451±57 0 1 .03 4.49±0.00 741±66 1707±1249 1469±646 −0.003 1 4.46±0.00 688±64 2149±110 636±566 −0.03 1 4.41±0.01 20±1 80±6 4.55±0.01 −0.003 1 .003 4.47±0.00 683±23 2494±1205 268±120 −0.03 1 .03 4.43±0.03 22±3 73±8 4.56±0.05 Table 1: The impact of clipped softmax hyperparameters on BERT-base. and batch size of 192. Similar to our BERT experiments, we use training and evaluation pipelines from HuggingFace libraries. We evaluate on Wikipedia validation set and report the CLM perplexity. ViT Finally, we explore the effectiveness of proposed techniques on vision transformer [15] (ViT- S/16 configuration, 22M parameters) trained on ImageNet-1K [11, 52]. For these experiments, we adopt the training and validation pipelines from PyTorch Image models library [64]. We report top-1 accuracy on the validation set of ImageNet. Quantization setup In all experiments, after the model is trained, we apply 8-bit PTQ. We use uniform affine quantization – symmetric weights, asymmetric activations – with the static activation range setting, as discussed in Section 2. We quantize all weights and activations (both input and output), except the final linear layer for BERT and OPT models. We explore several choices of range estimation (see Appendix C.4) and report the best configuration for each experiment, based on the model performance. We repeat each PTQ experiment 3 times with different random seeds8 and report mean and standard deviation for accuracy/perplexity. We train each network two times with different random seeds and report mean and standard deviation. To assess the amount of outliers in the trained model, we use two metrics: the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. These metrics have been shown to correlate well with the model quantizability [4, 6]. 5.1 The impact of clipped softmax hyperparameters (γ and ζ) We investigate the effect of different values of the clipped softmax stretch parameters and present the results in Table 1. We can see that most of the improvement happens when we use γ < 0 (clipping at zero). For instance, using the value of γ = −0.03 leads to a significantly smaller infinity norm, kurtosis, and quantized model perplexity, compared to the baseline. It is also clear that in the limit |γ| →0 we approach the vanilla softmax attention. Using ζ >1 (clipping at one) yields similar results to the vanilla softmax. Finally, when we combine both γ <0 and ζ >1, for which the results seem similar to just clipping at 0. We, therefore, conclude that for dampening outliers, only the lower-range clipping allows exact zeros matter. Going forward we use only γ <0 and in Appendix B.5 we confirm that ζ >1 is not required for ViT. These observations are in line with our hypothesis that by giving the model the mechanism for representing exact zeros in the attention, we don’t need to learn the strong outliers. 5.2 Clipped softmax γ vs. sequence length As having an extra hyper-parameter that needs to be tuned per model or setup is generally not desirable, we study the sensitivity of the stretch factor γ and its relation with the sequence length T. Recall that the matrix of attention probabilities P has dimensions T × T and each row sums up to one. Because of that, the average value in P is 1/T. It is reasonable to assume that if we define 8Different random subsets of training data are used for quantizer range estimation. 7(a) Relative FP16 log-perplexity  (b) Maximum infinity norm Figure 6: The performance of clipped softmax using γ = −α/T parameterization on BERT-6L. (a) Relative (compared to vanilla softmax pre-training) FP16 log-perplexity ↑ on Wikitext validation set. (b) Maximum infinity norm of the attention layer output (note the logarithmic y-axis). (a) BERT-6L  (b) ViT Figure 7: The performance of Linear gated attention using different bias initialization settings. γ := −α T , where α >0 is a new hyperparameter, there might be a set or a range of values of α that works well across different sequence lengths. To study this, we train a 6-layer variant of BERT-base (BERT-6L) for 500000 steps on WikiText- 103 [ 42] with a batch size of 128 with several values of maximum sequence lengths T ∈ {32, 64, 128, 192, 256} and values of α ∈ {1/4, 1/2, 1, 2, 4, 8}. As we can see from Figure 6, using a clipped softmax with α ∈ [2, 4] significantly dampens the magnitude of outliers while maintaining good FP16 perplexity across all explored sequence lengths. 5.3 The impact of bias initialization in gated attention In all our gated attention experiments, we randomly initialize the weights of G, following [22]. By initializing the bias to a specific value, however, we can set gates to be more open or more closed initially. More open at the start means we initialize closer to the original network, but given the exponential nature of the gate it might take many iterations for the gate to learn to close. Similarly, if the gates are all closed at the start, we deviate too far from the original model training, causing a potential decrease in performance. Assuming Linear Gi’s with small initial weights, if we set the bias to the value of binit, then Gi(·) ≈ binit and πi(·) = sigmoid(Gi(·)) ≈ sigmoid(binit) =:πinit, at the start of training. We study the effect of different values of binit for Linear gated attention on BERT-6L and ViT. We set the bias for all Gi’s to the same value of binit. For BERT-6L, we use the same setup as in Section 5.2, with a fixed sequence length of 128. For ViT, we use the main setup, except we train it for 150 epochs instead of 300. In Figure 7 we see in both BERT and ViT cases that using bias with very highπinit generally performs similarly to the vanilla attention (comparable floating-point performance but strong outliers and poor quantized performance) while setting bias to have very low πinit dampens outliers quite well but leads to strong degradation in the floating-point and quantized performance. The reasonable ranges of πinit seems to be around [0.25, 0.9] for BERT and [0.1, 0.5] for ViT. The wide range indicates the relative robustness of our method to this hyperparameter. 8Model Method FP16/32 Max inf. norm Avg. kurtosis W8A8 BERT (ppl.↓) Vanilla 4.49 ±0.01 735±55 3076±262 1294±1046 Clipped softmax 4.39±0.00 21.5±1.5 80±6 4.52±0.01 Gated attention 4.45 ±0.03 39.2±26.0 201±181 4.65±0.04 OPT (ppl.↓) Vanilla 15.84 ±0.05 340±47 1778±444 21.18±1.89 Clipped softmax 16.29 ±0.07 63.2±8.8 19728±7480 37.20±2.40 Gated attention 15.55±0.05 8.7±0.6 18.9±0.9 16.02±0.07 ViT (acc.↑) Vanilla 80.75 ±0.10 359±81 1018±471 69.24±6.93 Clipped softmax 80.89 ±0.13 73.7±14.9 22.9±1.6 79.77±0.25 Gated attention 81.01±0.06 79.8±0.5 19.9±0.3 79.82±0.11 Table 2: A summary of results for our proposed methods applied on BERT, OPT-125m, and ViT. Model Method FP16 Max inf. norm Avg. kurtosis W8A8 OPT-350m (ppl.↓) Vanilla 13.19 253 2689 37.52 ±3.84 Gated attention 13.01 65.4 261 14.42±0.06 OPT-1.3B (ppl.↓) Vanilla 12.13 428 2756 989.6 ±175 Gated attention 12.21 67.2 444 29.95±0.42 Table 3: The performance of gated attention applied on bigger variants of OPT model. 5.4 Main results We summarize our main set of results in Table 2. As we can see, in almost all cases, both of our proposed techniques dampen the outliers’ magnitude to a great extent, reduce the kurtosis, and yield models with significantly higher quantized performance, which is close to the original FP16/32 performance. In addition to that, for each model, at least one of our methods also improves the floating-point task performance. We hypothesize this is because the network is helped with learning the “no-op” updates more easily. However, we are cautious about the improved performance as this is not consistent across all hyper-parameters and it is unclear if it generalizes to more architectures and larger models. The only case where our method failed to perform well was the clipped softmax applied to OPT. At the moment, we do not have an explanation of why this is the case and leave it for future work. We list selected hyper-parameters and show extended results in Appendix B. We also show the results of our proposed methods quantized to lower bitwidths in Appendix B.7. Results for bigger modelsWe study the question of scalability of our methods to larger models. In Table 3 we show the gated attention results for 350m and 1.3B variants of OPT. Due to compute constraints, we trained networks for 105 steps with batch size of 256 and the rest is the same as in our main pre-training setup. As we can see, our proposed gated attention is also very effective at dampening the outliers and significantly improving the quantized model performance when applied to bigger models. We further study in Appendix B.6 how gated attention can decrease outliers when fine-tuning bigger pre-trained models with outliers. 5.5 Qualitative results In Figure 8 we compare the learned attention patterns using vanilla softmax and our proposed methods (more examples in Appendix A.1). As we can see, both methods can represent a partial/soft no-op behavior, but in case of our methods this does not require strong outliers elsewhere in the network. Note that we found similar patterns in multiple attention heads, but the exact head indices where we observed such patterns depend on random initialization. In the case of clipped softmax, smaller attention weights are generally more diffused while higher weights are more saturated (which comes from the stretching and clipping). In the case of gated attention, the output of the softmax is significantly different since the update of the hidden representation is now further modulated by gating probabilities. 9(a) Vanilla softmax (Attention layer #11, head #3) (b) Clipped softmax (Attention layer #11, head #8) (c) Gated attention (Attention layer #11, head #5) Figure 8: Visualization of the self-attention patterns for BERT-base trained using vanilla and our proposed techniques, computed on data sequence #5 from MNLI-m validation set. (a), (b): attention probabilities, values, and their product. (c): gating probabilities π = sigmoid (G(x)), attention probabilities (output of softmax), values, and their combined product. 6 Discussion “No-op” behavior It is interesting to note that the identified “no-op” behavior is likely not limited to transformers and that convolutional architectures likely learn something similar. We also see that despite the network trying to learn a full “no-op”, still a small amount of noise is added to each residual, which may constitute a form of network regularization. Investigating this further might give us a clue as to why neural networks generalize despite being significantly overparametrized if many parameters are rendered unused by not updating the representation in later layers [72]. Limitations While we studied the scalability of our method for models up to 1.3B size, we haven’t explored the case of very large transformers that are trained for way longer. Given the fundamental understanding of the issue underlying our solutions, we expect the same effect on larger-scale models. We show a very small improvement in FP16/FP32 performance due to our methods, but we do not deem our results exhaustive enough to claim that this will hold in general. Lastly, our methods do have a hyperparameter each, although we show that both methods are relatively robust to its hyperparameter, having one is never optimal. Impact As our methods help transformers to be more efficient, we expect only positive outcomes of our work. Making neural networks more efficient will help with their high power consumption at inference. It further helps to move inference from the cloud to edge devices which can overcome potential privacy concerns. We cannot fathom any negative impact from our work that is not severely construed. 7 Conclusions We have thoroughly analyzed the activation outlier problem that makes transformers difficult to quantize. We showed that transformer networks try to learn not to update residuals and that by doing so, through the combination of the softmax, residual connections and LayerNorm, significant outliers appear in transformers. Based on this insight, we proposed two methods to address this at the core – clipped softmax and gated attention. These structural changes to transformers give similar, if not better, floating-point performance after training but significantly improve the post-training quantization results. We hope that with these two architectural changes to transformers, anyone can train high-performance transformers that are easy to quantize and can benefit from efficient integer inference. 10References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018. [3] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the chal- lenges of efficient transformer quantization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7947–7969, Online and Punta Cana, Dominican Republic, Novem- ber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.627. URL https://aclanthology.org/2021.emnlp-main.627. [5] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13169–13178, 2020. [6] Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser, et al. Robust quantization: One model to rule them all. Advances in neural information processing systems , 33: 5308–5317, 2020. [7] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks for efficient inference. In ICCV Workshops, pages 3009–3018, 2019. [8] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276–286, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://aclanthology.org/ W19-4828. [9] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020. [10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702–703, 2020. [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. [12] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022. [13] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplica- tion for transformers at scale. In Advances in Neural Information Processing Systems, 2022. [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423. [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [16] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. Learned step size quantization. In International Conference on Learning Representations (ICLR), 2020. 11[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Hervé Jégou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020. [18] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [19] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021. [20] Sylvain Gugger, Lysandre Debu, Thomas Wolf, Philipp Schmid, Zachary Mueller, and Sourab Mangrulkar. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/ huggingface/accelerate, 2022. [21] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International conference on machine learning, pages 1737–1746. PMLR, 2015. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. [23] M. Horowitz. 1.1 computing’s energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10–14, 2014. doi: 10.1109/ ISSCC.2014.6757323. [24] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. The Journal of Machine Learning Research, 18(1):6869–6898, 2017. [25] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020. [26] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer- arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2704–2713, 2018. [27] Minsoo Kim, Sihwa Lee, Sukjin Hong, Du-Seong Chang, and Jungwook Choi. Understanding and improving knowledge distillation for quantization-aware training of large transformer encoders. arXiv preprint arXiv:2211.11014, 2022. [28] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. arXiv preprint arXiv:2101.01321, 2021. [29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [30] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4365–4374, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10. 18653/v1/D19-1445. URL https://aclanthology.org/D19-1445. [31] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. Bert busters: Outlier dimen- sions that disrupt transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3392–3405, 2021. [32] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. [33] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871–7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/ 2020.acl-main.703. 12[34] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysan- dre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstra- tions, pages 175–184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-demo.21. [35] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [36] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [37] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. [39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [40] Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0 regularization. arXiv preprint arXiv:1712.01312, 2017. [41] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Re- covering neural network quantization error through weight factorization. In International Conference on Machine Learning, pages 4486–4495. PMLR, 2019. [42] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [43] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325–1334, 2019. [44] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020. [45] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197–7206. PMLR, 2020. [46] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Blankevoort Tijmen. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021. [47] Vinod Nair and Geoffrey E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In Proceedings of the 27th International Conference on Machine Learning, pages 807–814. Omnipress, 2010. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems (NeuRIPS). 2019. [49] Giovanni Puccetti, Alessio Miaschi, and Felice Dell’Orletta. How do BERT embeddings organize linguistic knowledge? In Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 48–57, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.deelio-1.6. URL https://aclanthology.org/ 2021.deelio-1.6. 13[50] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [51] Bita Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na, Prerak Patel, Shuai Che, Lok Chand Koppaka, Xia Song, Subhojit Som, Kaustav Das, Saurabh Tiwary, Steve Reinhardt, Sitaram Lanka, Eric Chung, and Doug Burger. Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point. In Neural Information Processing Systems (NeurIPS 2020). ACM, November 2020. [52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. [53] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. [54] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815–8821, 2020. [55] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. MobileBERT: a compact task-agnostic BERT for resource-limited devices. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2158–2170, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.195. URL https://aclanthology.org/ 2020.acl-main.195. [56] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015. [57] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32–42, 2021. [58] Hugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIV, pages 516–533. Springer, 2022. [59] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, and Tijmen Blankevoort. Fp8 versus int8 for efficient deep learning inference. 2023. [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000–6010, 2017. [61] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446. [62] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. arXiv preprint arXiv:2209.13325, 2022. [63] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [64] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019. [65] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. 14[66] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4 quantization for transformer models: Latency speedup, composability, and failure cases. 2023. [67] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In CVPR, 2022. [68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xl- net: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Pro- cessing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf. [69] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transform- ers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Ad- vances in Neural Information Processing Systems , volume 35, pages 27168–27183. Curran Asso- ciates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf. [70] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023–6032, 2019. [71] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019. [72] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. 2017. [73] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. [74] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [75] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In International conference on machine learning, pages 7543–7552. PMLR, 2019. [76] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. [77] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015. 15Supplementary materials A Additional graphs from outlier analysis In this section, we present additional graphs from our outlier investigation in Section 3 for BERT and vision transformer. (a)  (b)  (c) Figure 9: A summary of several outlier statistics recorded from ImageNet validation set on ViT. (a) Average infinity norm of the output of each attention layer. (b) A histogram of outlier counts in attention layer #10 vs. hidden dimensions. We use zero-based indexing for dimensions. (c) A heatmap of outlier counts in attention layer #10 vs. patch positions. A.1 BERT Recall from Figure 1 that all the outliers are only present in hidden dimensions #123, #180, #225, #308, #381, #526, #720 (with the majority of them in #180, #720). These hidden dimensions correspond to attention heads #2, #3, #4, #5, #6, #9, and #12. In Figures 10 and 11 we show more examples of the discovered self-attention patterns for attention heads #3 and #12 (↔ hidden dim #180 and #720, respectively). We also show self-attention patterns in attention heads and layers which are not associated with the outliers in Figures 12 and 13, respectively. Finally, in Figures 14 and 15 we show more examples of the attention patterns learned in the network trained with clipped softmax and gated attention. A.2 ViT Figure 9 further shows that there are a lot of similarities in the outlier behavior in the vision transformer, compared to BERT. The strongest magnitude outliers generally happen in the later layers, peaking at layers #10 and #11. The majority of outliers (> 99%) are only ever happening in only 10 hidden dimensions, primarily in dimensions #48 and #43, which corresponds to the attention head #1. Finally, averaged across the entire ImageNet validation set, the outliers seem to be concentrated at the boundaries of the image, which suggest a strong correlation with the background (and a negative correlation with the object, which is usually in the center of the image in the ImageNet dataset). In Figures 16 and 17, we show more examples of outlier and self-attention patterns in the attention head #1 (↔ hidden dimensions #48, #43) for a random subset of images from the ImageNet validation set (in layers #10 and #11, respecively). B Detailed results In this section, we provide extended results for each model, including the used hyperparameters and other design choices. We also present some additional ablation studies. 16Configuration G Memory overhead (per attention layer) # extra parameters # extra tokens Linear nheads × Linear(dhead → 1) nheads(dhead + 1) ∼ 1 MLP nheads × MLP(dhead → nhid → 1) nheads(nhid(dhead + 2) + 1) ∼ nhid All-heads-linear Linear(dmodel → nheads) nheads(dmodel + 1) ∼ nheads Table 4: An overview of the gating function parameterizations explored in this paper and their memory overhead. B.1 Gating architectures We investigate the choice of several gating functions, summarized in Table 4. The configuration “MLP” parameterizes each Gi with a feed-forward net with one hidden layer of size nhid and a ReLU non-linearity [47]. We also explore what happens if we allow the mixing of the representation from different attention heads in the “All-heads-linear” setting, where we use a single linear layer to produce the gating probabilities for all attention heads at once. All three options are tested below. Unless explicitly stated otherwise, we initialize the bias of the gating function to zero (i.e., binit = 0 ↔ πinit = 0.5). B.2 BERT Method FP16 ppl. ↓ Max inf norm Avg. Kurtosis W8A8 ppl. ↓ Vanilla 4.49 ±0.01 735.0±54.9 3076±262 1294±1046 CS (γ = −0.005) 4.44 ±0.02 406.6±35.2 1963±753 75.27±39.57 CS (γ = −0.01) 4.35 ±0.01 198.3±78.7 1581±839 7.06±2.37 CS (γ = −0.015) 4.37 ±0.01 38.9±7.9 165±34 4.54±0.01 CS (γ = −0.02) 4.39 ±0.02 31.7±6.3 90±20 4.56±0.02 CS (γ = −0.025) 4.39±0.00 21.5±1.5 80±6 4.52±0.01 CS (γ = −0.03) 4.41 ±0.01 20.4±0.2 79±6 4.55±0.01 CS (γ = −0.04) 4.51 ±0.05 19.8±9.0 85±7 4.65±0.06 GA, Linear (πinit = 0.25) 4.49 ±0.00 139.8±62.3 739±412 5.05±0.27 GA, Linear (πinit = 0.5) 4.48 ±0.00 177.3±33.2 652±81 5.13±0.15 GA, Linear (πinit = 0.75) 4.49 ±0.00 71.4±49.9 262±147 4.88±0.22 GA, Linear (πinit = 0.9) 4.49 ±0.00 171.5±8.8 559±141 5.15±0.03 GA, MLP (nhid = 4) 4.45±0.03 39.2±26.0 201±181 4.65±0.04 GA, MLP (nhid = 64) 4.49 ±0.01 117.0±48.3 507±167 4.77±0.01 GA, All-heads-linear 4.49 ±0.01 58.3±41.2 334±321 4.67±0.03 Table 5: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to BERT-base. We report the masked language modeling perplexity (ppl. for short) on the English Wikipedia validation set for both the floating-point baseline and W8A8 quantized model. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. Detailed results for BERT-base are summarized in Table 5. As we can see, across most of the settings, both of our methods significantly dampen the outliers’ magnitude, reduce the kurtosis, drastically improve the quantized performance, while maintaining and sometimes improving the FP16 perplexity. B.3 OPT Detailed results for OPT-125m are summarized in Table 6. In our early experiments on a smaller OPT model, we found that applying the weight decay on LayerNorm weights γ (which isn’t the case, by default) has a strong effect on reducing the outliers’ magnitude while yielding the comparable FP16 performance. Therefore, we present the results of applying our gated attention approach in both cases, with and without applying weight decay on LNγ. As we can see in Table 6, in both cases gated attention (further) dampens the outliers’ magnitude to a 17Method LN γ wd FP16 ppl. ↓ Max inf norm Avg. Kurtosis W8A8 ppl. ↓ Vanilla ✕ 15.84±0.05 339.6±47.2 1777±444. 21.18±1.89 GA, Linear (πinit = 0.1) ✕ 15.61±0.05 35.6±4.5 42.4±22.9 16.41±0.18 GA, Linear (πinit = 0.25) ✕ 15.50±0.04 35.8±0.5 59.0±48.3 16.25±0.08 GA, Linear (πinit = 0.5) ✕ 15.54±0.01 46.5±5.0 40.6±8.9 16.30±0.01 GA, All-heads-linear ✕ 15.43±0.01 32.8±1.7 24.2±3 16.30±0.12 Vanilla ✓ 15.96±0.03 87.7±31.9 2080±1460 39.46±16.59 CS (γ = −1/512) ✓ 15.99±0.02 106.4±7.0 5764±2150 185.23±220.00 CS (γ = −2/512) ✓ 15.90±0.02 102.0±27.0 11290±4372 60.90±52.70 CS (γ = −4/512) ✓ 15.86±0.01 83.1±20.6 17174±7791 84.64±10.55 CS (γ = −8/512) ✓ 16.13±0.09 61.5±9.9 19204±4284 42.62±3.64 CS (γ = −12/512) ✓ 16.29±0.07 63.2±8.8 19727±7479 37.22±2.39 GA, Linear (πinit = 0.1) ✓ 15.69±0.05 7.3±0.4 25.4±10 16.23±0.08 GA, Linear (πinit = 0.25) ✓ 15.55±0.05 8.7±0.6 18.9±1 16.02±0.07 GA, Linear (πinit = 0.5) ✓ 15.63±0.00 10.8±0.7 42.0±19 16.20±0.01 GA, All-heads-linear ✓ 15.53±0.01 7.9±0.3 13.8±1 16.09±0.08 Table 6: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to OPT-125m. We report the causal language modeling perplexity (ppl. for short) on the English Wikipedia validation set for both the floating-point baseline and W8A8 quantized model. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. great extent, reduces the kurtosis, and yields models with significantly higher quantized performance, which is close to the original FP16 performance. B.4 ViT Method Patch. Embd. LN FP32 acc. Max inf norm Avg. Kurtosis W8A8 acc. Vanilla ✕ 80.75±0.10 358.5±81.2 1018.3±471.5 69.24±6.93 CS (γ = −0.003) ✕ 80.24±0.05 69.3±20.7 25.6±8.6 78.71±0.33 CS (γ = −0.004) ✕ 80.38±0.01 74.9±10.6 30.6±4.9 78.66±0.49 GA, Linear (πinit = 0.25) ✕ 80.62±0.01 86.0±8.0 23.4±2.7 79.16±0.05 GA, Linear (πinit = 0.5) ✕ 80.32±0.02 88.4±17.9 27.9±14.0 78.90±0.25 GA, MLP (nhid = 4) ✕ 80.62±0.05 118.2±40.5 47.8±29.8 78.79±0.29 Vanilla ✓ 80.98±0.08 81.1±2.5 24.5±1.8 79.62±0.06 CS (γ = −0.0001) ✓ 80.89±0.13 73.7±14.9 22.9±1.6 79.77±0.25 CS (γ = −0.0003) ✓ 80.92±0.07 78.9±5.5 23.8±0.5 79.63±0.05 CS (γ = −0.0005) ✓ 80.95±0.08 72.9±11.8 24.4±0.7 79.73±0.08 CS (γ = −0.001) ✓ 80.95±0.16 80.8±2.1 24.1±0.7 79.69±0.03 CS (γ = −0.002) ✓ 80.80±0.07 78.0±0.5 25.8±0.7 79.32±0.07 CS (γ = −0.003) ✓ 80.79±0.02 75.6±7.9 28.1±4.0 79.00±0.10 GA, Linear (πinit = 0.5) ✓ 81.01±0.06 79.8±0.5 19.9±0.3 79.82±0.11 GA, Linear (πinit = 0.75) ✓ 81.01±0.05 77.8±0.3 21.8±1.9 79.80±0.08 GA, Linear (πinit = 0.9) ✓ 80.92±0.11 70.6±8.0 23.2±3.7 79.64±0.09 Table 7: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to ViT-S/16. We report the top-1 accuracy on ImageNet-1K validation set for floating-point baseline and W8A8 quantized model. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of the attention layer. Detailed results for ViT-S/16 are summarized in Table 7. After our preliminary experiments on ViT, we noticed that distinct outliers already originate after the patch embeddings. Therefore, we experimented with adding the LayerNorm after the patch 18embeddings (which was absent in the model definition, by default). As we can see in Table 6, together with this change, both of our proposed methods greatly dampens the outliers’ magnitude, reduces the kurtosis, and yields models with significantly higher quantized performance, which is within 1% of the original FP32 accuracy. B.5 The impact of clipped softmax hyperparameters (γ and ζ) on ViT γ ζ FP32 acc. Max inf norm W8A8 acc. 0 1 78.80±0.42 426±69 71.27±0.88 (= Vanilla) 0 1 .001 78.78±0.29 411±88 71.24±0.59 0 1 .002 78.90±0.17 420±47 70.74±0.34 0 1 .004 78.80±0.45 377±67 72.31±0.06 0 1 .01 78.81±0.30 419±77 71.35±0.26 −0.00001 1 78.81±0.21 432±76 69.02±0.19 −0.0001 1 78.81±0.36 380±64 64.04±10.8 −0.001 1 78.42±0.63 282±105 68.43±6.50 −0.003 1 78.26±0.06 99±36 76.49±0.48 −0.01 1 78.10±0.14 391±21 75.83±1.12 −0.03 1 70.26±1.46 197±2 65.80±1.41 −0.001 1 .001 78.45±0.53 283±82 65.03±8.54 −0.003 1 .003 78.25±0.14 119±17 76.37±0.45 Table 8: The impact of clipped softmax hyperparameters on ViT-S/16. We investigate the effect of different values of the clipped softmax stretch parameters applied to the vision transformer and present the results in Table 8. To speed up training, for this experiment we trained ViT for 150 epochs instead of the usual 300 epochs. For this experiment, we did not apply LayerNorm after the patch embeddings. We found similar observations compared to BERT. Specifically, most of the improvement happens when we use γ <0 (clipping at zero) whereas using ζ >1 (clipping at one) yields similar results to the vanilla softmax and combining both γ <0 and ζ >1 yields similar results compared to just clipping at zero. B.6 Fine-tuning experiment Method FP16 ppl. ↓ Max inf norm Avg. Kurtosis Vanilla fine-tuning 29.46 79.3 2086 Fine-tuning w/ Gated attention 29.18 50.9 665 Table 9: OPT-1.3B fine-tuning results with vanilla softmax and gated attention. We report the causal language modeling perplexity (ppl. for short) on the English Wikipedia validation set. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. One of the drawbacks of our proposed framework is that it requires training from scratch, which could be expensive when applied to very large models. To address this, we explored whetherfine-tuning using gated attention can still lead to improved performance and decreased outliers for larger models. We used OPT-1.3B pre-trained checkpoint from HuggingFace and fine-tuned it on Bookcorpus + Wikipedia for 4000 steps with batch size 256, maximum sequence length 512, maximum learning rate 10−5, and linear LR schedule with 400 warmup steps. We use the same LR for both model parameters and gating module parameters. The rest of hyper-parameters are the same as for our pre-training setup. We adapted our gating approach as follows. We initialized bias as binit = 0, which corresponds to the expected initial gating probability output of πinit = 0.5. We multiply the gating probability by 2 so that the expected gate output is 1 and we approximate the attention output of 19the vanilla softmax at the start of fine-tuning. We add a small activation regularization term at the output of each FFN to further encourage the reduction in the magnitude of activations, as unlike when training from scratch outliers are already present in the pre-trained model and need to be suppressed. As we can see from Table 9, fine-tuning with our proposed gated attention results in a better perplexity and also reduced maximum infinity norm and the average kurtosis compared to fine-tuning with vanilla softmax. B.7 Low-bit quantization results Bitwidths Weight range estimation Vanilla Clipped softmax Gated attention FP16 − 4.49±0.01 4.39±0.00 4.45±0.03 W8A8 min-max 1294 ±1046 4.52±0.01 4.65±0.04 W6A8 min-max 598 ±254 4.64±0.01 4.79±0.03 W6A8 MSE 6.49 ±0.38 4.56±0.01 4.71±0.03 W4A8 MSE 6.52 ±0.02 4.90±0.02 5.02±0.03 W6A6 MSE 42.8 ±11.7 6.64±0.14 5.90±0.11 Table 10: A summary of results for our proposed methods applied to BERT-base and quantized to different bitwidthds for weights and activations (using the same PTQ setup as in all previous experi- ments). We report the masked language modeling perplexity on the English Wikipedia validation set. Note that our proposed methods are not limited to 8-bit quantization only and in general can be combined with other more advanced quantization and weight compression methods, including [18, 35, 36, 45, 63, 67]. In Table 10, we show the results of our proposed methods applied to BERT-base and quantized to different bitwidths using our simple post-training quantization setup. Unless stated otherwise, for low-bit (<8-bit) weights and activations we use MSE range estimator as recommended by [2, 7] since it gives better results. As we can see, in all cases both of our methods significantly improve the perplexity compared to the vanilla softmax pre-training. We also notice that generally the performance progressively degrades as we decrease the bitwidths, which is to be expected. Achieving good results with low-bit activation quantization in general is a challenging problem. Further, we notice that the perplexity of the vanilla model significantly improves whenever we consider a low-bit weight quantization with MSE ranges compared to the INT8 case. This can be explained by the fact that using MSE range estimation for weights leads to an implicit clipping of activations (in the same and all subsequent layers in the network), which happen to be of the right amount so that it doesn’t hurt the perplexity. We found that by going from W8A8 to W6A8 the average kurtosis is reduced from 3406±547 to 631±94 and the maximum infinity norm is reduced from 577±80 to 158±40. However, in all cases the resulting model still has significantly larger outliers and a worse performance than both of our proposed methods. Finally, as said before, if achieving good low-bit quantization performance is the goal, it is recommended to combine our methods with more advanced quantization techniques. C Experimental details C.1 BERT Fine-tuning on MNLI dataset We use pre-trained checkpoint BERT-base-uncased (109M param- eters) from HuggingFace repository. We follow standard fine-tuning practices from [14] and [ 65] Each data sequence is tokenized and truncated to the maximum sequence length of 128. Shorter sequences are padded to the same length of 128 using a special [PAD] token. We fine-tune for 3 epochs using Adam [29] with a batch size of 16 and no weight decay. The learning rate is initially set to its maximum value of of 2 · 10−5 and is linearly decayed to zero by the end of fine-tuning. Pre-training from scratch We follow closely the pre-training procedure from [14]. We concate- nate, tokenize, and split the training set into sequences of length 128 (to speed up training and experimentation, we do not fine-tune on longer sequences of 512). We use the masked language modeling objective with the probability of masking p = 0.15. We train with a batch size of 256 20sequences for 106 steps, using AdamW optimizer [ 39] with the maximum learning rate of 10−4, learning rate warm up over the first 104 steps, following by a linear decay to zero by the end of training. We use L2 weight decay of 0.01, L2 gradient norm clipping of 1.0, and dropout probability of 0.1 on all layers. We also use FP16 mixed-precision from HuggingFace Accelerate library [20]. C.2 OPT pre-training To speed up experimentation, we train OPT-125m sized model on the concatenation of Wikipedia and BookCorpus (same as BERT pre-training). We train with a batch size of 48 and 4 gradient accumulation steps (which results in the effective batch size of 192), so that we can perform pre- training on a single A100 80GB GPU. We concatenate, tokenize, and split the training set into sequences of length 512 and train for 125000 steps (500000 forward passes). We use the rest of the hyper-parameters and follow pre-training practices from [74] and [65]. We initialize weights using a normal distribution with zero mean and a standard deviation of 0.006. All bias terms are initialized to zero. We use AdamW optimizer with (β1, β2) = (0.9, 0.95). We use the linear learning rate schedule, warming up from 0 to the maximum value† of 4 · 10−4 over the first 2000 steps, following by a linear decay to zero by the end of training. We use L2 weight decay of 0.1, L2 gradient norm clipping of 1.0, and dropout probability of 0.1 on all layers. We also use FP16 mixed-precision from HuggingFace Accelerate library [20]. Note that in our experiments for all model sizes we use the consistent LayerNorm placement before the attention block, unlike OPT-350m checkpoint from HuggingFace that places LayerNorm after the attention block. C.3 ViT pre-training We use the model definition for ViT-S/16 and the training pipeline from PyTorch Image models library [64]. All training is done on resolution 224 ×224 and 16 ×16 patches. For data augmentation, we use RandAugment [10], Mixup [73], CutMix [70], random image cropping [56], horizontal flip, label smoothing ε = 0.1, color jitter 0.4, and random (between bilinear and bicubic) interpolation during training. We train with a batch size of 512 for 300 epochs, using AdamW optimizer and the L2 weight decay of 0.03. We use the cosine learning rate schedule, warming up from 10−6 to the maximum value of 10−3 over the first 20 epochs, followed by a LR decay by a factor of 10 every 30 epochs, until it reaches the minimum value of 10−5. C.4 Quantization settings Weights In all cases, we use symmetric uniform quantization of weights. We use min-max weight quantization for all models except the OPT model, for which we found the MSE estimator to perform better in all cases. Activations We adopt static range estimation approach, which determines quantization parameters for the network by passing a few batches of calibration data through the model before inference. Specifically, we use a running min-max estimator [32], which uses an exponential moving average of the min and max over multiple batches. In all cases, we use running min-max with 0.9 momentum over 16 batches randomly sampled from respective training sets. For OPT model, we also experiment with using 99.99% and 99.999% percentiles instead of actual min and max. We select the best configuration for each experiment (including baseline), based on the model performance. In almost all cases, we found that setting activation quantization ranges using 99.999% percentiles gives the lowest W8A8 perplexity. D Compute cost We compare the runtime of our proposed methods in Table 11. As we can see, the clipped softmax is only marginally more expensive compared to using the vanilla softmax attention. The gated attention †In our experiments, we found this value to perform better compared to the value of 6 · 10−4 listed in the paper. 21Model Vanilla Clipped softmax Gated attention (Linear / MLP) BERT 92.8 ±1.2 93.6±0.8 97.7 / 119.1 OPT 53.6 ±0.4 54.4±0.4 55.7 / 64.7 ViT 101.8 ±0.3 104.0±0.7 110.8 / 122.9 Table 11: An overview of the runtime of the proposed methods, compared to the vanilla pre-training, measured in hours on Nvidia-A100 GPUs. using the linear G adds the compute overhead between 3% and 8%, depending on the model. We found that adding weight decay on LayerNorm γ for OPT and adding the LayerNorm after the patch embeddings for ViT had a negligible effect on the runtime. We estimated that the compute cost of producing the main results in the paper is about 320 GPU days (on A100) and the total cost of the project (including preliminary experiments and ablation studies) to be about 1400 GPU days. 22(a) Attention layer #10, data sequence #16 (b) Attention layer #11, data sequence #16 (c) Attention layer #10, data sequence #21 (d) Attention layer #11, data sequence #21 (e) Attention layer #10, data sequence #61 (f) Attention layer #11, data sequence #61 (g) Attention layer #10, data sequence #88 (h) Attention layer #11, data sequence #88 Figure 10: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #3 (↔ channel dim #180) for BERT-base trained with vanilla softmax, computed on several random data sequences from MNLI-m validation set. 23(a) Attention layer #10, data sequence #16 (b) Attention layer #11, data sequence #16 (c) Attention layer #10, data sequence #21 (d) Attention layer #11, data sequence #21 (e) Attention layer #10, data sequence #61 (f) Attention layer #11, data sequence #61 (g) Attention layer #10, data sequence #88 (h) Attention layer #11, data sequence #88 Figure 11: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #12 ( ↔ channel dim #720) for BERT-base trained with vanilla softmax, computed on several random data sequences from MNLI-m validation set. 24(a) Attention layer #10, attention head #1 (b) Attention layer #11, attention head #1 (c) Attention layer #10, attention head #7 (d) Attention layer #11, attention head #7 (e) Attention layer #10, attention head #8 (f) Attention layer #11, attention head #8 (g) Attention layer #10, attention head #10 (h) Attention layer #11, attention head #10 Figure 12: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention heads that are not associated with the strong outliers for BERT-base trained with vanilla softmax, computed on data sequences #16 from MNLI-m validation set. 25(a) Attention layer #1 (b) Attention layer #2 (c) Attention layer #3 (d) Attention layer #4 (e) Attention layer #5 (f) Attention layer #6 (g) Attention layer #7 (h) Attention layer #8 Figure 13: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #3 (↔ channel dim #180) and the first eight layers of BERT-base trained with vanilla softmax, computed on data sequences #16 from MNLI-m validation set. 26(a) Attention layer #10, Attention head #3, data sequence #1 (b) Attention layer #11, Attention head #3, data sequence #1 (c) Attention layer #10, Attention head #3, data sequence #5 (d) Attention layer #11, Attention head #3, data sequence #5 (e) Attention layer #10, Attention head #3, data sequence #7 (f) Attention layer #11, Attention head #3, data sequence #7 (g) Attention layer #10, Attention head #12, data sequence #1 (h) Attention layer #11, Attention head #12, data sequence #1 Figure 14: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) for BERT-base trained with Clipped softmax, computed on several random data sequences from MNLI-m validation set. 27(a) Attention layer #10, Attention head #3, data sequence #1 (b) Attention layer #11, Attention head #3, data sequence #1 (c) Attention layer #10, Attention head #3, data sequence #5 (d) Attention layer #11, Attention head #3, data sequence #5 (e) Attention layer #10, Attention head #3, data sequence #7 (f) Attention layer #11, Attention head #3, data sequence #7 (g) Attention layer #10, Attention head #12, data sequence #1 (h) Attention layer #11, Attention head #12, data sequence #1 Figure 15: Visualization of the self-attention patterns (from left to right: gating probabilities π = sigmoid (G(x)), output of softmax, values, and their combined product) for BERT-base trained with gated attention, computed on several random data sequences from MNLI-m validation set. 28(a)  (b)  (c)  (d)  (e) Figure 16: A summary of our outlier analysis for ViT demonstrated on a random subset from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #10. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, in the next layer #11. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values (V ) for outlier and non-outlier patches. 29(a)  (b)  (c)  (d)  (e) Figure 17: A summary of our outlier analysis for ViT demonstrated on a random subset from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, in the next layer #12. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values (V ) for outlier and non-outlier patches. 30",
      "meta_data": {
        "arxiv_id": "2306.12929v2",
        "authors": [
          "Yelysei Bondarenko",
          "Markus Nagel",
          "Tijmen Blankevoort"
        ],
        "published_date": "2023-06-22T14:39:04Z",
        "pdf_url": "https://arxiv.org/pdf/2306.12929v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "Transformer models often learn strong outliers in activations, making them difficult to quantize to low bitwidths without significant performance loss. This paper identifies that these outliers arise when attention heads attempt to learn a \"no-op\" or partial update of the residual, which forces the softmax input to grow very large. The key contributions are two novel, independent modifications to the attention mechanism: clipped softmax and gated attention. These methods enable pre-trained models to learn significantly smaller outliers, maintain or improve floating-point task performance, and allow full INT8 quantization of activations effortlessly. The effectiveness is demonstrated across language models (BERT, OPT) and vision transformers (ViT).",
        "methodology": "The methodology involves a detailed outlier analysis of BERT and ViT, revealing that outliers correlate with attention heads attempting \"no-op\" updates on uninformative tokens or patches, causing large softmax inputs due to the combination of softmax, residual connections, and LayerNorm. Based on this, two architectural modifications are proposed: 1. Clipped Softmax: Replaces the standard softmax with `clipped_softmax(x; ζ, γ)`, which stretches the output range and clips values to allow exact zeros (when γ < 0) and ones (when ζ > 1) with a finite input range, preventing gradient propagation for clipped values. 2. Gated Attention: Introduces an explicit conditional gating mechanism `Gated_attention(x) := sigmoid(G(x)) ⊙ softmax(...)V(x)`, where G is a lightweight per-head neural network (linear layer or MLP) that learns to selectively nullify or keep updates to token representations, without relying on extreme softmax values.",
        "experimental_setup": "The methods were evaluated on BERT-base-uncased (109M parameters) for Masked Language Modeling (MLM), OPT models (125M, 350M, 1.3B parameters) for Causal Language Modeling (CLM), and ViT-S/16 (22M parameters) for image classification. Pre-training datasets included BookCorpus and English Wikipedia for language models, and ImageNet-1K for ViT. Evaluation metrics were MLM/CLM perplexity and Top-1 accuracy. Outliers were quantified using maximum L-infinity norm and kurtosis of attention layer outputs. For quantization, an 8-bit Post-Training Quantization (PTQ) scheme (W8A8) was used: uniform affine quantization with symmetric weights and asymmetric activations, employing a static range estimation using a running min-max estimator over 16 calibration batches. For OPT, 99.99% and 99.999% percentiles were also explored. All weights and activations were quantized except the final linear layer in BERT/OPT. Hyperparameter tuning was performed for clipped softmax (γ, ζ) and gated attention bias initialization (binit). A fine-tuning experiment for OPT-1.3B was also conducted.",
        "limitations": "The scalability of the proposed methods to very large transformers trained for significantly longer durations has not been fully explored, although the underlying issue's fundamental understanding suggests they would still be effective. While some improvements in FP16/FP32 performance were observed, these were not consistent across all hyperparameters, making it unclear if this generalizes to more architectures and larger models. Each proposed method also introduces a new hyperparameter, despite evidence suggesting relative robustness to their specific values.",
        "future_research_directions": "Future research could investigate whether the identified \"no-op\" behavior extends to convolutional neural network architectures. Further exploration into the regularization effect of the small amount of noise added to residuals, even in partial no-op scenarios, might provide insights into neural network generalization. An explanation for why clipped softmax specifically failed to perform well on the OPT model is needed. Additionally, combining the proposed methods with more advanced quantization and weight compression techniques could push the limits of low-bit quantization, and further studies are needed to confirm the generalization of improved floating-point performance to a broader range of architectures and larger models."
      }
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors",
      "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood\naggregation that assigns weights to neighbors according to their importance. In\npractice, however, GATs are often unable to switch off task-irrelevant\nneighborhood aggregation, as we show experimentally and analytically. To\naddress this challenge, we propose GATE, a GAT extension that holds three major\nadvantages: i) It alleviates over-smoothing by addressing its root cause of\nunnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits\nfrom higher depth as it can still utilize additional layers for (non-)linear\nfeature transformations in case of (nearly) switched-off neighborhood\naggregation. iii) By down-weighting connections to unrelated neighbors, it\noften outperforms GATs on real-world heterophilic datasets. To further validate\nour claims, we construct a synthetic test bed to analyze a model's ability to\nutilize the appropriate amount of neighborhood aggregation, which could be of\nindependent interest.",
      "full_text": "GATE: How to Keep Out Intrusive Neighbors Nimrah Mustafa 1 Rebekka Burkholz 1 Abstract Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neigh- borhood aggregation, as we show experimen- tally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over- smoothing by addressing its root cause of un- necessary neighborhood aggregation. ii) Simi- larly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non- )linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neigh- bors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to an- alyze a model’s ability to utilize the appropri- ate amount of neighborhood aggregation, which could be of independent interest. 1. Introduction Graph neural networks (GNNs) (Gori et al., 2005) are a standard class of models for machine learning on graph- structured data that utilize node feature and graph struc- ture information jointly to achieve strong empirical per- formance, particularly on node classification tasks. In- put graphs to GNNs stem from various domains of real- world systems such as social (Bian et al., 2020), com- mercial (Zhang & Chen, 2020), academic (Hamaguchi et al., 2017), economic (Monken et al., 2021), biochem- ical(Kearnes et al., 2016), physical (Shlomi et al., 2021), and transport (Wu et al., 2019) networks that are diverse in their node feature and graph structure properties. 1CISPA Helmholtz Center for Information Security, 66123 Saarbr¨ucken, Germany. Correspondence to: Nimrah Mustafa <nimrah.mustafa@cispa.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). The message-passing mechanism of GNNs (Kipf & Welling, 2017; Xu et al., 2019) involves two key steps: a transformation of the node features, and the aggregation of these transformed features from a node’s neighborhood to update the node’s representation during training. While this has proven to be largely successful in certain cases, it gen- erally introduces some problems for learning with GNNs, the most notorious of which is over-smoothing (Li et al., 2018). The enforced use of structural information in ad- dition to node features may be detrimental to learning the node classification task, as shown by recent results where state-of-the-art GNNs perform the same as or worse than multi-layer perceptrons (MLPs) (Gomes et al., 2022; Yan et al., 2022; Ma et al., 2022). One such task is where node labels can be easily determined by informative node fea- tures and require no contribution from the neighborhood. Here, standard neighborhood aggregation, as in most GNN architectures, would impair model performance, particu- larly with an increase in model depth. A popular standard GNN architecture that, in principle, tries to resolve this problem is the Graph Attention Net- work (GAT) (Veliˇckovi´c et al., 2018; Brody et al., 2022). By design, neighborhood aggregation in GATs is charac- terized by learnable coefficients intended to assign larger weights to more important neighboring nodes (including the node itself) in order to learn better node representations. Therefore, in the above example, GATs should ideally re- sort to assigning near-zero importance to neighbor nodes, effectively switching off neighborhood aggregation. How- ever, we find that, counter-intuitively, GATs are unable to do this in practice and continue to aggregate the uninforma- tive features in the neighborhood which impedes learning. One may ask why one would employ a GAT (or any GNN architecture) if an MLP suffices. In practice, we do not know whether neighborhood aggregation (of raw features or features transformed by a perceptron or MLP), would be beneficial or not beforehand. This raises a pertinent ques- tion for the GNN research community: How much neigh- borhood aggregation is needed for a given task? . Ideally, it is what we would want a model to learn. Otherwise, the right task-specific architecture would need to be identified by time and resource-intensive manual tuning. We address the challenge faced by GAT to effectively de- 1 arXiv:2406.00418v2  [cs.LG]  29 Jul 2024GATE: How to Keep Out Intrusive Neighbors termine how well a node is represented by its own features compared to the features of neighboring nodes, i.e., dis- tinguish between the relative importance of available node features and graph structure information for a given task. Firstly, we provide an intuitive explanation for the problem based on a conservation law of GAT gradient flow dynam- ics derived by Mustafa & Burkholz (2023). Building on this insight, we present GATE, an extension of the GAT architecture that can switch neighborhood aggregation on and off as necessary. This allows our proposed architecture to gain the following advantages over GAT: 1. It alleviates the notorious over-smoothing problem by addressing the root cause of unnecessarily repeated neighborhood aggregation. 2. It allows the model to benefit from more meaningful representations obtained solely by deeper non-linear transformations, similarly to perceptrons, in layers with little to no neighborhood aggregation. 3. It often outperforms GATs on real-world heterophilic datasets by weighing down unrelated neighbors. 4. It offers interpretable learned self-attention coeffi- cients, at the node level, that are indicative of the rel- ative importance of feature and structure information in the locality of the node. In order to validate these claims, we construct a syn- thetic test bed of two opposite types of learning problems for node classification where label-relevant information is completely present only in a node’s i) own features and ii) neighboring nodes’ features (see Fig. 2). GATE is able to adapt to both cases as necessary. On real-world datasets, GATE performs competitively on homophilic datasets and is substantially better than GAT on heterophilic datasets. Furthermore, up to our knowledge, it achieves a new state of the art on the relatively large OGB-arxiv dataset (Hu et al., 2021) (i.e., 79.57 ± 0.84% test accuracy). In sum- mary, our contributions are as follows: • We identify and experimentally demonstrate a struc- tural limitation of GAT, i.e., its inability to switch off neighborhood aggregation. • We propose GATE, an extension of GAT, that over- comes this limitation and, in doing so, unlocks several benefits of the architecture. • We update an existing conservation law relating the structure of gradients in GAT to GATE. • We construct a synthetic test bed to validate our claims, which could be of independent interest to mea- sure progress in developing adaptive neighborhood aggregation schemes. Figure 1: MLP only performs node feature transforma- tions, whereas GAT also always aggregates over the neigh- borhood. With the ability to switch off neighborhood ag- gregation, GATE can learn to emulate MLP behavior and potentially interleave effective perceptron and standard lay- ers in a flexible manner. This allows for more expressive power that we find to benefit real-world tasks (see Table 3). 2. Related Work To relieve GNNs from the drawbacks of unnecessarily re- peated neighborhood aggregation in deeper models, ini- tial techniques were inspired by classical deep learning of MLPs such as normalization (Cai et al., 2021; Zhao & Akoglu, 2020; Zhou et al., 2020; 2021) and regularization (Papp et al., 2021; Rong et al., 2020; Yang et al., 202; Zou et al., 2019). More recently, the need for deeper models and architectural changes to limit neighborhood aggregation as necessary has been recognized. Some approaches use linear combi- nations of initial features and current layer representation (Gasteiger et al., 2019), add skip connections and identity mapping (Chen et al., 2020; Cong et al., 2021), combine representations of all previous layers at the last layer (Xu et al., 2018), aggregate information from a node-wise de- fined range of k-hop neighbors(Liu et al., 2020), and limit the number of aggregation iterations based on node influ- ence scores (Zhang et al., 2021). However, these architec- tures are not flexible enough to utilize additional network layers to simulate perceptron behavior, which, as we find, benefits heterophilic tasks. (Ma et al., 2023) discuss ‘good’ and ‘bad’ heterophily, which are also task-dependent. Other contemporary works for general GNNs propose the use of bilevel optimization to determine a node’s strategic discrete action to a received message (Finkelshtein et al., 2023) and a variational inference framework for adaptive message passing (Errica et al., 2023). While these non- attentive architectures improve message passing in GNNs, we focus on identifying and explaining a structural limita- tion of self-attention in GAT, that continues to be used as a strong baseline architecture. An orthogonal line of research uses graph structural learn- ing (Yang et al., 2019; Stretcu et al., 2019; Franceschi et al., 2020) to amend the input graph structure such that neighborhood aggregation benefits the given task. Such 2GATE: How to Keep Out Intrusive Neighbors approaches are difficult to scale, more susceptible to over- smoothing, and potentially destroy any inherent informa- tion in the original graph structure. On the contrary, a standard GNN architecture equipped to selectively per- form neighborhood aggregation avoids these pitfalls. Self- supervision of the attention mechanism has also been pro- posed (Wang et al., 2019; Kim & Oh, 2021). Methods such as graph rewiring (Deac et al., 2022) to overcome problems such as over-squashing (Alon & Yahav, 2021) are comple- mentary and may also be combined with GATE. While we focus our insights on GAT, architectures based on GAT such as ωGAT (Eliasof et al., 2023) also suffer from the same problem (see Fig. 14 in Appendix C). This further confirms that the universal problem with GAT has been correctly identified. In general, recent works direct ef- fort to understand the current limitations of graph attention (Lee et al., 2023; Fountoulakis et al., 2023). 3. Architecture Notation Consider a graph G = ( V, E) with node set V and edge set E ⊆ V × V, where for a node v ∈ V the neighborhood is N(v) = {u|(u, v) ∈ E} and input features are h0 v. A GNN layer updates each node’s repre- sentation by aggregating over its neighbors’ representation and combining it with its own features. The aggregation and combination steps can be performed together by in- troducing self-loops in G such that, ∀v ∈ V, (v, v) ∈ E. We assume the presence of self-loops in G unless spec- ified otherwise. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indi- cate the importance of node u for v. A network is con- structed by stacking L layers, defined as follows, using a non-linear activation function ϕ that is homogeneous (i.e ϕ(x) = xϕ′(x)) and consequently, ϕ(ax) = aϕ(x) for positive scalars a) such as ReLU ϕ(x) = max {x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. GAT Given input representations hl−1 v for v ∈ V, a GAT 1 layer l ∈ [L] transforms those to: hl v = ϕ   X u∈N(v) αl uv · Wl shl−1 u  , where (1) αl uv = exp \u0000 el uv \u0001 P u′∈N(v) exp \u0000 el u′v \u0001, and (2) el uv = \u0000 al\u0001⊤ · ϕ \u0000 Wl shl−1 u + Wl thl−1 v \u0001 (3) The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that Ws = Wt. We denote this variant of GAT by GATS. 1Throughout, we refer to GATv2 (Brody et al., 2022) as GAT. GATE In addition, we propose GATE, a GAT variant that flexibly weights the importance of node features and neigh- borhood features. A GATE layer is also defined by Eq. (1) and (2) but modifies euv in Eq. (3) to Eq. (4). el uv = \u0000 1u̸=val s + 1u=val t \u0001⊤ ·ϕ \u0000 Ulhl−1 u + Vlhl−1 v \u0001 . (4) We denote euv in Eq. (3) and (4) as el vv if u = v. For GATE, Wl s in Eq. (1) is denoted as Wl. A weight- sharing variant of GATE, GATE S, is characterized by all feature transformation parameters being shared in a layer (i.e. Wl = Ul = Vl). Note that, then, for a d-dimensional layer, GATE adds onlyd more parameters to GAT. We next present theoretical insights into the reasoning be- hind the inability of GATs to switch off neighborhood ag- gregation, which is rooted in norm constraints imposed by the inherent conservation law for GATs. The gradients of GATE fulfill an updated conservation law (Thoerem 4.3) that enables switching off neighborhood aggregation in a parameter regime with well-trainable attention. 4. Theoretical Insights For simplicity, we limit our discussion here to GATs with weight sharing. We derive similar arguments for GATs without weight sharing in Appendix A.1. The following conservation law was recently derived for GATs to explain trainability issues of standard initialization schemes. Even with improved initializations, we argue that this law lim- its the effective expressiveness of GATs and their ability to switch off neighborhood aggregation when necessary. Theorem 4.1 (Thm. 2.2 by Mustafa & Burkholz (2023)) . The parameters θ of a layer l ∈ [L − 1] in a GAT network and their gradients ∇θL w.r.t. loss L fulfill: ⟨Wl [i,:], ∇Wl [i,:] ⟩ = ⟨Wl+1 [:,i] , ∇Wl+1 [:,i] ⟩ + ⟨al [i], ∇al [i] ⟩. (5) Intuitively, this equality limits the budget for the relative change of parameters and imposes indirectly a norm con- straint on the parameters. Under gradient flow that assumes infinitesimally small learning rates, this law implies that the relationship \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c stays constant during training, where c is defined by the initial norms. Other gradient-based optimizers fulfill this norm balance also approximately. Note that the norms\r\rWl[i, :] \r\r generally do not assume arbitrary values but are determined by the required scale of the output. Deeper models are especially less flexible in varying these norms as deviations could lead to exploding or diminishing out- puts and/or gradients. In consequence, the norms of the attention parameters are also bounded. Furthermore, a pa- rameter becomes harder to change during training when its 3GATE: How to Keep Out Intrusive Neighbors magnitude increases. This can be seen by transforming the law with respect to the relative change of a parameter de- fined as ∆θ = ∇θL/θ for θ ̸= 0 or ∆θ = 0 for θ = 0. nl+1X j=1 Wl ij 2 ∆Wl ij = nl+2X k=1 Wl+1 ki 2 ∆Wl+1 ki + al i 2 ∆al i. (6) The higher the magnitude of an attention parameter (al i)2, the smaller will be the relative change ∆al i and vice versa. This restricts the attention mechanism in the network to a less-trainable regime without converging to a meaningful model. We next explain why large \r\ral\r\r are required to switch off neighborhood aggregation in a layer. Insight 4.2 (Effective expressiveness of GATs). GATs are challenged to switch off neighborhood aggregation during training, as this requires the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. An intuitive derivation of this insight is presented in the appendix. Here, we outline the main argument based on the observation that to make the contribution of neighbor j insignificant relative to node i, we require αij/αii << 1. We use relativeαij/αii instead of αij and αii to cancel out normalization constants and simplify the analysis. Our key observation is that, given an insignificant link (i, j), its relative contribution to its two neighborhoods αij/αii << 1 and αji/αjj << 1 are affected in oppo- site ways by a feature f of the attention parameters a, i.e. if a[f] contributes to reducing αij/αii, it automatically in- creases αji/αjj . However, we require multiple features that contribute to reducing only αij without strengthen- ing αji that may only be possible in a high dimensional space requiring large norms of a. Yet, the norms ∥a∥2 are constrained by the parameter initialization and cannot in- crease arbitrarily due to the derived conservation law. Note that to switch off all neighborhood aggregation, we require αij/αii << 1, ∀ j ∈ N(i), further complicating the task. To address this challenge, we modify the GAT architecture by GATE that learns separate attention parameters for the node and the neighborhood contribution. As its conserva- tion law indicates, it can switch off neighborhood aggrega- tion in the well-trainable parameter regime. Theorem 4.3 (Structure of GATE gradients). The param- eters and gradients of a GATE network w.r.t. to loss L for layer l ∈ [L − 1] are conserved according to the following laws. Given Θ(θ) = ⟨θ, ∇θL⟩, it holds that: Θ(Wl [i,:]) − Θ(al+1 s [i]) − Θ(al+1 t [i]) = Θ(Wl+1 [:,i] ). (7) and, if additional independent matrices Ul and Vl are trainable, it also holds that: Θ(al s[i]) + Θ(al t[i]) = Θ(Ul [i,:]) + Θ(Vl [i,:]). (8) The proof is provided in the appendix. We utilize this the- orem for two purposes. Firstly, it induces an initialization that enables at least the initial trainability of the network. Similarly to GAT (Mustafa & Burkholz, 2023), we initial- ize all a parameters with zero and matrices W with ran- dom orthogonal looks-linear structure in GATE. This also ensures that we have no initial inductive bias or preference for specific neighbor or node features. As an ablation, we also verify that the initialization of the attention parameters in GAT with zero alone can not switch off neighborhood aggregation in GAT (see Fig. 7 in Appendix C). Secondly, the conservation law leads to the insight that a GATE network is more easily capable of switching off neighborhood aggregation or node feature contributions in comparison with GAT. Insight 4.4 (GATE is able to switch off neighborhood ag- gregation.). GATE can flexibly switch off neighborhood aggregation or node features in the well-trainable regime of the attention parameters. This insight follows immediately from the related conser- vation law for GATE that shows that at and as can inter- change the available budget for relative change among each other. Furthermore, the contribution of neighbors and the nodes are controlled separately. We show how the respec- tive switch-off can be achieved with relatively small atten- tion parameter norms that correspond to the well-trainable regime in Appendix A.3. To verify these insights in exper- iments, we next design synthetic data generators that can test the ability of GNNs to take structural infromation into account in a task-appropriate manner. 5. Experiments We validate the ability of GATE to perform the appropri- ate amount of neighborhood aggregation, as relevant for the given task and input graph, on both synthetic and real- world graphs. In order to gauge the amount of neighbor- hood aggregation, we study the distribution of αvv values (over the nodes) at various epochs during training and lay- ers in the network. This serves as a fair proxy since ∀ v ∈ V, αvv = 1 − P u∈N(v),u̸=v αuv. Thus, αvv = 1 implies no neighborhood aggregation (i.e. only hv is used) whereas αvv = 0 implies only neighborhood aggregation (i.e. hv is not used). Figure 2 shows an exemplary con- struction of both these cases. We defer a discussion of the experimental setup to Appendix B. 5.1. Synthetic Test Bed We construct the synthetic test bed as a node classification task for two types of problems: self-sufficient learning and neighbor-dependent learning. In the self-sufficient learning problem, complete label-relevant information is present in 4GATE: How to Keep Out Intrusive Neighbors (a) No neighborhood contribution required. (b) Only neighborhood contribution required. Figure 2: Examples of synthetic input graphs constructed for learning tasks that are (a) self-sufficient and can be bet- ter solved by switching off neighborhood aggregation, i.e. αvv = 1 and (b) neighbor-dependent that benefit from ig- noring the node’s own features, i.e.αvv = 0. In both cases, ∀ v ∈ V, P u∈N(v),u̸=v αuv + αvv = 1. These represent opposite ends of the spectrum whereas real-world tasks of- ten lie in between and require αii ∈ [0, 1]. GATE’s atten- tion mechanism is more flexible than GAT’s in learning the level of neighborhood aggregation required for a task. a node’s own features. On the contrary, in the neighbor- dependent learning problem, label-relevant information is present in the node features of the k-hop neighbors. We discuss both cases in detail, beginning with the simpler one. Learning self-sufficient node labels In order to model this task exactly, we generate an Erd ˝os–R´enyi (ER) graph structure G with N = 1000 nodes and edge probabil- ity p = 0 .01. Node labels yv are assigned uniformly at random from C = [2 , 8] classes. Input node features h0 v are generated as one-hot encoded node labels in both cases, i.e., h0 v = 1yv . Nodes are divided randomly into train/validation/test split with a 2 : 1 : 1 ratio. We also use a real-world graph structure of the Cora dataset. Two cases using this graph structure are tested: i) using the original node labels consisting of7 classes, and ii) randomized labels of 7 classes. Input node features are generated as one-hot encoding of node labels in both cases. The standard splits of Cora are used. Table 1: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of net- work layers, respectively. Original (Orig.) and randomized (Rand.) labels are used for the Cora structure. In all cases, 100% train accuracy is achieved except in ones marked with ∗ and GATE eventually achieves 100% test accuracy a few epochs later except in one marked with ‡. GAT S and GAT models marked with † also eventually achieve 100% test accuracy. GATE S behaves similarly to GATE and achieves 100% train and test accuracy. . S C L Test Acc.@Epoch of Min. Train Loss GATS GAT GATE Cora O,7 1 99.1@215† 97.7@166† 99.0@127 2 93.4@218 94.5@158 99.6@35 5 85.9@92 85.5@72 98.4@36‡ R,7 1 99.4@263 † 99.8@268† 100@104 2 61.7@2088 ∗ 52.8@341∗ 99.9@36 5 35.1@609 32.1@1299 99.9@23 ER(p = 0.01) R,2 1 100@341 † 100@182† 100@1313 2 99.2@100 † 99.2@119† 99.6@79 5 64.0@7778 ∗ 99.6@239 100@45 R,8 1 88.8@9578 ∗ 98.4@3290 99.2@1755 2 90.4@2459 ∗ 94.8@2237 99.6@44 5 23.6@8152 26.0@8121 100@28 As evident in Table 1, GAT is unable to perfectly learn this task whereas GATE easily achieves100% train and test ac- curacy, and often in fewer training epochs. In line with the homophilic nature of Cora, GAT achieves reasonably good accuracy when the original labels of the Cora graph structure are used as neighborhood aggregation is relatively less detrimental, particularly in a single-layer model. Nevertheless, in the same case, GATE generalizes better than GAT with an increase in model depth. This in- dicates that over-smoothing, a major cause of performance degradation with model depth in GNNs, is also alleviated due to reduced neighborhood aggregations (see Fig. 3). On the contrary, random labels pose a real challenge to GAT. Since the neighborhood features are fully uninforma- tive about a node’s label in the randomized case, aggrega- tion over such a neighborhood distorts the fully informa- tive features of the node itself. This impedes the GAT net- work from learning the task, as it is unable to effectively switch off aggregation (see Fig. 3), whereas GATE is able to adapt to the required level of neighborhood aggregation (i.e. none, in this case). Interestingly, note that a single 5GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 3: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 1 and 2 layer models. Due to space limitation, we defer the plots of 5 layer networks to Fig. 8 in Appendix C. layer GAT in the case of random labels can almost, though not completely, switch off neighborhood aggregation (see Fig. 3) and achieve (near) perfect accuracy in the simpler cases. This is in line with our theoretical analysis (see In- sight 4.2), as the norms of a single-layer model are not con- strained and thus the attention parameters have more free- dom to change. Overall, the accuracy of GAT worsens drastically along two dimensions simultaneously: i) an increase in the depth of the model (due to increased unnecessary aggregation), and ii) an increase in the complexity of the task (due to an in- crease in the number of classes in an ER graph and conse- quently in node neighborhoods). In the interest of space, we defer results for GATE S to Fig. 9 in Appendix C as aggregation patterns similar to GATE are observed. Having established that GATE excels GAT in avoiding task-irrelevant neighborhood aggregation, it is also impor- tant to verify whether GATE can perform task-relevant neighborhood aggregation when required, and as much as required. We answer this question next by studying the be- havior of GATE, in comparison to GAT, on a synthetically constructed neighbor-dependent learning problem. Learning neighbor-dependent node labels To model this task, we generate an ER graph structure with N = 1000 nodes and edge probability p = 0 .01. Input node features h0 v ∈ Rd are sampled from a multivariate normal distribution N(0d, Id). For simplicity, d = 2. This input graph G is fed to a random GAT network Mk with k layers of width d. Note that this input graph G has no self-loops on nodes (i.e. v /∈ N(v)).The parameters of Mk are initialized with the standard Xavier (Glorot & Ben- gio, 2010) initialization. Thus, for each node v, the node embedding output by Mk, hMk v is effectively a function f Table 2: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Mean test accuracy ±95% confi- dence interval over 5 runs is reported. In all cases, a GATE variant outperforms the GAT variants. We further analyze one experimental run in detail in Table 8 in Appendix C. k L GATS GAT GATE S GATE 1 1 93 .6±1.3 92 .3±1.3 96.4 ± 0.7 93.5±1.3 2 93 .5±0.7 92 .7±2.7 97.9 ± 0.8 94.6±2.1 3 88 .2±4.9 91 .8±3.4 92 .1±4.6 94.0 ± 1.5 2 2 90 .4±1.3 87 .7±1.6 93.8 ± 0.5 88.7±2.5 3 82 .2±4.5 88 .9±2.1 85 .8±2.5 93.4 ± 3.3 4 84 .0±5.0 83 .0±4.8 89.2 ± 2.3 87.8±2.4 3 3 84 .3±3.2 83 .8±2.7 87 .5±1.8 88.6 ± 2.0 4 71 .4±3.9 75 .9±7.6 89.2 ± 1.0 89.0±0.5 5 80 .2±4.8 83 .9±2.2 86 .1±0.8 87.8 ± 1.6 of the k-hop neighboring nodes of node v represented by a random GAT network. Let Nk(v) denote the set of k-hop neighbors of v and v /∈ Nk(v). Finally, we run K-means clustering on the neighborhood aggregated representation of nodes hMk v to divide nodes into C clusters. For simplicity, we set C = 2. This cluster- ing serves as the node labels (i.e. yv = argc∈[C](v ∈ c) for our node classification task. Thus, the label yv of a node v to be learned is highly dependent on the input features of the neighboring nodes h0 u ∈ Nk(v) rather than the node’s own input features h0 v. The generated input data and the real decision boundary for varying k are shown in Fig. 4. Corresponding results in Table 2 and Fig. 5 exhibit that GATE can better detect the amount of necessary neighborhood aggregation than GAT. However, this task is more challenging than the previous one, and GATE too can not achieve perfect100% test accu- racy. This could be attributed to data points close to the real decision boundary which is not crisply defined (see Fig. 4). 5.2. Real-World Data To demonstrate that the ability of GATE to switch-off neighborhood aggregation has real application relevance, we evaluate GATE on relatively large-scale real-world node classification tasks, namely on five heterophilic benchmark datasets (Platonov et al., 2023) (see Table 3) and three OGB datasets (Hu et al., 2021) (see Table 5). We defer results and discussion on five small-scale datasets with varying homophily levels to Table 7 in Appendix C. To analyze 6GATE: How to Keep Out Intrusive Neighbors (a) k = 1  (b) k = 2  (c) k = 3 Figure 4: (a)-(c): Distribution of node labels of a synthetic dataset, with neighbor-dependent node labels, based on nodes’ own random features (left) and neighbors’ features aggregated k times (right). Figure 5: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 3. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 3, 4, and 5 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 3 layers of the 4 and 5 layer models perform neighborhood aggregation. the potential benefits of combining MLP and GAT layers in GATE, we compare its behavior with GAT and MLP. We argue that the better performance of GATE, by a large margin in most cases, can be attributed to down-weighting unrelated neighbors, leveraging deeper non-linear feature transformations, and reducing over-smoothing. While we focus our exposition on the neighborhood aggre- gation perspective of GATs, we also consider the FAGCN architecture (Bo et al., 2021), which relies on a similar at- tention mechanism and, in theory, could switch off neigh- borhood aggregation when positive and negative contribu- tions of neighbors cancel out. In contrast to GATE, it re- quires tuning a hyperparameter ϵ, which controls the con- tribution of raw node features to each layer. Furthermore, on our synthetic tasks, we find that, like GAT, FAGCN is also unable to limit neighborhood contribution. We also provide a detailed qualitative and quantitative discussion comparing GATE and FAGCN using the synthetic testbed in Appendix C. Next, we analyze GATE’s ability to mix MLP and GAT layers. To this end, we evaluate a GNN architecture con- structed by alternately placing GAT and MLP layers in the network that we denote by MLP +GAT on various het- erophilic tasks. The purpose of this experiment is twofold. Firstly, we observe in Table 3 that MLP+GAT outperforms both GAT and MLP in most cases. This highlights the benefit of only performing non-linear transformations on raw or aggregated neighborhood features without immedi- ate further neighborhood aggregation to learn potentially more complex features. Secondly, we find GATE to outper- form MLP+GAT (see Table 3). This illustrates that rigidly embedding MLP layers in a GNN with arbitrary predefined roles is not ideal as the appropriate degree and placement of neighborhood aggregation is unknown a-priori. In con- trast, GATE offers more flexibility to learn intricate com- binations of GNN layers and nonlinear feature transforma- tions that define more adequate models for a given task, as exemplified in Fig. 6. The distributions ofαvv in Fig. 6 across layers in GATE re- veal information about the relative importance of node fea- ture and graph structure at the node level, which allows us to analyze the question to which degree graph information is helpful for a task. For example, in Fig. 6, we observe that the αvv values are mostly lower in the minesweeper dataset than the roman-empire dataset. This indicates that aggregation, particularly over the input node features and the final layer’s learned representations, is more beneficial compared to the node’s own features for the minesweeper dataset. On the other hand, for roman-empire, the model has a higher preference to utilize features of the node it- self (as most values of αvv approach 1) over features of the neighbors. This aligns with the homophily levels, 0.05 and 0.68, of the roman-empire and minesweeper datasets, re- spectively. A similar analysis for datasets Texas and Actor can be found in Fig. 13 in Appendix C. We also observe in Fig. 6 that when neighborhood aggrega- 7GATE: How to Keep Out Intrusive Neighbors Table 3: We report mean test accuracy ±95% confidence interval for roman-empire and amazon-ratings and AUC-ROC for the other three datasets over the standard 10 splits, following (Platonov et al., 2023). All architectures were run with networks of depth 5 and 10 layers. The better performance for each architecture is shown with the number of network layers used in parentheses. GATE outperforms GAT and other baselines on all datasets, mostly by a significant margin. roman-empire amazon-ratings questions minesweeper tolokers GAT 26.10 ± 1.25 (5) 45 .58 ± 0.41 (10) 57 .72 ± 1.58 (5) 50 .83 ± 0.41 (5) 63 .57 ± 1.03 (10) MLP 65.12 ± 0.25 (5) 43 .26 ± 0.34 (5) 59 .44 ± 0.94 (10) 50 .74 ± 0.56 (5) 62 .67 ± 1.06 (10) MLP+GAT 70.83 ± 0.39 (5) 45 .25 ± 0.17 (10) 59 .12 ± 1.57 (10) 60 .07 ± 1.11 (5) 65 .85 ± 0.64 (10) FAGCN 67.55 ± 0.81 (5) 42 .85 ± 0.83 (10) 60 .38 ± 1.21 (5) 63 .38 ± 0.91 (10) 60 .89 ± 1.12 (5) GATE 75.55 ± 0.30 (5) 45.73 ± 0.24(10) 62.95 ± 0.71 (5) 66.14 ± 1.57 (5) 66.63 ± 1.15(10) (a) roman-empire, GAT:28.96% test accuracy.  (b) roman-empire, GATE: 75.94% test accuracy. (c) Minesweeper, GAT:50.50% test AUROC.  (d) Minesweeper, GATE:67.57% test AUROC. Figure 6: Distribution of αvv against training epoch for a run of 5 layer networks on real-world heterophilic tasks. As ex- pected, GAT is unable to significantly vary neighborhood aggregation whereas GATE exhibits varying aggregation patterns across layers and tasks. These could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods for a given task. We defer the plots of the 10-layer models for tolokers dataset to Fig. 11 in Appendix C. tion takes place, the level of aggregation across all nodes, as indicated by the shape of αvv distribution, varies over network layers. This is expected as different nodes need different levels of aggregation depending on where they are situated in the graph topology. For example, peripheral nodes would require more aggregation than central nodes to obtain a similar amount of information. Therefore, as already observed with purposefully constructed synthetic data, GATE offers a more interpretable model than GAT in a real-world setting. While we focus our evaluation in Table 3 on comparison with the most relevant baselines such as attention-based ar- chitectures, we next present a more extensive comparison with 14 other baseline architectures in Table 4. For the re- sults reported in Table 3, we conduct experiments in a sim- ple setting without additional elements that may impact the performance such as skip connections, normalization, etc., to isolate the effect of the architecture and evaluate solely the impact of GATE’s ability to switch off neighborhood aggregation on real-world data. However, for the results in Table 4, we adopt the original codebase of (Platonov et al., 2023), which utilizes such elements to evaluate the performance of baseline GNNs and architectures specifi- cally designed for heterophilic datasets. We evaluate GATE in the same settings optimized for their experiments. For easy comparison, we replicate their results from Table 4 in (Platonov et al., 2023). We observe in Table 4 that while GATE outperforms GAT (and other baselines) significantly, GATE has comparable performance to GAT-sep, a variant of GAT, despite GATE being more parameter efficient by an order of magnitude. More specifically, GAT-sep and GATE introduce d2 and d additional parameters, respectively, in a layer. By corre- spondingly adapting GATE, we find GATE-sep to achieve the best performance in most cases. Therefore, additional techniques generally employed to boost performance are compatible and complementary to GATE. GATE’s ability to benefit from depth in terms of generaliza- tion is demonstrated on OGB datasets (see Table 5). In par- 8GATE: How to Keep Out Intrusive Neighbors Table 4: An extensive comparison of GATE with baseline GNNs using the experimental setup of (Platonov et al., 2023). Accuracy is reported for roman-empire and amazon-ratings, and ROC AUC is reported for the remaining three datasets. roman-empire amazon-ratings minesweeper tolokers questions GATE 89.51 ± 0.49 52.49 ± 0.46 92.82 ± 0.90 84.62 ± 0.69 78.46 ± 1.17 GAT 80.87 ± 0.30 49.09 ± 0.63 92.01 ± 0.68 83.70 ± 0.47 77.43 ± 1.20 GATE-sep 89.78 ± 0.54 54.51 ± 0.38 94.18 ± 0.43 84.48 ± 0.57 78.20 ± 1.00 GAT-sep 88.75 ± 0.41 52.70 ± 0.62 93.91 ± 0.35 83.78 ± 0.43 76.79 ± 0.71 GT 86.51 ± 0.73 51.17 ± 0.66 91.85 ± 0.76 83.23 ± 0.64 77.95 ± 0.68 GT-sep 87.32 ± 0.39 52.18 ± 0.80 92.29 ± 0.47 82.52 ± 0.92 78.05 ± 0.93 GCN 73.69 ± 0.74 48.70 ± 0.63 89.75 ± 0.52 83.64 ± 0.67 76.09 ± 1.27 SAGE 85.74 ± 0.67 53.63 ± 0.39 93.51 ± 0.57 82.43 ± 0.44 76.44 ± 0.62 H2GCN 60.11 ± 0.52 36.47 ± 0.23 89.71 ± 0.31 73.35 ± 1.01 63.59 ± 1.46 CPGNN 63.96 ± 0.62 39.79 ± 0.77 52.03 ± 5.46 73.36 ± 1.01 65.96 ± 1.95 GPR-GNN 64.85 ± 0.27 44.88 ± 0.34 86.24 ± 0.61 72.94 ± 0.97 55.48 ± 0.91 FSGNN 79.92 ± 0.56 52.74 ± 0.83 90.08 ± 0.70 82.76 ± 0.61 78.86 ± 0.92 GloGNN 59.63 ± 0.69 36.89 ± 0.14 51.08 ± 1.23 73.39 ± 1.17 65.74 ± 1.19 FAGCN 65.22 ± 0.56 44.12 ± 0.30 88.17 ± 0.73 77.75 ± 1.05 77.24 ± 1.26 GBK-GNN 74.57 ± 0.47 45.98 ± 0.71 90.85 ± 0.58 81.01 ± 0.67 74.47 ± 0.86 JacobiConv 71.14 ± 0.42 43.55 ± 0.48 89.66 ± 0.40 68.66 ± 0.65 73.88 ± 1.16 ResNet 65.88 ± 0.38 45.90 ± 0.52 50.89 ± 1.39 72.95 ± 1.06 70.34 ± 0.76 ResNet+SGC 73.90 ± 0.51 50.66 ± 0.48 70.88 ± 0.90 80.70 ± 0.97 75.81 ± 0.96 ResNet+adj 52.25 ± 0.40 51.83 ± 0.57 50.42 ± 0.83 78.78 ± 1.11 75.77 ± 1.24 Table 5: Mean test accuracy ±95% confidence interval (and number of network layers). We replicate the results for GAT reported by (Brody et al., 2022). GATE leverages deeper networks to substantially outperform GAT. OGB- GAT GATE arxiv 71.87 ± 0.16 (3) 79.57 ± 0.84 (12) products 80.63 ± 0.46 (3) 86.24 ± 1.01 (8) mag 32.61 ± 0.29 (2) 35.29 ± .36 (5) ticular, GATE improves the SOTA test accuracy (78.03%) on the arxiv dataset achieved by a model using embed- dings learned by a language model instead of raw node features(Duan et al., 2023), as reported on the OGB leader- board. While the better performance of deeper models with limited neighborhood aggregation in certain layers indi- cates reduced over-smoothing, we also verify this insight quantitatively (see Table 9 in Appendix C). Our experimental code is available at https:// github.com/RelationalML/GATE.git. 6. Conclusion We experimentally illustrate a structural limitation of GAT that disables the architecture, in practice, to switch off task- irrelevant neighborhood aggregation. This obstructs GAT from achieving its intended potential. Based on insights from an existing conservation law of gradient flow dynam- ics in GAT, we have explained the source of this problem. To verify that we have identified the correct issue, we re- solve it with a modification of GAT, which we call GATE, and derive the corresponding modified conservation law. GATE holds multiple advantages over GAT, as it can lever- age the benefits of depth as in MLPs, offer interpretable, learned self-attention coefficients, and adapt the model to the necessary degree of neighborhood aggregation for a given task. We verify this on multiple synthetic and real- world tasks, where GATE significantly outperforms GAT and also achieves a new SOTA test accuracy on the OGB- arxiv dataset. Therefore, we argue that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. 9GATE: How to Keep Out Intrusive Neighbors Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. Bian, T., Xiao, X., Xu, T., Zhao, P., Huang, W., Rong, Y ., and Huang, J. Rumor detection on social media with bi- directional graph convolutional networks. In AAAI Con- ference on Artificial Intelligence, 2020. Bo, D., Wang, X., Shi, C., and Shen, H. Beyond low- frequency information in graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. Burkholz, R. and Dubatovka, A. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. In Graph Representation Learning Workshop, International Conference on Machine Learn- ing, 2020. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y ., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Simple and deep graph convolutional networks. InInternational Conference on Machine Learning, 2020. Cong, W., Ramezani, M., and Mahdavi, M. On provable benefits of depth in training graph convolutional net- works. In Advances in Neural Information Processing Systems, 2021. Deac, A., Lackenby, M., and Veliˇckovi´c, P. Expander graph propagation. In Learning on Graphs Conference, 2022. Duan, K., Liu, Q., Chua, T.-S., Yan, S., Ooi, W. T., Xie, Q., and He, J. Simteg: A frustratingly simple approach improves textual graph learning, 2023. Eliasof, M., Ruthotto, L., and Treister, E. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general framework to mitigate oversmoothing, over- squashing, and underreaching, 2023. Finkelshtein, B., Huang, X., Bronstein, M., and ˙Ismail ˙Ilkan Ceylan. Cooperative graph neural networks, 2023. Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. In Journal of Machine Learning Research, 2023. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learn- ing discrete structures for graph neural networks. In In- ternational Conference on Machine Learning, 2020. Gasteiger, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2019. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 9, pp. 249–256, May 2010. Gomes, D., Ruelens, F., Efthymiadis, K., Nowe, A., and Vrancx, P. When are graph neural networks better than structure agnostic methods? In Neural Information Pro- cessing Systems Workshop ICBINB, 2022. Gori, M., Monfardini, G., and Scarselli, F. A new model for learnig in graph domains. InIEEE International Joint Conference on Neural Networks, 2005. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach. In International Joint Conference on Artificial Intelligence, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021. 10GATE: How to Keep Out Intrusive Neighbors Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. Kim, D. and Oh, A. How to find your friendly neighbor- hood: Graph attention design with self-supervision. In International Conference on Learning Representations , 2021. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. InInternational Conference on Learning Representations, 2017. Lee, S. Y ., Bu, F., Yoo, J., and Shin, K. Towards deep attention in graph neural networks: Problems and reme- dies. In International Conference on Machine Learning, 2023. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Liu, M., Gao, H., and Ji, S. Towards deeper graph neu- ral networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2022. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2023. Monken, A., Haberkorn, F., Gopinatha, M., Freeman, L., and Batarseh, F. A. Graph neural networks for modeling causality in international trade. In AAAI Conference on Artificial Intelligence, 2021. Mustafa, N. and Burkholz, R. Are GATS out of balance? In Advances in Neural Information Processing Systems, 2023. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. In Advances in Neural Infor- mation Processing Systems, 2021. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations , 2020. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evalua- tion of gnns under heterophily: are we really making progress? In International Conference on Learning Rep- resentations, 2023. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node clas- sification. In International Conference on Learning Rep- resentations, 2020. Shlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. 2021. Stretcu, O., Viswanathan, K., Movshovitz-Attias, D., Pla- tanios, E., Ravi, S., and Tomkins, A. Graph agreement models for semi-supervised learning. In Advances in Neural Information Processing Systems, 2019. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations , 2018. Wang, G., Ying, R., Huang, J., and Leskovec, J. Improving graph attention networks with large margin-based con- straints. In Graph Representation Learning Workshop, Neural Information Processing Systems, 2019. Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Yan, Y ., Hashemi, M., Swersky, K., Yang, Y ., and Koutra, D. Two sides of the same coin: Heterophily and over- smoothing in graph convolutional neural networks. In IEEE International Conference on Data Mining, 2022. Yang, H., Ma, K., and Cheng, J. Rethinking graph regular- ization for graph neural networks. InAdvances in Neural Information Processing Systems, 202. Yang, L., Kang, Z., Cao, X., Jin, D., Yang, B., and Guo, Y . Topology optimization based graph convolutional net- work. In International Joint Conference on Artificial In- telligence, 2019. Zhang, M. and Chen, Y . Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. Zhang, W., Yang, M., Sheng, Z., Li, Y ., Ouyang, W., Tao, Y ., Yang, Z., and Cui, B. Node dependent local smooth- ing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. 11GATE: How to Keep Out Intrusive Neighbors Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth- ing in gnns. In International Conference on Learning Representations, 2020. Zhou, K., Huang, X., Li, Y ., Zha, D., Chen, R., and Hu, X. Towards deeper graph neural networks with differen- tiable group normalization. In Advances in Neural Infor- mation Processing Systems, 2020. Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in graph convolutional networks. InConfer- ence on Information and Knowledge Management, 2021. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 12GATE: How to Keep Out Intrusive Neighbors A. Theoretical Derivations A.1. Derivation of Insight 4.2 Statement (Restated Insight 4.2) . GATs are challenged to switch off neighborhood aggregation during training, as this would require the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. We have to distinguish GATs with and without weight sharing in our analysis. GATs with weight sharing: To investigate the ability of a GAT to switch off neighborhood aggregation, let us focus on a link(i, j) that should neither contribute to the feature transformation of i nor j. This implies that we need to find attention parameters a (and potentially feature transformations W) so that αij/αii << 1 with αij/αii = exp ( eij − eii). This implies that we require eij − eii << 0 and thus aT ϕ (W (hi + hj)) − 2aT ϕ (W (hi)) << 0. Since we also require αij/αjj << 1, it follows from adding both inequalities that aT [ϕ (W (hi + hj)) − (ϕ (Whi) + ϕ (Whj))] << 0. This inequality can only be fulfilled if there exists at least one feature f for which ∆fij ; = a[f] [ϕ (W[f, :] (hi + hj)) − (ϕ (W[f, :]hi) + ϕ (W[f, :]hi))] fulfills ∆fij << 0. Yet, note that if both ϕ (W[f, :]hi) and ϕ (W[f, :]hj) are positive or both are negative, we just get ∆fij = 0 because of the definition of a LeakyReLU. Thus, there must exist at least one feature f so that without loss of generality ϕ (W[f, :]hi) < 0 and ϕ (W[f, :]hj) > 0. It follows that if a[f] > 0 that 0 > a[f]ϕ (W[f, :]hi) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hj)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hj)) also receives a negative contribution that makes αij/αjj smaller. Yet, what happens to αij/αii? By distinguishing two cases, namely W[f, :] (hi + hj) > 0 or W[f, :] (hi + hj) < 0 and computing a[f] [ϕ (W (hi + hj)) − 2ϕ (W[f, :]hj)] > 0 we find the feature contribution to be positive. If a[f] < 0, then 0 > a[f]ϕ (W[f, :]hj) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hi)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hi)) and αij/αjj is reduced. Similarly, we can derive that at the same time αij/αii is increased, however. This implies that any feature that contributes to reducing ∆fij automatically increases one feature while it increases another. We therefore need multiple featuresf to contribute to reducing either αij/αii or αij/αjj to compensate for other increases. This implies, in order to switch off neighborhood aggregation, we would need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Yet, all these norms are constrained by the derived conservation law and therefore prevent learning a representation that switches off full neighborhoods. GATs without weight sharing: The flow of argumentation without weight sharing is very similar to the one above with weight sharing. Yet, we have to distinguish more cases. 13GATE: How to Keep Out Intrusive Neighbors Similarly to before, we require αij/αjj << 1 and αji/αii << 1. It follows from adding both related inequalities that aT [ϕ (Wshi + Wthj) + ϕ (Wshj + Wthi) − ϕ ((Ws + Wt) hi) − ϕ ((Ws + Wt) hj)] << 0. This implies that for at least one feature f, we require a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ ((Ws[f, :] + Wt[f, :]) hi) − ϕ ((Ws[f, :] + Wt[f, :]) hj)] << 0. (9) Again, our goal is to show that this feature automatically decreases the contribution of one feature while it increases another. As argued above, switching off neighborhood aggregation would therefore need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Our derived norm constraints, however, prevent learning such a model representation. Concretely, without loss of generality, we therefore have to show that if a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 < 0, (10) at the same time, we receive a[f][ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 > 0, (11) (or vice versa). In principle, we have to show this for 16 different cases of pre-activation sign configurations for the four terms in Eq. (9). Yet, since the argument is symmetric with respect to exchanging i and j, only 8 different cases remain. Two trivial cases are identical signs for all four terms. These are excluded, as the left hand side (LHS) of Eq. (9) would become zero and thus not contribute to our goal to switch off neighborhood aggregation. In the following, we will discuss the remaining six cases. Please note that for the remainder of this derivation α >0 denotes the slope of the leakyReLU and not the attention weights αij. 1. Case (+ − ++): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj > 0. From this assumption and the fact that ϕ is a leakyReLU it follows that the LHS of Eq. (9) be- comes: a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 ] = a[f](α −1)[Ws[f, :]hj + Wt[f, :]hi]. Since α −1 < 0 and [Ws[f, :]hj + Wt[f, : ]hi] < 0 according to our assumption, Eq. (9) demands a[f] < 0. To switch off neighborhood aggregation, we would need to be able to make the LHS of Eq. (10) and Eq. (11) Eq. (11) negative. Yet, a negative a[f] leads to a positive LHS of Eq. (11). Thus, the assumed sign configuration cannot support switching off neighborhood aggregation. 2. Case (+ − −−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hi + Wt[f, :]hj], which demands a[f] < 0. Accordingly, the LHS of Eq. (10) is clearly negative, while the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that implyWs[f, :]hj < Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj < 0 and Ws[f, :]hi + Wt[f, :]hj > 0. Again, this result implies that the considered sign configuration does not support switching off neighborhood aggregation. 3. Case (+ + + −): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi > 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes positive, which hampers switching-off neighborhood aggregation as discussed. 14GATE: How to Keep Out Intrusive Neighbors 4. Case (− − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj < 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. The LHS of Eq. (9) becomes a[f](α −1)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes clearly negative. However, the LHS of Eq. (11) is positive, asa[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that imply Ws[f, :]hj > Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj > 0 and Ws[f, :]hi + Wt[f, :]hj < 0. Again, this analysis implies that the considered sign configuration does not support switching off neighborhood aggregation. 5. Case (+ − +−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. According to our assumptions the LHS of Eq. (10) can only be negative if a[f] < 0. Yet, the LHS of Eq. (11) can only be negative if a[f] > 0. Thus, this case clearly cannot contribute to switching off neighborhood aggregation. 6. Case (+ − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. Eq. (9) becomes a[f](1 − α)Ws[f, :] (hi − hj) < 0. At the same time, the LHS of Eq. (10) simplifies to a[f]Ws[f, : ](hi − hj) and the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. Hence, a negative Eq. (9) leads to a positive Eq. (11). Accordingly, the last possible sign configuration also does not support switching off neighborhood aggregation, which concludes our derivation. A.2. Proof of Theorem 4.3 Statement (Restated Theorem 4.3). The gradients and parameters of GATE for layer l ∈ [L − 1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al+1 s [i], ∇al+1 s [i]L⟩ + ⟨al+1 t [i], ∇al+1 t [i]L⟩. (12) and, if additional independent matrices Ul and Vl are trainable, it also holds ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩ = ⟨Ul[i, :], ∇Ul[i,:]L⟩ + ⟨V l[i, :], ∇V l[i,:]L⟩. (13) The proof is analogous to the derivation of Theorem 2.2 by (Mustafa & Burkholz, 2023) that is restated in this work as Theorem 4.1. For ease, we replicate their notation and definitions here. Statement (Rescale invariance: Def 5.1 by Mustafa & Burkholz (2023)). The loss L(θ) is rescale-invariant with respect to disjoint subsets of the parametersθ1 and θ2 if for everyλ >0 we haveL(θ) = L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). Statement (Gradient structure due to rescale invariance Lemma 5.2 in (Mustafa & Burkholz, 2023)). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1L⟩ − ⟨θ2, ∇θ2L⟩ = 0. (14) We first consider the simpler case of GATES, i.e. W = U = V Theorem A.1 (Structure of GATES gradients). The gradients and parameters of GATES for layer l ∈ [L−1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩. (15) Following a similar strategy to (Mustafa & Burkholz, 2023), we identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Given the following definition of disjoint subsets θ1 and θ2 of the parameter set θ, associated with neuron i in layer l, 15GATE: How to Keep Out Intrusive Neighbors θ1 = {x|x ∈ Wl[i, :]} θ2 = {w|w ∈ Wl+1[:, i]} ∪ {al s[i]} ∪ {al t[i]} We show that the loss of GATES remains invariant for any λ >0. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. The scaled network parameters are denoted with a tilde as ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i], and ˜Wl[i, j] = λWl[i, j], and the corresponding networks components scaled as a result are denoted by ˜hl u[i], ˜hl+1 v [k], and ˜αl uv. We show that the parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] = hl+1 v [k]. Also recall Eq. (4) for W = U = V as: el uv = ((1 − quv)al s + (quv)al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) where quv = 1 if u = v and quv = 0 if u ̸= v. For simplicity, we rewrite this as: el uv,u̸=v = (al s)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (16) el uv,u=v = (al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (17) We show that ˜αl uv = exp(˜el uv)P u′∈N(v) exp(˜eluv) = αl uv , because (18) ˜el uv,u̸=v = el uv,u̸=v , and ˜el uv,u=v = el uv,u=v (19) which follows from the positive homogeneity of ϕ that allows ˜el uv,u=v = λ−1al s[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (20) = λ−1λal s[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (21) = el uv,u̸=v. (22) and similarly, 16GATE: How to Keep Out Intrusive Neighbors ˜el uv,u=v = λ−1al t[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (23) = λ−1λal t[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (24) = el uv,u=v. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] = ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i]. In the next layer, we therefore have ˜hl+1 v [k] = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k]. Thus, the output node representations of the network remain unchanged, and the loss L is rescale-invariant. Next consider the case that Wl, Ul, and V l are independent matrices. Similarly to the previous reasoning, we see that if we scale ˜Wl[i, :] = Wl[i, :]λ, then also scaling ˜Wl+1[:, i] = Wl+1[:, i]λ−1 and ˜al+1 s [i] = al+1 s [i]λ−1 and ˜al+1 t [i] = al+1 t [i]λ−1 will keep the GATE layer unaltered. In this case, we obtain an additional rescaling relationship between al s, al t and Ul, V l. A rescaling of the form ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i] could be compensated by ˜Ul[i, :] = Ul[i, :]λ and ˜V l[i, :] = V l[i, :]λ. It follows immediately that ˜euv = euv. A.3. Derivation of Insight 4.4 Following the analysis in A.1, in contrast to GAT, αij/αii << 1 can be easily realized in GATE with as[f] < 0 and at[f] > 0 for all or only a subset of the features. Note that for the non-weight-sharing case, U and V in GATE would simply correspond to Ws and Wt, respectively, in GATE and the same line of reasoning holds. Large norms are usually not required to create a notable difference in size between eii and eij. 17GATE: How to Keep Out Intrusive Neighbors B. Experimental Settings Our complete experimental setup is described as follows. Non-linearity For GATS and GAT networks, we substitute ϕ in Eq. (3) with LeakyReLU as defined in the standard architecture. For GATE, we substitute ϕ in Eq. (4) with ReLU in order to be able to interpret the sign of as and at parameters as contributing positively or negatively to neighborhood aggregation. MLP, MLP +GAT , and FAGCN also all use ReLU after every hidden layer. Network Width We vary the depth of GAT and GATE networks in all our experiments as specified. For synthetic datasets, the network width is fixed to 64 in all cases. For OGB datasets, we use the hidden dimensions used by (Brody et al., 2022). For the remaining datasets, the network width is also fixed to 64. Initialization The feature transformation parameter matrices, i.e.,W, U, and V are initialized randomly with an orthog- onal looks-linear structure (Burkholz & Dubatovka, 2019) for MLP, MLP+GAT , GAT(S) and GATE(S). The parameters a in GAT(S) use Xavier initialization (Glorot & Bengio, 2010), as is the standard. In GATE(S), as and at are initialized to 0 to initially give equal weights to the features of a node itself and its neighboring nodes. Optimization Synthetic, OGB, and remaining real-world tasks are run for a maximum of 10000, 2000, 5000 epochs, respectively, using the Adam optimizer. To isolate the effect of the architecture and study the parameter dynamics during training as best as possible, we do not use any additional elements such as weight decay and dropout regularization. We also do not perform any hyperparameter optimization. However, the learning rate is adjusted for different real-world datasets to enable stable training of models as specified in Table 6. Nevertheless, for a fair comparison, the same learning rate is used for a given problem across all architectures. For all synthetic data, a learning rate of 0.005 is used. Real-world datasets use their standard train/test/validation splits, i.e. those provided by Pytorch Geometric for Planetoid datasets Cora and Citeseer, by OGB framework for OGB datasets, and by (Platonov et al., 2023) for all remaining real-world datasets. Code Our experimental code and synthetic data generators are available at https://github.com/ RelationalML/GATE.git. Table 6: Details of real-world datasets used in experiments. dataset # nodes # edges # features # classes learning rate used for L layer networks ogb-arxiv 169, 343 2 , 315, 598 128 40 L = [12] : 0.001 ogb-products 2, 449, 029 123 , 718, 152 100 47 L = [8] : 0.001 ogb-mag 736, 389 10 , 792, 672 128 349 L = [5] : 0.005 roman-empire 22, 662 32 , 927 300 18 L = [5] : 0.001, L= [10] : 0.0005 amazon-ratings 24, 492 93 , 050 300 5 L = [5] : 0.001, L= [10] : 0.0005 questions 48, 921 153 , 540 301 2 L = [5] : 0.001, L= [10] : 0.0005 minesweeper 10, 000 39 , 402 7 2 L = [5] : 0.001, L= [10] : 0.0005 tolokers 11, 758 519 , 000 10 2 L = [5] : 0.001, L= [10] : 0.0005 cora 2, 708 10 , 556 1 , 433 7 L = [2, 5] : 0.005, L= [10, 20] : 0.0005 citeseer 3, 327 9 , 104 3 , 703 6 L = [2, 5] : 0.001, L= [10, 20] : 0.0001 actor 7, 600 26 , 659 932 5 L = [2, 5, 10, 20] : 0.005 texas 183 279 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.0005 wisconsin 251 450 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.005 18GATE: How to Keep Out Intrusive Neighbors C. Additional Results Smaller Real-World Datasets We evaluate GAT and GATE on five small-scale real-world datasets with varying ho- mophily levels β as defined in (Pei et al., 2020) and report results in Table 7. Higher values ofβ indicate higher homophily, i.e. similar nodes (with the same label) tend to be connected. We note that a 2-layer network of a baseline method for het- erophilic datasets, Geom-GCN (Pei et al., 2020), attains test accuracy (%) of64.1, 67.6, and 31.6 for Wisconsin, Texas, and Actor datasets, respectively, which is in line with that achieved by GATE. Except for Citeseer, the best overall performance for each dataset is achieved on a shallow model. This is not surprising as these datasets are small-scale and potentially prone to over-fitting in large models, particularly since we do not use any skip connections or regularization to retain model performance. Furthermore, the three heterophilic datasets have been recently shown to be problematic (Platonov et al., 2023). Therefore, a better evaluation of GATE is on relatively large-scale OGB datasets (Hu et al., 2021) and more recent heterophilic datasets (Platonov et al., 2023) that can exploit the flexibility of GATE. Although GATE is more pa- rameterized than GAT, it usually requires fewer training epochs and generalizes better, in addition to other advantages over GAT as discussed in the paper. Table 7: Test accuracy (%) of GAT and GATE models for network depthL on small-scale real-world datasets with varying homophily levels β. Entries marked with * indicate models that achieve 100% training accuracy and stable test accuracy. Otherwise, test accuracy at max. validation accuracy is reported. Data β L = 2 L = 5 L = 10 L = 20 GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE Wisc. 0.21 62 .7* 80.4 70.5* 51.0 70.5 60.7* 45.1 62.7 58.8 47 .1 62.7 60.7 Texas 0.11 56 .7* 67.6* 67.6* 51.4 67.6* 67.6* 56.7* 62.2* 62.3* 59.4* 62.1* 64.9 Actor 0.24 27 .1 32.2 31.6 25 .4 27 .5 29.2 25.3 27 .4 27.9 24.5 24 .6 29.4 Cora 0.83 80 .0 81.0* 80.8 79 .8 80.8* 80.4 77 .6 80.0* 79.2 77 .7 77 .2* 79.0 Cite. 0.71 68 .0 67 .6* 68.3 67.2 68.7* 67.8 66 .9 67.6* 67.6 68.2 67 .1* 69.2 Initialization of attention parameters in GAT We show in Fig. 7 that setting the initial value of attention parameters as and at in GATE to zero is, in fact, not what enables neighborhood aggregation but rather the separation ofa into as and at as discussed in Insight 4.4. Figure 7: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels for GAT with attention parameters a initialized to zero. Left to right:1, 2 and 5 layer models that achieve test accuracy of100%, 52.7%, and 36.2%, respectively, which is similar to the results obtained by standard Xavier initialization of attention parameters in GAT. Further analysis of experiments We present the analysis of α coefficients learned for some experiments in the main paper that were deferred to the appendix due to space limitations. 19GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 8: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 5 layer models. (a) original labels (b) Random labels Figure 9: Distribution of αvv against training epoch for the self-sufficient learning problem using Cora graph structure with original (top) and random (bottom) node labels and input node features as a one-hot encoding of labels. Left to right: 1, 2, and 5 layer GATES models that all 100% test accuracy except in the case of 5 layer model using original labels. In this case, although a training accuracy if 100% is achieved at 32 epochs with test accuracy 97.3%, a maximum test accuracy of 98.4% is reached at 7257 epochs. Training the model to run to 15000 epochs only increases it to 98.4%. An increased learning rate did not improve this case. However, we also run the GAT model for 15000 epochs for this case, and it achieves 85.9% test accuracy at epoch 47 where the model achieves 100% accuracy and only achieves a maximum test accuracy of 89.3% briefly at epoch 8. 20GATE: How to Keep Out Intrusive Neighbors Figure 10: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 1. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 1, 2, and 3 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 1 layer of the 2 and 3 layer models perform neighborhood aggregation. Table 8: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Entries marked with * identify models where 100% train accuracy is not achieved. Underlined entries identify the model with the highest train accuracy at the epoch of max. test accuracy. This provides an insight into how similar the function represented by the trained model is to the function used to generate node labels, i.e. whether the model is simply overfitting to the train data or really learning the task. Higher training and test accuracy simultaneously indicate better learning. In this regard, the difference in train accuracy at max. test accuracy between GATE and GATS or GAT is only 0.4, 1.0 and 0.6 for the settings (k = 1, L= 3), (k = 2, L= 4) and (k = 3, L= 3), respectively. k L Test Acc. @ Epoch of Max. Train Acc. Max Test Acc. @ Epoch GATS GAT GATE GAT S GAT GATE 1 1 92.0@2082* 91.2@6830* 93.2@3712* 93.2@1421 92.0@9564 93.6@3511 2 89.6@8524* 88.0@8935 91.2@942 91.6@5188 92.8@4198 95.6@111 3 86.4@9180* 88.8@997 92.8@618 91.2@6994 92.8@437 97.2@82 2 2 88.8@6736* 89.6@3907 88.8@467 93.2@151 93.2@95 92.0@105 3 82.0@7612 89.2@1950 91.6@370 91.6@1108 93.2@856 95.2@189 4 84.8@4898 82.4@739 87.2@639 88.0@1744 88.4@423 90.4@447 3 3 80.8@8670 80.4@737 85.2@391 86.4@1578 88.8@285 92.0@47 4 78.0@3012 80.4@767 89.6@480 86.8@1762 85.6@469 91.6@139 5 80.0@6611 74.4@1701 86.0@447 85.6@921 83.6@1098 91.2@243 21GATE: How to Keep Out Intrusive Neighbors (a) Tolokers, GAT:61.6% test AUROC.  (b) Tolokers, GATE:69.2% test AUROC. Figure 11: Distribution of αvv against training epoch for one run of 10 layer networks on real-world heterophilic task. Figure 12: Distribution of αvv against training epoch of 2-layer (left) and 5-layer (right) GAT networks for heterophilic datasets Texas (top) and Actor (bottom) 2-layer modes. Despite having connections to unrelated neighbors, GAT is unable to switch off neighborhood aggregation. Figure 13: Distribution of αvv, against training epoch of 2-layer (left) and 5-layer (right) GATE networks for heterophilic datasets Texas (top) and Actor (bottom), across layers could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods. For instance, in the case of Texas, GATE carries out little to no neighborhood aggregation in the first layer over input node features. Instead, aggregation is mainly done over node features transformed in earlier layers that effectuate non-linear feature learning as in perceptrons. However, in the case of Actor, GATE prefers most of the neighborhood aggregation to occur over the input node features, indicating that they are more informative for the task at hand. 22GATE: How to Keep Out Intrusive Neighbors Over-smoothing analysis In the main paper, we have already established the superior performance of GATE, compared to GAT, on several tasks. Intuitively, this can partially be attributed to reduced over-smoothing as its root cause, unnecessary neighborhood aggregation, is alleviated. Here, we verify this insight quantitatively. A widely accepted measure of over-smoothing is the Dirichlet energy (DE) (Cai & Wang, 2020). However, Eliasof et al. (2023) propose a modification of DE to measure GAT energy EGAT , that we use to evaluate over-smoothing in our experiments (see Table 9). We note that the notion of ‘over-smoothing’ is itself task-dependent. It is difficult to determine the optimal degree of smoothing for a task and the threshold that determines ‘over’-smoothing. This merits an in-depth analysis and curation of task-dependent smoothness measures that are not our focus. To show that GATE reduced over- smoothing relative to GAT, it suffices that a decrease in smoothing and an increase in accuracy occur simultaneously. Table 9: The measures Einput, EGAT and EGATE denote the smoothness of input node features, node features at the last layer L of the trained GAT and GATE models, respectively. Two cases are considered: All node pairs and only adjacent node pairs to measure smoothing at the global graph and local node level. Higher values indicate less smoothing. Node representations learned by GATE achieve higher test accuracy on all these tasks, as reported in the main paper, and are simultaneously less smooth than GAT in most cases, indicating that GATE potentially alleviates over-smoothing in GATs. (a) Synthetic self-sufficient task: varying graph structure and label distribution, node features as one-hot encoding of labels, L = 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE Cora, Original Labels 6.016 e + 06 1 .281 e + 08 2.037 e + 09 2.006 e + 03 7 .903 e + 03 1.971 e + 05 Cora, Random Labels 6.283 e + 06 3 .472 e + 09 3.747 e + 09 9.080 e + 03 9 .306 e + 05 3.966 e + 06 ER(p = .01), 2 Classes 4.994 e + 05 2 .701 e + 06 4.478 e + 07 5.042 e + 03 2 .272 e + 04 3.229 e + 05 ER(p = .01), 8 Classes 8.745 e + 05 3 .350 e + 07 2.615 e + 08 8.694 e + 03 1 .762 e + 05 1.960 e + 06 (b) Synthetic neighbor-dependent task: graph structure, node features, generator parameter k, and label distribution as in Section 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE k = 1, L= 3 1 .953 e + 06 5.306 e + 07 5.095 e + 07 1 .957 e + 05 4.234 e + 05 3.610 e + 05 k = 2, L= 4 1 .975 e + 06 1 .193 e + 07 2.198 e + 07 2.012 e + 04 1 .016 e + 05 1.939 e + 05 k = 3, L= 5 1 .951 e + 06 1 .645 e + 07 1.053 e + 08 1.966 e + 04 1 .408 e + 05 9.096 e + 05 (c) Real-world tasks. Dataset L All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE roman-empire 5 1 .274 e + 09 1 .002 e + 11 7.491 e + 11 7.878 e + 04 2 .441 e + 06 4.009 e + 07 amazon-ratings 10 3 .844 e + 08 1 .187 e + 10 2.272 e + 10 4.933 e + 04 3 .848 e + 05 7.430 e + 05 minesweeper 5 6 .869 e + 07 1 .386 e + 09 2.531 e + 10 2.628 e + 04 1 .946 e + 05 7.017 e + 06 tolokers 10 1 .391 e + 08 1 .044 e + 11 1.042 e + 11 3.423 e + 05 1 .249 e + 08 1.397 e + 08 cora 10 5 .088 e + 05 1 .437 e + 07 2.783 e + 08 6.490 e + 02 5 .959 e + 02 1.226 e + 04 citeseer 10 3 .463 e + 05 2 .916 e + 05 1.126 e + 07 2.360 e + 02 2 .426 e + 00 1.030 e + 02 texas 10 1 .945 e + 06 3 .280 e + 04 3.877 e + 04 1.758 e + 04 8 .487 e + 01 9.695 e + 01 actor 10 2 .612 e + 08 1.800 e + 07 1.364 e + 07 1 .237 e + 05 2.810 e + 03 2.215 e + 03 wisconsin 10 4 .438 e + 06 1 .057 e + 07 1.008 e + 08 3.299 e + 04 3 .765 e + 04 7.363 e + 08 23GATE: How to Keep Out Intrusive Neighbors Comparison with other GNNs Other GNN architectures could potentially switch off neighborhood aggregation, as we show here. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatena- tion operation). We evaluate the performance of three such architectures that, in principle, employ different aggregation methods, which are likely to be capable of switching off neighborhood aggregation, on synthetic datasets empirically and discuss their ability or inability to switch off neighborhood aggregation qualitatively as follows. 1. ωGAT (Eliasof et al., 2023) introduces an additional feature-wise layer parameter ω that can, in principle, switch off neighborhood aggregation by setting ω parameters to 0, in addition to the attention mechanism based on GAT. How- ever, in practice, as we verify on our synthetic dataset in Figure 14, it is unable to effectively switch off neighborhood aggregation. Although it outperforms GAT, it is still substantially worse than GATE, especially for the deeper model due to unnecessary neighborhood aggregations. Another architecture based on graph attention, superGAT(Kim & Oh, 2021), falls under the paradigm of structural learning as it uses a self-supervised attention mechanism essentially for link prediction between nodes, and therefore its comparison with GATE is infeasible. 2. GraphSAGE (Hamilton et al., 2018) uses the concatenation operation to combine the node’s own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neigh- borhood aggregation for the synthetic datasets designed for the self-sufficient learning task (see Table 10). Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing (see Table 11). 3. FAGCN (Bo et al., 2021) requires a slightly more detailed analysis. Authors of FAGCN state in the paper that: ‘When αG ij ≈ 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.’ where αG ij defined in the paper can be considered analogous to αij in GAT, though they are defined differently. Thus, from an expressivity point of view, FAGCN should be able to assign parameters such that all αG ij = 0. However, we empirically observe on synthetic datasets designed for the self-sufficient learning task, values of αG ij do not, in fact, approach zero. Despite being unable to switch off neighborhood aggregation, FAGCN, in its default implementation, achieves 100% test accuracy on the task. We discover this is so because FAGCN introduces direct skip connections of non-linearly transformed raw node features to every hidden layer. Given the simplicity of the one-hot encoded features in the datasets and the complete dependence of the label on these features, FAGCN is able to represent the desired function. In order to better judge its ability to switch off neighborhood aggregation by setting αG ij = 0, we remove this skip connection. From an expressivity point of view, FAGCN should still be able to achieve 100% test accuracy by using only the (non-)linear transformations of raw features initially and performing no neighborhood aggregation in the hidden layers. However, we find that FAGCN was unable to emulate this behavior in practice. For a fair comparison of the differently designed attention mechanism in FAGCN with GATE, we introduce self-loops in the data so FAGCN may also receive a node’s own features in every hidden layer. Even then, FAGCN fails to achieve perfect test accuracy as shown in Table 10. Therefore, we suspect the attention mechanism in FAGCN may also be susceptible to the trainability issues we have identified for the attention mechanism in GAT. Nevertheless, the capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE and both could be combined. It would be interesting to derive conservation laws inherent to other architectures such as FAGCN and GraphSAGE and study how they govern the behaviour of parameters. Furthermore, by design, FAGCN does not perform any non-linear transformations of aggregated neighborhood features which may be necessary in some tasks, such as our synthetic dataset for the neighbor-dependent learning task. As Table 11 shows, GATE outperforms FAGCN on such a task. Lastly, we would like to emphasize that our aim is to provide insights into the attention mechanism of GAT and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. 24GATE: How to Keep Out Intrusive Neighbors Figure 14: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels, for theωGAT architecture for the1, 2 and 5 layer models that achieve test accuracy of 100%, 98.5%, and 49.3%, respectively. Table 10: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of network layers, respectively. Original (Orig.) and Randomized (Rand.) labels are used for the Cora structure. The FAGCN model is implemented without skip connections from the input layer to every other layer and without any self-loops in input data, whereas FAGCN* denotes the model also without skip connections but with self-loops introduced for all nodes in input data. Structure C L Max. Test Accuracy (%) GAT GATE FAGCN FAGCN* SAGE Cora O,7 1 100 100 90.1 97 .6 100 2 94 .6 100 94.2 94 .9 98 .8 5 88 .5 99.7 87.1 89 .1 92 .4 R,7 1 100 100 61.6 97 .8 100 2 57 .0 100 69.2 70 .5 100 5 36 .7 100 21.2 36 .7 99 .6 ER (p = 0.01) R,2 1 100 100 100 100 100 2 100 100 100 100 100 5 99 .6 100 96.4 99 .2 100 R,8 1 99 .2 100 86.4 98 .8 100 2 97 .6 100 86.0 91 .6 100 5 38 .4 100 31.6 40 .4 100 25GATE: How to Keep Out Intrusive Neighbors Table 11: Neighbor-dependent learning: k and L denote the number of hops aggregated in the neighborhood to generate labels, and the number of layers of the evaluated network, respectively. k L Max Test Accuracy (%) @ Epoch GAT GATE SAGE FAGCN 1 1 92@9564 93.6 @ 3511 93.2@2370 93 .2@1618 2 92 .8@4198 95.6 @ 111 95.6@723 94 .1@1455 3 92 .8@437 97.2 @ 82 96.8@100 81 .2@573 2 2 93.2 @ 95 92.0@105 90 .8@199 90 .4@170 3 93 .2@856 95.2 @ 189 94.4@113 88 .8@283 4 88 .4@423 90 .4@447 92.4 @ 139 87.6@549 3 3 88 .8@285 92.0 @ 47 87.6@45 89 .2@528 4 85 .6@469 91.6 @ 139 88@60 89 .2@3191 5 83 .6@1098 91.2 @ 243 86.0@35 88 .8@205 26",
      "meta_data": {
        "arxiv_id": "2406.00418v2",
        "authors": [
          "Nimrah Mustafa",
          "Rebekka Burkholz"
        ],
        "published_date": "2024-06-01T12:31:15Z",
        "pdf_url": "https://arxiv.org/pdf/2406.00418v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The research identifies a structural limitation in Graph Attention Networks (GATs): their inability to switch off task-irrelevant neighborhood aggregation, leading to over-smoothing and performance degradation, especially in deeper models. To address this, the authors propose GATE, an extension that can selectively switch neighborhood aggregation on or off. GATE alleviates over-smoothing, benefits from increased model depth by leveraging layers for non-linear feature transformations, and often outperforms GATs on heterophilic datasets by down-weighting unrelated neighbors. It also offers interpretable learned self-attention coefficients. GATE achieves a new state-of-the-art test accuracy on the OGB-arxiv dataset and includes a synthetic testbed for analyzing adaptive neighborhood aggregation schemes.",
        "methodology": "The core methodology involves modifying the GAT architecture. While GAT layers are defined by a standard attention mechanism, GATE introduces separate learnable attention parameters, 'as' for neighborhood features and 'at' for a node's own features (as per Eq. 4, modifying Eq. 3 of GAT). This allows GATE to flexibly weigh the importance of node and neighborhood features. The theoretical foundation is an updated conservation law (Theorem 4.3) for GATE gradients, which enables the model to switch off neighborhood aggregation in a well-trainable parameter regime. This contrasts with GAT, which requires large attention parameter norms (a less trainable regime) to achieve similar selective aggregation. The non-linear activation function used in GATE is ReLU for interpretability, while GAT uses LeakyReLU. Initial parameters 'as' and 'at' in GATE are set to zero to avoid initial inductive bias.",
        "experimental_setup": "Experiments were conducted on both synthetic and real-world graphs for node classification. The synthetic testbed included two problem types: self-sufficient learning (label-relevant information in own features, using Erdős–Rényi graphs and Cora structure with original/randomized labels) and neighbor-dependent learning (label-relevant information in k-hop neighbors' features, using Erdős–Rényi graphs with K-means clustered labels). Real-world evaluations used five heterophilic benchmark datasets (roman-empire, amazon-ratings, questions, minesweeper, tolokers), three OGB datasets (OGB-arxiv, OGB-products, OGB-mag), and five smaller-scale datasets (Cora, Citeseer, Actor, Texas, Wisconsin). Models were optimized using the Adam optimizer for up to 10,000 epochs, with orthogonal looks-linear initialization for feature transformation matrices and Xavier or zero initialization for attention parameters. No weight decay or dropout regularization was applied to isolate architectural effects. Performance was measured by test accuracy, AUC-ROC, and a quantitative measure of over-smoothing using a modified Dirichlet energy (EGAT). Comparisons were made against GAT, MLP, MLP+GAT, FAGCN, GAT-sep, GT, SAGE, H2GCN, CPGNN, GPR-GNN, FSGNN, GloGNN, GBK-GNN, JacobiConv, ResNet, ResNet+SGC, ResNet+adj, ωGAT, and GraphSAGE.",
        "limitations": "The paper notes that for small-scale datasets, shallower models often performed better, suggesting that deeper GATE models might be prone to overfitting without additional elements like skip connections or regularization, which were intentionally omitted in the experiments to isolate the architecture's effect. Furthermore, the authors acknowledge the difficulty in defining and determining the optimal degree of smoothing or the threshold for 'over'-smoothing for a given task, indicating a broader open challenge in GNN research that was not the focus of this work.",
        "future_research_directions": "Future research could explore GATE's potential to answer questions about the inherent importance of specific graph structures for various tasks. It also suggests combining GATE with complementary architectures like FAGCN, which has the capacity to learn negative associations with neighboring nodes. Another direction is to derive conservation laws for other GNN architectures, such as FAGCN and GraphSAGE, to better understand how they govern parameter behavior and aggregation mechanisms."
      }
    },
    {
      "title": "GraphQNTK: Quantum Neural Tangent Kernel for Graph Data"
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The research identifies a major limitation in Graph Attention Networks (GATs) where increased network depth leads to severe performance degradation and trainability issues. This problem is attributed to hampered signal propagation during training, causing a high portion of parameters in GATs with standard initialization to struggle changing. The main contributions include: 1) deriving a conservation law of GAT gradient flow dynamics, which explains this lack of trainability in deeper GATs with standard initialization; 2) proposing a novel balanced initialization scheme that enables more effective gradient propagation, thereby allowing the successful training of deeper GAT networks; and 3) demonstrating empirically that this balanced initialization significantly speeds up training and convergence time while establishing a causal link between parameter balancedness and trainability. The derived theorem also serves as a foundation for studying learning dynamics in other positive homogeneous models with attention mechanisms.",
        "methodology": "The methodology involves a theoretical derivation of a conservation law for GATs with positive homogeneous activation functions (like ReLU) under gradient flow, including variations such as shared feature weights and multiple attention heads. This law is derived by identifying a multiplicative rescale invariance in GATs and applying a lemma on gradient structure due to rescale invariance. Based on this theoretical insight, a balanced initialization procedure (Procedure 2.6) is proposed. This procedure involves setting attention parameters to zero and scaling feature weights to ensure that the l2-norms of incoming and outgoing weights at each neuron are balanced. Two specific balanced initialization schemes are evaluated: Balanced Xavier (BalX), using Xavier initialization for feature weights before balancing, and Balanced Orthogonal (BalO), employing a 'looks-linear (LL) mirrored block structure' orthogonal initialization for feature weights before balancing. Training is performed using SGD and Adam optimizers.",
        "experimental_setup": "The experiments were conducted on nine common benchmark datasets for semi-supervised node classification: Cora, Citeseer, Pubmed, Actor, Chameleon, Squirrel, Cornell, Texas, and Wisconsin. GAT models of varying depths (L=2, 5, 10, 20, 40, and stress-tested up to 64, 80 layers) and a fixed hidden dimension (width=64, with some tests at 512) were trained using both SGD and Adam optimizers for up to 5000 epochs or until convergence. Five initialization schemes were compared: standard Xavier (Xav), Xavier with zero attention (XavZ) for ablation, Balanced Xavier (BalX), and Balanced Orthogonal (BalO). Model performance was evaluated based on test accuracy (mean ±95% confidence interval over five runs) and epochs to reach the best model state. The study also analyzed the relative change of parameters and relative gradient norms. Architectural variations like multiple attention heads, ELU activation, dropout, weight decay, and unshared weights were also investigated. Comparisons were made against LipschitzNorm [14] for training deep GATs.",
        "limitations": "The derived conservation law is specifically applicable to the self-attention mechanisms in the original GAT and GATv2 models, and architectural variants like ωGAT, but requires modifications for other types of self-attention (e.g., dot-product self-attention) or Transformer-based architectures. The theoretical framework assumes positively homogeneous activation functions, which means non-homogeneous functions like ELU can negatively impact certain balanced initializations (e.g., BalO). The study also notes that techniques like dropout and weight decay, while aiding optimization, are insufficient alone for enabling trainability in deeper GATs. Additionally, at very high depths, reduced performance with balanced initialization might be due to a lack of convergence within the set epoch limit rather than a fundamental trainability issue. The theory provides a coarse-level conservation law and cannot fully explain all fine-grained training dynamics observed.",
        "future_research_directions": "Future research directions include extending the study of learning dynamics to other positive homogeneous models incorporating attention mechanisms, such as Transformers and Vision Transformers, building upon the stepping stone provided by this work. Another promising area is exploring methods to achieve or approximate dynamical isometry in general Graph Neural Networks (GNNs), as the current LL-orthogonal initialization does not perfectly achieve it in GATs. Modifying the conservation law to accommodate different self-attention mechanisms, particularly dot-product self-attention found in models like SuperGAT, is also suggested. Further investigation into how network width and overparameterization aid generalization performance in GNNs, potentially relating to random feature models, and studying implicit regularization effects during gradient descent in the context of GATs are also proposed. Finally, the integration and impact of residual skip connections within a balanced initialization framework could be explored."
      }
    },
    {
      "title": "Generalizing CNNs to graphs with learnable neighborhood quantization"
    }
  ]
}